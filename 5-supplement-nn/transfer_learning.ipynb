{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习介绍\n",
    "\n",
    "对于 pub 问题 等，迁移学习是个不错的方法，因此这里了解下迁移学习的基本方法。首先可以简要回顾下深度学习的相关概念：[深度学习入门者必看：25个你一定要知道的概念](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247485927&idx=1&sn=606f87b569a0eb7c29f1c2374ff698dd&chksm=e8d3be95dfa43783ddbd577ce8cc7db64c1b02bb55480c1d3f2ba8e06e4b948d1fd6b8e2defc&scene=21#wechat_redirect)\n",
    "\n",
    "A survey on Transfer Learning 是一篇比较著名的文章。最后再看这篇文章，现在参考一些blog 记录关于迁移学习的基础知识：\n",
    "\n",
    "- [Transfer Learning - Machine Learning's Next Frontier](https://ruder.io/transfer-learning/)\n",
    "- [深度 | 迁移学习全面概述：从基本概念到相关研究（基本算是上一篇的翻译）](https://www.sohu.com/a/130511730_465975)\n",
    "- [什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？](https://www.zhihu.com/question/41979241)\n",
    "- [独家 | 一文读懂迁移学习（附学习工具包） ](https://www.sohu.com/a/156932670_387563)\n",
    "- [一文看懂迁移学习：怎样用预训练模型搞定深度神经网络？](https://zhuanlan.zhihu.com/p/27657264)\n",
    "- [模型汇总-14 多任务学习-Multitask Learning概述](https://zhuanlan.zhihu.com/p/27421983)\n",
    "- [A Gentle Introduction to Transfer Learning for Deep Learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n",
    "- [Transfer Learning in Keras with Computer Vision Models](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/)\n",
    "- [An Introduction to Transfer Learning in Machine Learning](https://medium.com/kansas-city-machine-learning-artificial-intelligen/an-introduction-to-transfer-learning-in-machine-learning-7efd104b6026)\n",
    "\n",
    "了解了一些基本的术语和概念之后，补充几篇paper做参考：\n",
    "\n",
    "- [A Survey on Transfer Learning](https://ieeexplore.ieee.org/abstract/document/5288526)\n",
    "- [Chapter 11: Transfer Learning --> torrey.handbook09.pdf](ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group)\n",
    "- [How transfer-able are features in deep neural networks?](https://arxiv.org/abs/1411.1792)\n",
    "- [Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/abs/1502.02791)\n",
    "- [Unsupervised Domain Adaptation with Residual Transfer Networks](http://papers.nips.cc/paper/6110-unsupervised-domain-adaptation-with-residual-transfer-networks)\n",
    "\n",
    "## 为什么需要迁移学习 why\n",
    "\n",
    "现实世界是messy的，并且包含了无限多的场景，很多场景可能都是模型训练的时候没有碰到的，对于这些场景，模型是没有准备好去预测的。将知识迁移到新的条件下就是迁移学习。\n",
    "\n",
    "传统机器学习通常有两个基本假设，即**训练样本与测试样本满足独立同分布的**假设和**必须有足够可利用的训练样本**假设。然而，现实生活中这两个基本假设有时往往难以满足。比如，股票数据的时效性通常很强，利用上个月数据训练出来的模型，往往很难顺利地运用到下个月的预测中去；比如公司开设新业务，但愁于没有足够的数据建立模型进行用户推荐。近年来在机器学习领域受到广泛关注的迁移学习恰恰解决了这两个问题。迁移学习用已有的知识来**解决目标领域中仅有少量有标签样本数据甚至没有数据的学习问题**，从根本上放宽了传统机器学习的基本假设。由于被赋予了人类特有的举一反三的智慧，迁移学习能够**将适用于大数据的模型迁移到小数据上，发现问题的共性，从而将通用的模型迁移到个性化的数据上，实现个性化迁移**。\n",
    "\n",
    "迁移学习(Transfer learning) 顾名思义就是就是**把已学训练好的模型参数迁移到新的模型来帮助新模型训练**。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以**将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型**从而加快并优化模型的学习效率不用像大多数网络那样从零学习（starting from scratch，tabula rasa）。\n",
    "\n",
    "Transfer Learning是机器学习的分支，很多方法其实是不需要用NN的，而现在之所以Transfer Learning和神经网络联系如此紧密，主要因为NN的发展太快，太强大，太火爆，导致Transfer Learning的研究都往神经网络靠了。\n",
    "\n",
    "为什么往深度神经网络DNN上靠呢？因为DNN是一个通过pre-train获得数据的分层特征表达，然后用高层语义分类的模型。模型底层是低级语义特征（比如说，边缘信息，颜色信息等），这样的特征实际上在不同的分类任务中都是不变的，而**真正区别的是高层特征**。这也解释了通常使用新的数据集去更新AlexNet，GoogleNet的最后几层网络权值，来实现简单的“迁移”这个小trick。和深度神经网络相结合的transfer模型基本思想都类似，为了保证域差异最小，loss项都会加一个MMD约束。\n",
    "\n",
    "在监督学习方面，在过去几年中已经获得了训练越来越准确的模型的能力。这些成功的模型都是极其地重视数据的，依靠大量的标签数据来实现它们的性能。\n",
    "\n",
    "但是把机器学习的模型应用在自然环境中时，模型会面临大量之前未曾遇到的条件，它不知道如何去处理；另外，每一个用户都有他们自己的偏好，也需要处理和生成不同于之前用来训练的数据；还有有时会要求模型执行很多和训练相关但是不相同的任务。在所有这些情况下，会遭遇明显的表现下降甚至完全失败。\n",
    "\n",
    "Transfer Learning的初衷是节省人工标注样本的时间，让模型可以通过已有的标记数据（source domain data）向未标记数据（target domain data）迁移。比如下图所示，假设前两张正对着摄像机的行人作为训练集，后两张背对着的行人图片作为测试集，结果该模型的测试评分会很差，因为训练时没有考虑到摄像机观察角引起的问题，相类似在图像识别领域会有很多因素会降低识别率（例如光照，背景等）。ok，那能否用一些未标记的图片（类似图3，4这样的图），增强我们的行人检测模型，让它不仅可以识别正对着的行人，还可以识别背对着的行人？这就是迁移学习要干的事。\n",
    "\n",
    "![](v2-e92213c12444fc75ec3e9f5514e1ac28_hd.jpg)\n",
    "\n",
    "迁移学习可以帮助我们处理这些全新的场景，它是机器学习在**没有大量标签数据的任务和域**中规模化应用所必须的。\n",
    "\n",
    "非监督学习和强化学习虽然有很多前沿成果，但是在工业界，还存在着瓶颈。而迁移学习可以很快做出一些有影响的成果，low-hanging fruit。\n",
    "\n",
    "## 什么是迁移学习 what\n",
    "\n",
    "神经网络需要用数据来训练，它从数据中获得信息，进而把它们转换成相应的权重。这些**权重能够被提取出来，迁移到其他的神经网络中**，我们“迁移”了这些学来的特征，就不需要从零开始训练一个神经网络了 。\n",
    "\n",
    "当在训练经网络的时候我们希望网络能够在多次正向反向迭代的过程中，找到合适的权重。通过使用之前在大数据集上经过训练的预训练模型，我们可以直接使用相应的结构和权重，将它们应用到我们正在面对的问题上。\n",
    "\n",
    "预训练模型(pre-trained model)是前人为了解决类似问题所创造出来的模型。你在解决问题的时候，不用从零开始训练一个新模型，可以从在类似问题中训练过的模型入手。\n",
    "\n",
    "一个预训练模型可能对于你的应用中并不是100%的准确对口，但是它可以为你节省大量功夫。\n",
    "\n",
    "在选择预训练模型的时候你需要非常仔细，如果你的问题与预训练模型训练情景下有很大的出入，那么模型所得到的预测结果将会非常不准确。Transfer learning only works in deep learning if the model features learned from the first task are general.\n",
    "\n",
    "如果预训练模型已经训练得很好，我们就不会在短时间内去修改过多的权重，在迁移学习中用到它的时候，往往只是进行微调(fine tune)。在修改模型的过程中，我们通过会采用比一般训练模型更低的学习速率。关于微调整部分可以参考本项目文件夹下的cnn-mnist.ipynb。\n",
    "\n",
    "现如今深度学习模型仍旧缺乏泛化到不同于训练的环境的能力。什么时候需要这种能力呢？就是每一次将你的**模型用到现实世界**，而不是精心构建的数据集的时候。现实世界是混乱的，并且包含大量全新的场景，其中很多是模型在训练的时候未曾遇到的，因此这又使得模型不足以做出好的预测。将**知识迁移到新环境中的能力通常被称为迁移学习**（transfer learning）。\n",
    "\n",
    "比如，在机器学习的经典监督学习场景中，如果我们要针对一些任务和域 A 训练一个模型，我们会假设被提供了针对同一个域和任务的标签数据，如下图所示。\n",
    "\n",
    "![](bc62ca0bdd3e4ac78e72c850e3143246_th.jpeg)\n",
    "\n",
    "现在我们可以在这个数据集上训练一个模型 A，并期望它在同一个任务和域中的未知数据上表现良好。\n",
    "\n",
    "另一个场景，当给定一些任务或域 B 的数据时，我们还需要可以用来训练模型 B 的有标签数据，这些数据要属于同一个任务和域，这样我们才能预期能在这个数据集上表现良好。\n",
    "\n",
    "比如我们的目标是训练识别夜间行人的模型，我们可以训练识别白天行人的，但是我们如果直接拿白天训练的结果用到夜晚，那么效果可能是很差的。因为模型继承了训练数据的偏差，它不知道怎么将模型泛化到新的领域。如果我们检测的是自行车，那么源模型是完全不能用的。\n",
    "\n",
    "当我们没有足够的来自于我们关心的任务或域的标签数据来训练可靠的模型时，传统的监督学习范式就支持不了了。\n",
    "\n",
    "迁移学习允许我们通过借用已经存在的一些相关的任务或域的标签数据来处理这些场景。我们尝试着把在源域中解决源任务时获得的知识存储下来，并将其应用在我们感兴趣的目标域中的目标任务上去。如下图所示：\n",
    "\n",
    "![](transfer_learning_setup.png)\n",
    "\n",
    "这里先提一个概念－－domain adaptation， 在研究source domain和target domain时，基于某一特征，会发现两个domain的数据分布差别很大，如下图所示：\n",
    "\n",
    "![](v2-8da69cb2647edf0ce726fa5391837e78_hd.jpg)\n",
    "\n",
    "红线是source dataset的颜色信息值分布，蓝线是target dataset的颜色值分布。很明显对于这一特征来讲，两个域的数据本来就是有shift的。而这个shift导致我们evaluate这个模型的时候准确率会大大降低。\n",
    "\n",
    "既然这个特征不合适，那我们就**换特征**，domain adaptation旨在利用各种的**feature transformation**手段，**学习一个域间不变的特征表达**，基于这一特征，我们就可以更好的同时对两个域的数据进行分类了。\n",
    "\n",
    "\n",
    "\n",
    "具体的定义如下所示：\n",
    "\n",
    "迁移学习涉及到域和任务的概念。以二元文档分类为例，一个域 $\\mathcal{D}$ 由一个特征空间 $\\mathcal{X}$ 和特征空间上的边际概率分布 $P(X)$ 组成，其中，$X=x_1,...x_n \\in \\mathcal{X}$。对于有词袋表征（bag-of-words representation）的文档分类，$\\mathcal{X}$ 是所有文档表征的空间，$x_i$ 是第 i 个单词的二进制特征，$X$ 是一个特定的文档。\n",
    "\n",
    "给定一个域 $\\mathcal{D}={\\mathcal{X},P(X)}$，一个任务$\\mathcal{T}$由一个标签空间$\\mathcal{y}$及一个条件概率分布$P(Y|X)$构成，这个条件概率分布通常是从由特征—标签对 $x_i \\in X,y_i \\in \\mathcal{y}$ 组成的训练数据中学习得到。在这里提到的文档分类例子中，$\\mathcal{y}$是所有标签的集合，比如True，False，$y_i$要么是True，要么是False。\n",
    "\n",
    "给定一个资源域$\\mathcal{D}_S$，一个对应的源任务$\\mathcal{T}_S$，以及一个目标域$\\mathcal{D}_T$和一个目标任务$\\mathcal{T}_T$，迁移学习的目标就是用从$\\mathcal{D}_S$和$\\mathcal{T}_S$（$\\mathcal{D}_S \\neq \\mathcal{D}_T$ 或 $\\mathcal{T}_S \\neq \\mathcal{T}_T$）获取的信息来学习$\\mathcal{D}_T$中目标条件概率分布$P(Y_T|X_T)$。大多数情况下，目标任务的例子有限，比源数据少很多。\n",
    "\n",
    "$\\mathcal{D}$ 和 $\\mathcal{T}$ 由元组定义，根据他们的不相等程度有了四个迁移学习的场景。\n",
    "\n",
    "给定源和目标域$\\mathcal{D}_S$ 和 $\\mathcal{D}_T$，这里$\\mathcal{D}={\\mathcal{X},P(X)}$，和源及目标任务$\\mathcal{T}_S$和$\\mathcal{T}_T$($\\mathcal{T}={\\mathcal{y},P(Y|X)}$)。源和目标条件可以有四种方式，本例子下可以举例如下：\n",
    "\n",
    "1. $\\mathcal{X}_S \\neq \\mathcal{X}_T$。源和目标域的特征空间不同，比如文档是由两个不同语言写的。在NLP中，这是cross-lingual adaption。\n",
    "2. $P(\\mathcal{X}_S) \\neq P(\\mathcal{X}_T)$。源和目标域的边缘概率分布不同，比如文档讨论不同的主题，这称为domain adaption\n",
    "3. $\\mathcal{y}_S \\neq \\mathcal{y}_T$。两个任务的标签空间是不同的。比如目标任务中，文档需要分配不同的标签。这种场景通常和第四个一起出现，极少有标签空间不同的两个不同任务却有相同条件概率分布的。\n",
    "4. $P(Y_S|X_S) \\neq P(Y_T|X_T)$。源和目标任务的条件概率分布不同。比如源文档和目标文档的类别是不平衡的。这种情况在实践和方法中很常见，如过采样、欠采样\n",
    "\n",
    "## 怎么用迁移学习 how\n",
    "\n",
    "两种主要方式：\n",
    "\n",
    "- Develop Model Approach\n",
    "    1. Select Source Task. 选择一个相关的有大量数据的模型预测问题，在输入数据、输出数据和/或从输入到输出数据的映射过程中学习的概念之间存在某种关系。\n",
    "    2. Develop Source Model. 对选择的第一个任务建立模型。 模型必须比简单模型更好，以确保执行了一些特性学习。\n",
    "    3. Reuse Model. 在源任务上拟合的模型作为第二个任务的模型起始点。这种方式会使用源模型的全部或部分，这取决于所使用的模型技术。\n",
    "    4. Tune Model. 针对目标任务，模型需要针对输入输出数据对作调整和精炼。\n",
    "- Pre-trained Model Approach\n",
    "    1. Select Source Model. 选择一个预训练的模型，有很多研究机构发布了可用的基于大量数据训练的模型可供选择。\n",
    "    2. Reuse Model. 预训练的模型可以用来作为目标模型的起始点。同样的，可使用源模型的全部或部分，这取决于所使用的模型技术。\n",
    "    3. Tune Model. 同样地，针对目标任务，模型需要针对输入输出数据对作调整和精炼。\n",
    "    \n",
    "在深度学习领域，第二种方式更常见。\n",
    "\n",
    "在解决提及的四种迁移学习场景上，迁移学习拥有悠久的研究和技术的历史。深度学习的出现导致了一系列迁移学习的新方法，接下来会概览其中的一些。2010那篇综述文献可以看到一些早期方法的综述。\n",
    "\n",
    "### 使用预训练的 CNN 特征\n",
    "\n",
    "为了了解目前应用中最常见的迁移学习方式，我们必须理解在 ImageNet 上大型卷积神经网络所取得的杰出成功。\n",
    "\n",
    "虽然这些模型工作方式的许多细节仍然是一个谜，但我们现在已经意识到，**较低的卷积层捕获低级图像特征**，例如边缘，而**较高的卷积层捕获越来越复杂的细节**，例如身体部位、面部和其他组合性特征。\n",
    "\n",
    "最后的全连接层通常被认为是**捕获了与解决相应任务相关的信息**，例如 AlexNet 的全连接层可以指出哪些特征与在 1000 个物体类中分类一张图片相关的。\n",
    "\n",
    "然而，尽管知道一只猫有胡须、爪子、毛皮等是将一个动物识别为猫所必需的,但它并不能帮助我们识别新物体或解决其他共同的视觉任务（如场景识别、细粒度识别、属性检测和图像检索）。\n",
    "\n",
    "然而，能够帮助我们的是捕获关于一幅图像是如何组成的以及它包括什么样的边缘和形状组合的通用信息。正如我们之前所描述的，这种信息被包含在在 ImageNet 上训练出的**卷积神经网络的最后卷积层或者最后一层之前的全连接层**。\n",
    "\n",
    "所以，对于一个新任务，我们可以简单地使用这种由最先进的卷积神经网络在 ImageNet 上预训练出的现成特征，并在这些抽象的特征上面训练出新的模型。在实际中，我们要么**保持预训练的参数固定不变，要么以一个较小的学习率来调节它们**，以保证我们不会忘记之前学习到的知识。\n",
    "\n",
    "![](1_oCZ4n4iaOYTbgsMkSEJSgA.png)\n",
    "\n",
    "### 学习图像的隐含结构\n",
    "\n",
    "一个类似的假设被用来推动生成模型：当训练一个生成模型的时候，我们**假设生成逼真图像的能力需要对图像隐含结构的理解**，它反过来可以被用在很多其他任务中。这个假设本身也依赖一个前提，即**所有图像都由一个低维度的流形结构决定**，这就是说，可以用一个模型来抽象出图像的隐含结构。利用生成对抗网络来生成逼真图片的最新进展揭示，这种模型或许真的存在。\n",
    "\n",
    "预训练的特征在视觉之外的领域有用吗？\n",
    "\n",
    "现成的卷积神经网络特征在视觉任务上有着无出其右的结果，但是问题在于，**使用其它的数据，在其它领域（例如语言）能否复制这种成功？** 目前，对自然语言处理而言没有能够实现与图像领域一样令人惊叹结果的现成特征。为什么呢？这样的特征是否存在呢？如果不存在，那为什么视觉比语言更有利于这种形式的迁移呢？\n",
    "\n",
    "诸如词性标注或分块这类低级别任务的输出就可以被比作现成的特征，但是在没有语法学的帮助下，不能采集到更细粒度的特征。正如我们看到的，可概括的现成特征似乎存在似乎存在于众多任务中原型任务中。在视觉中，物体识别就占据了这样的地位。在语言中，最接近的类似物可能是语言建模：为了预测给定词汇序列中的下一个单词或者句子，模型需要处理语言是如何构造的知识，需要理解那些单词很可能是相关的，那些单词很可能是彼此跟随出现的，需要对长期依赖关系进行建模，等等。\n",
    "\n",
    "使用预训练特征是目前做迁移学习的最直接、最常用的方式。不过，到目前为止它并不是唯一的一种。\n",
    "\n",
    "### 学习域不变的表征\n",
    "\n",
    "在实际中，预训练特征通常被用在我们想适应的新任务的场景3中。 对其他场景而言，另一个由深度学习实现知识迁移的方式是学习**基于域而不会改变的表征**。给非视觉任务创建基于特定域的不变表征要比为所有任务生成有用的表征要更加经济，更加可行。\n",
    "\n",
    "### 让表征更加相似\n",
    "\n",
    "为了提高学到的表征从源域到目标域的可迁移性，我们希望两个域之间的表征尽可能相似，这样一来，我们就不用考虑可能阻碍迁移的特定域的特征，只需要考虑域之间的共同点。\n",
    "\n",
    "与其仅仅让我们的自动解码器学到一些表征，不如积极地激励以让两个域中的表征和彼此变得更加相似。我们可以像预处理步骤一样把这个直接应用在我们的数据表证过程中，然后把新的表征用来训练。我们也可以促使我们的模型中的表征变得更加相似\n",
    "\n",
    "### 混淆域\n",
    "\n",
    "另一个最近变得流行的、用来确保两个域的表征之间相似性的方式就是在现有的模型上增加一个目标函数来鼓励两个域的混淆。这个域混淆的损失函数就是常规的分类损失函数，模型尝试预测输入样例的类别。然而，又和常规的损失函数有所不同，从损失函数到网络的剩余部分的流动是反向的。\n",
    "\n",
    "\n",
    "## 什么时候用迁移学习 when\n",
    "\n",
    "一般来说，在模型开发和评估之前，在领域中使用迁移学习是否有好处并不明显。\n",
    "\n",
    "不过使用迁移学习有几种可能的好处：\n",
    "\n",
    "- Higher start. 源模型上的初始skill（训练成果）(在refine模型之前)比其他情况下更高。\n",
    "- Higher slope. 在源模型的训练过程中，skill改进的速度比不使用源模型时要快。\n",
    "- Higher asymptote. 经过训练的模型的聚合skill的表现比其他方法更好。\n",
    "\n",
    "更形象地，如下图所示：\n",
    "\n",
    "![](Three-ways-in-which-transfer-might-improve-learning-1024x505.png)\n",
    "\n",
    "试试如果你能识别一个有丰富数据的相关的任务并且你有足够的资源来开发一个模型,并能重用这个模型到自己的问题,或者有一个pre-trained模型可用,那么可以使用它作为自己的一个模型起始点。\n",
    "\n",
    "在一些你可能没有太多数据的问题上，迁移学习可以使你开发出熟练的模型，如果没有迁移学习，你根本无法开发出这些模型。\n",
    "\n",
    "基于前面说的四种场景，这里看看不同情境下的应用。\n",
    "\n",
    "### Learning from simulations\n",
    "\n",
    "一个非常在将来会见到更多的迁移学习应用就是**从模拟中学习**。对很多依靠硬件来交互的机器学习应用而言，在现实世界中收集数据、训练模型，要么很昂贵，要么很耗时间。所以最好能以某些风险较小的其他方式来收集数据。\n",
    "\n",
    "模拟是针对这个问题的首选工具，在现实世界中它被用来实现很多先进的机器学习系统。从模拟中学习并将学到的知识应用在现实世界，这是迁移学习场景 2 中的实例，因为源域和目标域的特征空间是一样的（仅仅依靠像素），但是模拟和现实世界的边际概率分布是不一样的，即模拟和目标域中的物体看上去是不同的，尽管随着模拟的逐渐逼真，这种差距会消失。同时，模拟和现实世界的条件概率分布可能是不一样的，因为模拟不会完全复制现实世界中的所有反应，例如，一个物理引擎不会完全模仿现实世界中物体的交互。\n",
    "\n",
    "从模拟中学习有利于让数据收集变得更加容易，因为物体可以容易地被限制和分析，同时实现快速训练，因为学习可以在多个实例之间并行进行。因此，这是需要与现实世界进行交互的大规模机器学习项目的先决条件，例如自动驾驶汽车。谷歌无人车的技术主管 Zhaoyin Jia 说，「如果你真的想做无人驾驶车，模拟是必需的」。\n",
    "\n",
    "另一个方向是通向通用人工智能的途径，其中模拟会是一个必需的组成部分。**在现实世界中直接训练一个代理来实现通用人工智能的代价太高**，并且不必要的复杂度还会在初始的时候阻碍训练。相反，如果基于诸如CommAI-env的模拟环境的话，学习也许会更加成功.\n",
    "\n",
    "### Adapting to new domains\n",
    "\n",
    "尽管从模拟中学习是域适应的一个特例，总结一些域适应的其他例子也是有价值的。\n",
    "\n",
    "比如域适应在视觉中是一个常规的需求，因为标签信息易于获取的数据和我们实际关心的数据经常是不一样的，无论这涉及识别自行车还是自然界中的其他物体。即使训练数据和测试数据看起来是一样的，训练数据也仍然可能包含人类难以察觉的偏差，而模型能够利用这种偏差在训练数据上实现过拟合。\n",
    "\n",
    "## 相关的研究\n",
    "\n",
    "迁移学习并不是唯一一个试图利用有限的数据、在新的任务上使用学到的知识、并让模型在新环境上具有较好的泛化能力的机器学习领域。所以，在下面的内容中，我们会介绍一些其他的与迁移学习相关或者能够补充迁移学习目标的方向。\n",
    "\n",
    "### 半监督学习\n",
    "\n",
    "迁移学习力图最大效率地使用某些任务或者域中的无标签数据。这也是半监督学习所恪守的准则，半监督学习遵循经典机器学习的设定，但是它仅仅采用有限数量的标签数据来训练。如此，半监督域适应本质上就是在域变化的情况下进行半监督学习。许多来自于半监督学习的教训和思想同样地适用于迁移学习。文献 [Semi-Supervised Learning Literature Survey]() 是一个很不错的关于半监督学习的综述。\n",
    "\n",
    "### 更有效地使用可用的数据\n",
    "\n",
    "另外一个与迁移学习和半监督学习相关的方向是让模型在有限数量的数据上运行得更好。\n",
    "\n",
    "这个可以用几种方式实现：你可以使用无监督学习或者半监督学习从无标签数据中抽取信息，以减少对有标签样本的依赖；你可以允许模型能够获取一些数据的固有属性，同时减轻正则化过程中的过拟合倾向；最后，你还可以使用至今仍然被忽视或者位于不明显的地方的一些数据。\n",
    "\n",
    "作为用户生成内容的意外结果，这种巧合的数据可能会被创建，例如能够提升命名实体和词性标注的超链接；它也可能作为注释的副产品存在，例如注释器不一致（annotator disagreement) 可能改进标注或者解析；或者来源于用户行为的数据，例如视线追踪或者键入变化，可以影响自然语言处理的任务。虽然这些数据只是以有限的方式被利用，但是这样的例子鼓励我们在意外的地方查找数据，并研究检索数据的新方法。\n",
    "\n",
    "### 提高模型的泛化能力\n",
    "\n",
    "让模型更好地泛化也是与此相关的一个方向。为了实现更好的泛化能力，我们首先必须理解大规模神经网络的行为和错综复杂的结构，并且去研究它们泛化的原因和方式。最近的工作已经朝着这个目标迈出了大有希望的步伐，但是很多问题仍然等待解答。\n",
    "\n",
    "### 让模型更加稳健（robust)\n",
    "\n",
    "尽管提升我们的模型的泛化能力这方面已经比较成功了，在类似的例子上面我们也许泛化得很好，但是在出乎意料或者者非典型的输入情况下仍然会失败。所以，一个关键的补充目标就是让我们的模型更加稳健。在近来对抗学习的进步的推动下，这个方向越来越受关注，并且，最近的方法研究了很多让模型在最糟糕的情况下或者面对不同设置的对抗样本时变得更加稳健的方式\n",
    "\n",
    "### 多任务学习\n",
    "\n",
    "在迁移学习中，我们主要关心在我们的目标任务和域上友好的表现。相反，多任务学习中的目标是在所有可用的任务上都要表现良好，尽管某个标签数据通常都被假定在一个任务上。虽然多任务学习的方法没有直接用在迁移学习上，但是对多任务学习有利的关于任务的思想仍然能够指引迁移学习的决策。\n",
    "\n",
    "单任务学习VS多任务学习\n",
    "\n",
    "单任务学习：一次只学习一个任务（task），大部分的机器学习任务都属于单任务学习。\n",
    "\n",
    "多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。\n",
    "\n",
    "多任务学习（multitask learning）产生的原因？\n",
    "\n",
    "大多数机器学习任务都是单任务学习。复杂的问题，也可以分解为简单且相互独立的子问题来单独解决，然后再合并结果，得到最初复杂问题的结果。不过把现实问题当做一个个独立的单任务处理，忽略了问题之间所富含的丰富的关联信息，那么多任务就有效？\n",
    "\n",
    "多个任务之间共享一些因素，它们可以在学习过程中，共享它们所学到的信息，这是单任务学习所具备的。相关联的多任务学习比单任务学习能去的更好的泛化（generalization）效果。\n",
    "\n",
    "多任务学习时，多个任务之间的模型空间（Trained Model）是共享的。如下图：\n",
    "\n",
    "![](v2-9eed3a14f160f9562a37eafe82991b8e_hd.png)\n",
    "\n",
    "假设用含一个隐含层的神经网络来表示学习一个任务，单任务学习和多任务学习可以表示成如下图所示。\n",
    "\n",
    "![](v2-2e2316ef5678c50b3b737335f5a0d7e9_hd.png)\n",
    "\n",
    "多任务学习时，多个任务之间的浅层表示共享（shared representation）。\n",
    "\n",
    "### 持续学习\n",
    "\n",
    "虽然多任务学习允许我们在许多任务中保留知识，而不会对我们的源任务造成性能损失，但只有在所有任务都处于训练时间的情况下，这才是可能的。对于每个新任务，我们通常需要重新训练我们所有任务的模型。\n",
    "\n",
    "然而，在现实世界中，我们希望一个代理能够通过使用它以往的一些经验来处理逐渐变得复杂的任务。为了达到这个目的，我们需要让一个模型在不忘记的情况下持续地学习。这个机器学习的领域被称为学会学习、元学习、终生学习，或者持续学习。持续学习在最近的强化学习 (强化学习以 Google DeepMind 对通用学习代理的探索而著称) 上已经取得了成功，也正在被用于序列到序列的模型上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
