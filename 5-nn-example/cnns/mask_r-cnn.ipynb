{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标检测 Mask R-CNN\n",
    "\n",
    "本文主要参考了pytorch官方文档：[TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)，了解目标检测的实践，后面会有能用到本专业内容上的。关于算法方面的基础只是可以参考elks项目中的记录。\n",
    "\n",
    "在Penn-Fudan数据库中对行人检测和分割的预训练Mask R-CNN模型进行微调。它包含170张图像，其中包含345个行人实例，我们将用它来说明如何在torchvision中使用新功能，以便在自定义数据集上训练实例细分模型。\n",
    "\n",
    "## 定义数据集\n",
    "\n",
    "用于训练对象检测，实例细分和人员关键点检测的参考脚本可轻松支持添加新的自定义数据集。数据集应继承自标准 torch.utils.data.Dataset类，并实现__len__和 __getitem__。\n",
    "\n",
    "我们唯一需要的特性是数据集__getitem__ 应该返回：\n",
    "\n",
    "- 图像：大小为(H, W)的PIL的图像 \n",
    "- 目标：包含以下字段的dict：\n",
    "    - boxes (FloatTensor[N, 4])：N 个边界框的坐标，格式为[x0, y0, x1, y1]，范围从 0到W，0到H\n",
    "    - labels (Int64Tensor[N])：每个边界框的标签\n",
    "    - image_id (Int64Tensor[1])：图像标识符。它在数据集中的所有图像之间应该是唯一的，并在评估过程中使用\n",
    "    - area (Tensor[N])：边界框的面积。在使用COCO度量进行评估时，可使用此值来区分小，中和大boxes之间的度量得分。\n",
    "    - iscrowd (UInt8Tensor[N])：iscrowd = True的实例在 evaluation 期间将被忽略。\n",
    "    - （可选）masks (UInt8Tensor[N, H, W])：每个对象的分割蒙版\n",
    "    - （可选）keypoints (FloatTensor[N, K, 3])：对于N个对象中的每个对象，它包含格式为[x, y, visibility]的K个关键点，用于定义对象。visibility= 0表示关键点不可见。请注意，对于数据扩充，翻转关键点的概念取决于数据表示形式，您可能应该使用references/detection/transforms.py来适应新的关键点表示形式\n",
    "    \n",
    "如果模型返回上述方法，则它们将使其适用于训练和评估，并将使用pycocotools中的评估脚本。\n",
    "\n",
    "此外，如果要在训练过程中使用宽高比分组（以便每个批次仅包含具有相似长宽比的图像），则建议实现一种get_height_and_width 方法，该方法可返回图像的高度和宽度。如果未提供此方法，那么将通过 __getitem__ 查询数据集的所有元素，这会将图像加载到内存中，使得比提供自定义方法时要慢。\n",
    "\n",
    "那么接下来就为PennFudan编写自定义数据集。\n",
    "\n",
    "先[下载](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip)并解压缩zip文件，放到 5-nn-example\\cnns\\data 文件夹下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "在本教程中，我们将使用基于 Faster R-CNN的Mask R-CNN。Faster R-CNN是预测 图像中潜在对象的边界框bounding boxes 和类分数class scores 的模型。\n",
    "\n",
    "Mask R-CNN在Faster R-CNN中增加了一个分支，可以预测每个实例的分割掩码。更多简介内容可以参考elks项目中的记录。\n",
    "\n",
    "在两种常见情况下，可能要修改Torchvision modelzoo中的可用模型之一。首先是当我们想从预训练模型开始，然后微调最后一层。另一个是当我们要用另一个模型替换主干时（例如，为了更快的预测）。\n",
    "\n",
    "接下来让我们看看如何做。\n",
    "\n",
    "1. 通过预训练模型进行微调：假设您想从在COCO上进行过预训练的模型开始，并希望针对您的特定班级对其进行微调。这是一种可行的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
