{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Requests等基本常用库\n",
    "\n",
    "这部分内容主要参考了：[Python-crawler](https://github.com/Ehco1996/Python-crawler)和[Python3网络爬虫开发实战](https://python3webspider.cuiqingcai.com/)。\n",
    "\n",
    "## Request\n",
    "\n",
    "概念性的内容，可直接参考《Python3网络爬虫开发实战》，这里直接从实战开始，从最简单的requests库的使用开始记录。也包括一些其他基本库的使用。\n",
    "\n",
    "### 安装requests库\n",
    "\n",
    "直接使用pip安装requests即可。\n",
    "\n",
    "``` python\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "然后检查一下是否安装了beautifulsoup4。\n",
    "\n",
    "``` python\n",
    "pip list\n",
    "```\n",
    "\n",
    "### requests基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">ç»å½</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å",
      "³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿",
      "è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#首先我们先导入requests这个包\n",
    "import requests\n",
    "\n",
    "#我们来吧百度的index页面的html源码抓取到本地，并用r变量保存\n",
    "#注意这里，网页前面的 http://一定要写出来，它并不能像真正的浏览器一样帮我们补全http协议\n",
    "r = requests.get(\"http://www.baidu.com\")\n",
    "\n",
    "#将下载到的内容打印一下：\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上面的抓取过程中，我们用到了requests库的**get方法**，这个方法是requests库中最常用的方法之一。他接受一个参数（url）并返回一个**HTTP response对象**。\n",
    "\n",
    "### get方法\n",
    "\n",
    "这个方法可以接收三个参数，其中第二个默认为None，params是字典或字节序列，作为参数增加到url中的，第三个可选，包括设置HTTP定制头headers等：def get(url, params=None, **kwargs)。\n",
    "\n",
    "get返回的是Response对象。是一个完整的对HTTP request的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>\n",
      "BDORZ=27315\n",
      "{\n",
      "  \"cookies\": {}\n",
      "}\n",
      "\n",
      "{\n",
      "  \"cookies\": {\n",
      "    \"number\": \"123456789\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def getHtmlText(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        # 如果状态码不是200 则应发HTTOError异常\n",
    "        r.raise_for_status()\n",
    "        # 设置正确的编码方式\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"Something Wrong!\"\n",
    "    \n",
    "r = requests.get('https://www.baidu.com')\n",
    "print(r.cookies)\n",
    "for key, target_directory in r.cookies.items():\n",
    "    print(key + '=' + target_directory)\n",
    "\n",
    "requests.get('http://httpbin.org/cookies/set/number/123456789')\n",
    "r = requests.get('http://httpbin.org/cookies')\n",
    "print(r.text)\n",
    "\n",
    "s = requests.Session()\n",
    "s.get('http://httpbin.org/cookies/set/number/123456789')\n",
    "r = s.get('http://httpbin.org/cookies')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "200\n",
      "<class 'str'>\n",
      "<!DOCTYPE html>\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class=\"bg s_btn\" autofocus></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=https://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">ç»å½</a>');\n",
      "                </script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å",
      "³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿",
      "è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\n",
      "\n",
      "<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://www.baidu.com/')\n",
    "print(type(r))\n",
    "print(r.status_code)\n",
    "print(type(r.text))\n",
    "print(r.text)\n",
    "print(r.cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "<re.Match object; span=(0, 25), match='Hello 123 4567 World_This'>\n",
      "Hello 123 4567 World_This\n",
      "(0, 25)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "content = 'Hello 123 4567 World_This is a Regex Demo'\n",
    "print(len(content))\n",
    "result = re.match('^Hello\\s\\d\\d\\d\\s\\d{4}\\s\\w{10}', content)\n",
    "print(result)\n",
    "print(result.group())\n",
    "print(result.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib\n",
    "\n",
    "使用urllib库也是一个常用的发送请求获取数据的方式。参考[urllib](https://www.liaoxuefeng.com/wiki/1016959663602400/1019223241745024)，更多细节可以参考[HOWTO 使用 urllib 包获取网络资源](https://docs.python.org/zh-cn/3/howto/urllib2.html).\n",
    "\n",
    "urllib提供了一系列用于操作URL的功能。\n",
    "\n",
    "其中最基本的urllib.request 是一个用于获取 URL （统一资源定位地址）的 Python 模块。它以 urlopen 函数的形式提供了一个非常简单的接口。该接口能够使用不同的协议获取 URL。同时它也提供了一个略微复杂的接口来处理常见情形——如：基本验证、cookies、代理等等。这些功能是通过叫做 handlers 和 opener 的对象来提供的。\n",
    "\n",
    "urllib.request 支持多种 \"URL 网址方案\" （通过 URL中 \":\" 之前的字符串加以区分——如 URL 地址 \"ftp://python.org/\"` 中的 ``\"ftp\"`） ，使用与之相关的网络协议（如：FTP、 HTTP）来获取 URL 资源。\n",
    "\n",
    "urllib的request模块可以非常方便地抓取URL内容，也就是发送一个GET请求到指定的页面，然后返回HTTP的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n",
      "200\n",
      "[('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'DENY'), ('Via', '1.1 vegur'), ('Via', '1.1 varnish'), ('Content-Length', '48781'), ('Accept-Ranges', 'bytes'), ('Date', 'Mon, 10 Feb 2020 21:09:04 GMT'), ('Via', '1.1 varnish'), ('Age', '2417'), ('Connection', 'close'), ('X-Served-By', 'cache-iad2139-IAD, cache-ewr18144-EWR'), ('X-Cache', 'HIT, HIT'), ('X-Cache-Hits', '2, 6'), ('X-Timer', 'S1581368945.633406,VS0,VE0'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]\n",
      "nginx\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# 以 Python 官网为例，我们来把这个网页抓下来：\n",
    "response = urllib.request.urlopen('https://www.python.org')\n",
    "# print(response.read().decode('utf-8'))\n",
    "\n",
    "# 利用 type() 方法输出 Response 的类型。通过输出结果可以发现它是一个 HTTPResposne 类型的对象\n",
    "print(type(response))\n",
    "\n",
    "# 得到这个对象之后，我们把它赋值为 response 变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。\n",
    "print(response.status)\n",
    "print(response.getheaders())\n",
    "print(response.getheader('Server'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要以POST发送一个请求，只需要把参数data以bytes形式传入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"args\": {}, \\n  \"data\": \"\", \\n  \"files\": {}, \\n  \"form\": {\\n    \"word\": \"hello\"\\n  }, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Content-Length\": \"10\", \\n    \"Content-Type\": \"application/x-www-form-urlencoded\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.8\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e41c672-95281b905c5664982c0cbab9\"\\n  }, \\n  \"json\": null, \\n  \"origin\": \"130.203.206.37\", \\n  \"url\": \"http://httpbin.org/post\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')\n",
    "response = urllib.request.urlopen('http://httpbin.org/post', data=data)\n",
    "print(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"args\": {}, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.7\"\\n  }, \\n  \"origin\": \"104.39.105.125, 104.39.105.125\", \\n  \"url\": \"https://httpbin.org/get\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)\n",
    "# 在这里我们设置了超时时间是 1 秒，程序 1 秒过后服务器依然没有响应，于是抛出了 URLError 异常，它属于 urllib.error 模块，错误原因是超时。\n",
    "print(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以利用更强大的 Request 类来构建一个请求，get和post的例子如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Germey\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"11\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"104.39.105.125, 104.39.105.125\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# get\n",
    "request = urllib.request.Request('https://python.org')\n",
    "response = urllib.request.urlopen(request)\n",
    "# print(response.read().decode('utf-8'))\n",
    "\n",
    "# post\n",
    "url = 'http://httpbin.org/post'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    'Host': 'httpbin.org'\n",
    "}\n",
    "dict = {\n",
    "    'name': 'Germey'\n",
    "}\n",
    "data = bytes(urllib.parse.urlencode(dict), encoding='utf8')\n",
    "req = urllib.request.Request(url=url, data=data, headers=headers, method='POST')\n",
    "response = urllib.request.urlopen(req)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib还有更强大的工具 Handler。我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情\n",
    "\n",
    "比如调用 urlopen() 的对象的更普遍的对象，也就是 Opener。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 111] Connection refused\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener\n",
    "from urllib.error import URLError\n",
    "\n",
    "# 输入用户名和密码，认证成功之后才能查看页面\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "url = 'http://localhost:5000/'\n",
    "\n",
    "p = HTTPPasswordMgrWithDefaultRealm()\n",
    "p.add_password(None, url, username, password)\n",
    "auth_handler = HTTPBasicAuthHandler(p)\n",
    "opener = build_opener(auth_handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(url)\n",
    "    html = result.read().decode('utf-8')\n",
    "    print(html)\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果还需要更复杂的控制，比如通过一个Proxy去访问网站，我们需要利用ProxyHandler来处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 111] Connection refused\n"
     ]
    }
   ],
   "source": [
    "# 在做爬虫的时候免不了要使用代理，如果要添加代理，可以这样做：\n",
    "from urllib.error import URLError\n",
    "from urllib.request import ProxyHandler, build_opener\n",
    "\n",
    "proxy_handler = ProxyHandler({\n",
    "    'http': 'http://127.0.0.1:9743',\n",
    "    'https': 'https://127.0.0.1:9743'\n",
    "})\n",
    "opener = build_opener(proxy_handler)\n",
    "try:\n",
    "    response = opener.open('https://www.baidu.com')\n",
    "    print(response.read().decode('utf-8'))\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cookies 的处理就需要 Cookies 相关的 Handler 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAIDUID=AA3AF6F88F1BF72D38745CF5DB673EFB:FG=1\n",
      "BIDUPSID=AA3AF6F88F1BF72D80CC3C04CD895F38\n",
      "H_PS_PSSID=1445_21078_30210_26350\n",
      "PSTM=1576536117\n",
      "delPer=0\n",
      "BDSVRTM=0\n",
      "BD_HOME=0\n"
     ]
    }
   ],
   "source": [
    "import http.cookiejar, urllib.request\n",
    "\n",
    "cookie = http.cookiejar.CookieJar()\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "response = opener.open('http://www.baidu.com')\n",
    "for item in cookie:\n",
    "    print(item.name+\"=\"+item.value)\n",
    "filename = './dataset/cookies.txt'\n",
    "cookie = http.cookiejar.MozillaCookieJar(filename)\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "response = opener.open('http://www.baidu.com')\n",
    "cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请求出错可以使用URLError处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, error\n",
    "try:\n",
    "    response = request.urlopen('http://cuiqingcai.com/index.htm')\n",
    "except error.URLError as e:\n",
    "    print(e.reason)\n",
    "\n",
    "\n",
    "\n",
    "from urllib import request,error\n",
    "try:\n",
    "    response = request.urlopen('http://cuiqingcai.com/index.htm')\n",
    "except error.HTTPError as e:\n",
    "    print(e.reason, e.code, e.headers, seq='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件下载\n",
    "\n",
    "文件下载也可以方便地使用urllib实现。接下来先给出一个直接下载zip文件并解压的例子，参考CSDN博主「Kenn7」的原创文章，原文链接：https://blog.csdn.net/kane7csdn/article/details/84025393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https:', '', 'archive.ics.uci.edu', 'ml', 'machine-learning-databases', '00240', 'UCI%20HAR%20Dataset.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "import zipfile\n",
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
    "DATA_DIR = './dataset/'\n",
    "DATA_NAME = 'UCI HAR Dataset.zip'\n",
    " \n",
    "filename = DATA_NAME\n",
    "filepath = os.path.join(DATA_DIR, filename)\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "filepath, _ = urllib.request.urlretrieve(DATA_URL, DATA_DIR)\n",
    "\n",
    "with zipfile.ZipFile(filepath, 'r') as zip:\n",
    "    zip.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据CSDN博主「YZXnuaa」的[原创文章](https://blog.csdn.net/YZXnuaa/article/details/91657379)所述，在url中，是只能使用ASCII中包含的字符的，也就是说，ASCII不包含的特殊字符，以及中文等字符都是不可以在url中使用的。而我们有时候又有将中文字符加入到url中的需求，例如百度的搜索地址：\n",
    "\n",
    "https://www.baidu.com/s?wd=周杰伦\n",
    "\n",
    "？之后的wd参数，则是我们搜索的关键词。那么我们实现的方法就是将特殊字符进行url编码，转换成可以url可以传输的格式，urllib中可以使用quote()方法来实现这个功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%E5%91%A8%E6%9D%B0%E4%BC%A6'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import parse\n",
    "keyword = '周杰伦'\n",
    "parse.quote(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要将编码后的数据转换回来，可以使用unquote()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'周杰伦'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.unquote('%E5%91%A8%E6%9D%B0%E4%BC%A6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以如果不想要手动指定文件名称的话，可以采用下面的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https:', '', 'archive.ics.uci.edu', 'ml', 'machine-learning-databases', '00240', 'UCI%20HAR%20Dataset.zip']\n",
      "UCI HAR Dataset.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "import zipfile\n",
    "from urllib import parse\n",
    "\n",
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
    "DATA_DIR = './dataset/'\n",
    "data_url_str=DATA_URL.split('/')\n",
    "print(data_url_str)\n",
    "\n",
    "DATA_NAME = parse.unquote(data_url_str[-1])\n",
    "print(DATA_NAME)\n",
    "\n",
    "filename = DATA_NAME\n",
    "filepath = os.path.join(DATA_DIR, filename)\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath)\n",
    "\n",
    "with zipfile.ZipFile(filepath, 'r') as zip:\n",
    "    zip.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述例子可以看到下载还是比较简单的，不过下载的情况是多种多样的，比如当文件较大时，使用上述方法可能导致内存不够，因此需要采用每次只下载小部分数据的方式。这里就各类下载文件的方式进行汇总记录，参考：[用python下载文件的若干种方法汇总](https://www.zhangqibot.com/post/crawler-downloader/)\n",
    "\n",
    "### requests标准模板\n",
    "\n",
    "这一小节以及接下来用到requests的地方参考了[python下载文件----requests](https://zhuanlan.zhihu.com/p/37824910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取失败...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url=\"******\"\n",
    "try:\n",
    "    r=requests.get(url)\n",
    "    r.raise_for_status()  #如果不是200，产生异常requests.HTTPError\n",
    "    r.encoding=r.apparent_encoding\n",
    "    print(r.text)\n",
    "except:\n",
    "    print(\"爬取失败...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "image_url = \"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\"\n",
    "r = requests.get(image_url) \n",
    "with open(\"./dataset/python_logo.png\",'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载图片\n",
    "\n",
    "直接使用requests或者使用wget均可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15770"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://www.python.org/static/img/python-logo@2x.png'\n",
    "myfile = requests.get(url)\n",
    "open('./dataset/PythonImage.png', 'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dataset/pythonLogo.png'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "url = \"https://www.python.org/static/img/python-logo@2x.png\"\n",
    "wget.download(url, './dataset/pythonLogo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载重定向的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4889945"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://readthedocs.org/projects/python-guide/downloads/pdf/latest/'\n",
    "myfile = requests.get(url, allow_redirects=True)\n",
    "open('./dataset/hello.pdf', 'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分块下载大文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://buildmedia.readthedocs.org/media/pdf/python-guide/latest/python-guide.pdf'\n",
    "r = requests.get(url, stream = True)\n",
    "with open(\"./dataset/PythonBook.pdf\", \"wb\") as Pypdf:\n",
    "    for chunk in r.iter_content(chunk_size = 1024): # 1024 bytes\n",
    "        if chunk:\n",
    "            Pypdf.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行下载多文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to download: 1.5507481098175049\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from time import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "def url_response(url):\n",
    "    path, url = url\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        for ch in r:\n",
    "            f.write(ch)\n",
    "        \n",
    "urls = [(\"./dataset/Event1\", \"https://www.python.org/events/python-events/805/\"),\n",
    "        (\"./dataset/Event2\", \"https://www.python.org/events/python-events/801/\"),\n",
    "        (\"./dataset/Event3\", \"https://www.python.org/events/python-events/790/\"),\n",
    "        (\"./dataset/Event4\", \"https://www.python.org/events/python-events/798/\"),\n",
    "        (\"./dataset/Event5\", \"https://www.python.org/events/python-events/807/\"),\n",
    "        (\"./dataset/Event6\", \"https://www.python.org/events/python-events/807/\"),\n",
    "        (\"./dataset/Event7\", \"https://www.python.org/events/python-events/757/\"),\n",
    "        (\"./dataset/Event8\", \"https://www.python.org/events/python-user-group/816/\")]\n",
    "\n",
    "start = time()\n",
    "for x in urls:\n",
    "    url_response(x)\n",
    "    \n",
    "print(f\"Time to download: {time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并行版本，只需改动一行代码ThreadPool(9).imap_unordered(url_response, urls)，时间会大幅度减少："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to download: 0.010510683059692383\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from time import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "def url_response(url):\n",
    "    path, url = url\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        for ch in r:\n",
    "            f.write(ch)\n",
    "            \n",
    "urls = [(\"./dataset/Event1\", \"https://www.python.org/events/python-events/805/\"),\n",
    "        (\"./dataset/Event2\", \"https://www.python.org/events/python-events/801/\"),\n",
    "        (\"./dataset/Event3\", \"https://www.python.org/events/python-events/790/\"),\n",
    "        (\"./dataset/Event4\", \"https://www.python.org/events/python-events/798/\"),\n",
    "        (\"./dataset/Event5\", \"https://www.python.org/events/python-events/807/\"),\n",
    "        (\"./dataset/Event6\", \"https://www.python.org/events/python-events/807/\"),\n",
    "        (\"./dataset/Event7\", \"https://www.python.org/events/python-events/757/\"),\n",
    "        (\"./dataset/Event8\", \"https://www.python.org/events/python-user-group/816/\")]\n",
    "\n",
    "start = time()\n",
    "ThreadPool(9).imap_unordered(url_response, urls)\n",
    "print(f\"Time to download: {time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实实际中下载用的最多最灵活的应该还是wget了，注意在windows下需要安装wget软件。\n",
    "\n",
    "### wget下载\n",
    "\n",
    "python的wget模块也可以方便的下载文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dataset/pythonLogo (1).png'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    " \n",
    "url = \"https://www.python.org/static/img/python-logo@2x.png\"\n",
    " \n",
    "wget.download(url, './dataset/pythonLogo.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
