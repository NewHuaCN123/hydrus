{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯优化\n",
    "\n",
    "本文主要记录[BayesianOptimization](https://github.com/fmfn/BayesianOptimization)库的使用。内容结构都与原文档一致。安装方法：\n",
    "\n",
    "```Shell\n",
    "conda install -c conda-forge bayesian-optimization\n",
    "```\n",
    "\n",
    "## 基本使用\n",
    "\n",
    "贝叶斯优化是对计算耗费大的函数进行优化时一种常用的方法，这里先给出一个贝叶斯优化的示例程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-7.135   \u001b[0m | \u001b[0m 2.834   \u001b[0m | \u001b[0m 1.322   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-7.78    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-1.186   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-16.13   \u001b[0m | \u001b[0m 2.294   \u001b[0m | \u001b[0m-2.446   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-8.341   \u001b[0m | \u001b[0m 2.373   \u001b[0m | \u001b[0m-0.9266  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-7.392   \u001b[0m | \u001b[0m 2.794   \u001b[0m | \u001b[0m 0.2329  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-7.069   \u001b[0m | \u001b[95m 2.838   \u001b[0m | \u001b[95m 1.111   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-6.412   \u001b[0m | \u001b[95m 2.409   \u001b[0m | \u001b[95m 2.269   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-3.223   \u001b[0m | \u001b[95m 2.055   \u001b[0m | \u001b[95m 1.023   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-7.455   \u001b[0m | \u001b[0m 2.835   \u001b[0m | \u001b[0m 0.3521  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-12.11   \u001b[0m | \u001b[0m 2.281   \u001b[0m | \u001b[0m-1.811   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m-3.071   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 1.266   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-3.671   \u001b[0m | \u001b[0m 2.151   \u001b[0m | \u001b[0m 1.211   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-3.041   \u001b[0m | \u001b[95m 2.006   \u001b[0m | \u001b[95m 1.127   \u001b[0m |\n",
      "=================================================\n",
      "{'target': -3.041430390491576, 'params': {'x': 2.006340577135143, 'y': 1.1266012599960877}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "\n",
    "def black_box_function(x, y):\n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "\n",
    "    This is just serving as an example, for all intents and\n",
    "    purposes think of the internals of this function, i.e.: the process\n",
    "    which generates its output values, as unknown.\n",
    "    \"\"\"\n",
    "    return -x ** 2 - (y - 1) ** 2 + 1\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'x': (2, 4), 'y': (-3, 3)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=10,\n",
    "    n_iter=3,\n",
    ")\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的代码可以看出直接在BayesianOptimization初始化时指定f为要优化的函数即可，生成 optimizer 即生成了贝叶斯优化类的对象，接下来调用它的maximize函数即可进行寻优。init_points从外围来看，是初始的迭代次数，后面n_iter是后来的迭代次数。\n",
    "\n",
    "接着可以从 optimizer 对象输出中间的迭代结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------各次迭代的结果------------------------------\n",
      "Iteration 0: \n",
      "\t{'target': -7.135455292718879, 'params': {'x': 2.8340440094051482, 'y': 1.3219469606529488}}\n",
      "Iteration 1: \n",
      "\t{'target': -7.779531005607566, 'params': {'x': 2.0002287496346898, 'y': -1.1860045642089614}}\n",
      "Iteration 2: \n",
      "\t{'target': -16.134894722612252, 'params': {'x': 2.2935117816342263, 'y': -2.445968431387213}}\n",
      "Iteration 3: \n",
      "\t{'target': -8.340778037007604, 'params': {'x': 2.3725204227553416, 'y': -0.9266356377417138}}\n",
      "Iteration 4: \n",
      "\t{'target': -7.392279298427363, 'params': {'x': 2.79353494846134, 'y': 0.23290040402014167}}\n",
      "Iteration 5: \n",
      "\t{'target': -7.068843753868606, 'params': {'x': 2.8383890288065894, 'y': 1.1113170023805568}}\n",
      "Iteration 6: \n",
      "\t{'target': -6.412432296144895, 'params': {'x': 2.408904499463035, 'y': 2.268704618345673}}\n",
      "Iteration 7: \n",
      "\t{'target': -3.2226211374385345, 'params': {'x': 2.0547751863958523, 'y': 1.0228050610704136}}\n",
      "Iteration 8: \n",
      "\t{'target': -7.454735524570369, 'params': {'x': 2.834609604734254, 'y': 0.3521389706745097}}\n",
      "Iteration 9: \n",
      "\t{'target': -12.105849201995525, 'params': {'x': 2.2807738771904678, 'y': -1.8113910654907273}}\n",
      "Iteration 10: \n",
      "\t{'target': -3.0705526422294334, 'params': {'x': 2.0, 'y': 1.2656174735017136}}\n",
      "Iteration 11: \n",
      "\t{'target': -3.670763101613826, 'params': {'x': 2.1508503398639527, 'y': 1.211201129544671}}\n",
      "Iteration 12: \n",
      "\t{'target': -3.0168771356940622, 'params': {'x': 2.0, 'y': 1.1299120305978725}}\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------各次迭代的结果------------------------------\")\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以寻优过程中改变寻优边界，来重新继续寻优，还可以指定参数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------寻优过程中改变寻优边界-------------------------------------\n",
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m-1.651   \u001b[0m | \u001b[95m 1.618   \u001b[0m | \u001b[95m 1.186   \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m-0.8391  \u001b[0m | \u001b[95m 1.356   \u001b[0m | \u001b[95m 1.023   \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m-0.2766  \u001b[0m | \u001b[95m 1.116   \u001b[0m | \u001b[95m 1.174   \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.2046  \u001b[0m | \u001b[95m 0.8918  \u001b[0m | \u001b[95m 1.011   \u001b[0m |\n",
      "| \u001b[95m 18      \u001b[0m | \u001b[95m 0.5723  \u001b[0m | \u001b[95m 0.6385  \u001b[0m | \u001b[95m 1.142   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------寻优过程中改变寻优边界-------------------------------------\")\n",
    "# 可以在寻优计算过程中改变寻优边界\n",
    "optimizer.set_bounds(new_bounds={\"x\": (-2, 3)})\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=0,\n",
    "    n_iter=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------寻优过程中探索特定参数值-------------------------------------\n",
      "['x', 'y']\n",
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.66    \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.7     \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.66    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7     \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m-0.3     \u001b[0m | \u001b[0m 0.1     \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------寻优过程中探索特定参数值-------------------------------------\")\n",
    "# 指定要探索的参数值，lazy表示下面maximize时执行\n",
    "optimizer.probe(\n",
    "    params={\"x\": 0.5, \"y\": 0.7},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "print(optimizer.space.keys)\n",
    "optimizer.probe(\n",
    "    params=[-0.3, 0.1],\n",
    "    lazy=True,\n",
    ")\n",
    "optimizer.maximize(init_points=0, n_iter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer对象的寻优过程也可以记录下来，注意optimizer对象要先subscribe，然后再maximize才会记录下日志："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------存储与加载寻优过程记录-------------------------------------\n",
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-6.289   \u001b[0m | \u001b[0m 2.004   \u001b[0m | \u001b[0m 2.81    \u001b[0m |\n",
      "| \u001b[95m 20      \u001b[0m | \u001b[95m 0.7889  \u001b[0m | \u001b[95m-0.4329  \u001b[0m | \u001b[95m 1.154   \u001b[0m |\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m 0.9917  \u001b[0m | \u001b[95m 0.007159\u001b[0m | \u001b[95m 0.9089  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.9026  \u001b[0m | \u001b[0m-0.0179  \u001b[0m | \u001b[0m 1.312   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9762  \u001b[0m | \u001b[0m-0.1296  \u001b[0m | \u001b[0m 1.084   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------存储与加载寻优过程记录-------------------------------------\")\n",
    "# 用JSONLogger存储、加载计算过程\n",
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTMIZATION_STEP, logger)\n",
    "# Results will be saved in ./logs.json\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，会生成一个logs.json文件。该文件可以在后续过程中加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------加载之前存储在./logs.json中的计算过程------------------------\n",
      "0\n",
      "<bayes_opt.bayesian_optimization.BayesianOptimization object at 0x000001DAD023D948>\n",
      "New optimizer is now aware of 5 points.\n",
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.334   \u001b[0m | \u001b[0m-0.1494  \u001b[0m | \u001b[0m-0.5203  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.454   \u001b[0m | \u001b[0m-1.567   \u001b[0m | \u001b[0m 2.0     \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.3436  \u001b[0m | \u001b[0m 0.7821  \u001b[0m | \u001b[0m 0.7884  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-5.139   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-0.4626  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.674   \u001b[0m | \u001b[0m 0.508   \u001b[0m | \u001b[0m 1.261   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.1656  \u001b[0m | \u001b[0m-0.7715  \u001b[0m | \u001b[0m 0.5109  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-12.0    \u001b[0m | \u001b[0m-2.0     \u001b[0m | \u001b[0m-2.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6889  \u001b[0m | \u001b[0m-0.08321 \u001b[0m | \u001b[0m 0.4485  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8517  \u001b[0m | \u001b[0m 0.3511  \u001b[0m | \u001b[0m 0.8419  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8457  \u001b[0m | \u001b[0m-0.3256  \u001b[0m | \u001b[0m 0.7802  \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------加载之前存储在./logs.json中的计算过程------------------------\")\n",
    "new_optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds={\"x\": (-2, 2), \"y\": (-2, 2)},\n",
    "    verbose=2,\n",
    "    random_state=7,\n",
    ")\n",
    "print(len(new_optimizer.space))\n",
    "\n",
    "logs=load_logs(new_optimizer, logs=[\"./logs.json\"]);\n",
    "print(logs)\n",
    "print(\"New optimizer is now aware of {} points.\".format(len(new_optimizer.space)))\n",
    "\n",
    "new_optimizer.maximize(\n",
    "    init_points=0,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级用法\n",
    "\n",
    "接下来是一些稍微高级一些的用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"advanced 操作\"\"\"\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "\n",
    "# Let's start by definying our function, bounds, and instanciating an optimization object.\n",
    "def black_box_function(x, y):\n",
    "    \"\"\"定义目标函数\"\"\"\n",
    "    return -x ** 2 - (y - 1) ** 2 + 1\n",
    "\n",
    "\n",
    "# 实例化bo\n",
    "optimizer = BayesianOptimization(\n",
    "    f=None,\n",
    "    pbounds={'x': (-2, 2), 'y': (-3, 3)},\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# 使用效用函数\n",
    "utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)\n",
    "\n",
    "next_point_to_probe = optimizer.suggest(utility)\n",
    "print(\"Next point to probe is:\", next_point_to_probe)\n",
    "\n",
    "target = black_box_function(**next_point_to_probe)\n",
    "print(\"Found the target value to be:\", target)\n",
    "\n",
    "optimizer.register(\n",
    "    params=next_point_to_probe,\n",
    "    target=target,\n",
    ")\n",
    "\n",
    "\n",
    "# 下面是处理离散参数的手段。 “//”运算符是向下取整的操作\n",
    "def func_with_discrete_params(x, y, d):\n",
    "    # Simulate necessity of having d being discrete.\n",
    "    assert type(d) == int\n",
    "\n",
    "    return ((x + y + d) // (1 + d)) / (1 + (x + y) ** 2)\n",
    "\n",
    "\n",
    "def function_to_be_optimized(x, y, w):\n",
    "    \"\"\"w是要离散化的，用int强行离散化了\"\"\"\n",
    "    d = int(w)\n",
    "    return func_with_discrete_params(x, y, d)\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=function_to_be_optimized,\n",
    "    pbounds={'x': (-10, 10), 'y': (-10, 10), 'w': (0, 5)},\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(alpha=1e-3)\n",
    "\n",
    "# 调整高斯回归\n",
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds={'x': (-2, 2), 'y': (-3, 3)},\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "optimizer.maximize(\n",
    "    init_points=1,\n",
    "    n_iter=5,\n",
    "    # What follows are GP regressor parameters\n",
    "    alpha=1e-3,\n",
    "    n_restarts_optimizer=5\n",
    ")\n",
    "optimizer.set_gp_params(normalize_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化过程\n",
    "\n",
    "接下来可视化过程来看看如何进行优选的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"bo优化过程的可视化\"\"\"\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "def target(x):\n",
    "    \"\"\"在优化里，第一步都是优化目标的确定\"\"\"\n",
    "    return np.exp(-(x - 2) ** 2) + np.exp(-(x - 6) ** 2 / 10) + 1 / (x ** 2 + 1)\n",
    "\n",
    "\n",
    "x = np.linspace(-2, 10, 10000).reshape(-1, 1)\n",
    "y = target(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "# 然后是构建BO对象，构建对象时需要目标函数，以及参数的取值范围，即定义要探索的参数空间.random_state是伪随机数生成器的种子\n",
    "optimizer = BayesianOptimization(target, {'x': (-2, 10)}, random_state=27)\n",
    "\n",
    "# maximize是执行寻优的函数，设置的参数包括随机探索的步数init_points,bo要执行的步数n_iter,以及提取函数权衡exploitation和exploration的参数kappa\n",
    "optimizer.maximize(init_points=2, n_iter=0, kappa=5)\n",
    "\n",
    "\n",
    "def posterior(optimizer, x_obs, y_obs, grid):\n",
    "    \"\"\"目标函数的函数（高斯过程）的后验分布\"\"\"\n",
    "    optimizer._gp.fit(x_obs, y_obs)\n",
    "    # 返回的mu和sigma应该是高斯分布的均值和方差平方根\n",
    "    mu, sigma = optimizer._gp.predict(grid, return_std=True)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def plot_gp(optimizer, x, y):\n",
    "    \"\"\"可视化高斯过程\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    steps = len(optimizer.space)\n",
    "    fig.suptitle(\n",
    "        'Gaussian Process and Utility Function After {} Steps'.format(steps),\n",
    "        fontdict={'size': 30}\n",
    "    )\n",
    "\n",
    "    # 划分图形绘制的模块位置,2行1列\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "    # gs[0]函数寻优绘制到上面，另一个提取函数相关的绘制在下面。\n",
    "    axis = plt.subplot(gs[0])\n",
    "    acq = plt.subplot(gs[1])\n",
    "\n",
    "    x_obs = np.array([[res[\"params\"][\"x\"]] for res in optimizer.res])\n",
    "    y_obs = np.array([res[\"target\"] for res in optimizer.res])\n",
    "    # 求出新的mu和sigma\n",
    "    mu, sigma = posterior(optimizer, x_obs, y_obs, x)\n",
    "    axis.plot(x, y, linewidth=3, label='Target')\n",
    "    axis.plot(x_obs.flatten(), y_obs, 'D', markersize=8, label=u'Observations', color='r')\n",
    "    axis.plot(x, mu, '--', color='k', label='Prediction')\n",
    "    # 绘制%95置信度的空间，就是均值附近1.96倍sigma\n",
    "    axis.fill(np.concatenate([x, x[::-1]]),\n",
    "              np.concatenate([mu - 1.9600 * sigma, (mu + 1.9600 * sigma)[::-1]]),\n",
    "              alpha=.6, fc='c', ec='None', label='95% confidence interval')\n",
    "    # 坐标方面相关的设置\n",
    "    axis.set_xlim((-2, 10))\n",
    "    axis.set_ylim((None, None))\n",
    "    axis.set_ylabel('f(x)', fontdict={'size': 20})\n",
    "    axis.set_xlabel('x', fontdict={'size': 20})\n",
    "    # 效用函数（也就是acquisition function），用的是ucb，这个在几个常用的提取函数里简单，但是却比较好用.ucb用搞不到xi\n",
    "    utility_function = UtilityFunction(kind=\"ucb\", kappa=5, xi=0)\n",
    "    # 调用效益函数进行计算\n",
    "    utility = utility_function.utility(x, optimizer._gp, 0)\n",
    "    acq.plot(x, utility, label='Utility Function', color='purple')\n",
    "    acq.plot(x[np.argmax(utility)], np.max(utility), '*', markersize=15,\n",
    "             label=u'Next Best Guess', markerfacecolor='gold', markeredgecolor='k', markeredgewidth=1)\n",
    "    acq.set_xlim((-2, 10))\n",
    "    acq.set_ylim((0, np.max(utility) + 0.5))\n",
    "    acq.set_ylabel('Utility', fontdict={'size': 20})\n",
    "    acq.set_xlabel('x', fontdict={'size': 20})\n",
    "\n",
    "    axis.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.)\n",
    "    acq.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.)\n",
    "\n",
    "\n",
    "# 每一次优化都可视化其优化过程\n",
    "plot_gp(optimizer, x, y)\n",
    "plt.show()\n",
    "\n",
    "optimizer.maximize(init_points=0, n_iter=1, kappa=5)\n",
    "plot_gp(optimizer, x, y)\n",
    "plt.show()\n",
    "\n",
    "optimizer.maximize(init_points=0, n_iter=1, kappa=5)\n",
    "plot_gp(optimizer, x, y)\n",
    "plt.show()\n",
    "\n",
    "optimizer.maximize(init_points=0, n_iter=1, kappa=5)\n",
    "plot_gp(optimizer, x, y)\n",
    "plt.show()\n",
    "\n",
    "optimizer.maximize(init_points=0, n_iter=1, kappa=5)\n",
    "plot_gp(optimizer, x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploration 与  exploitation \n",
    "\n",
    "这是贝叶斯超参数优化中的两个重要术语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "np.random.seed(42)\n",
    "xs = np.linspace(-2, 10, 10000)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"定义目标函数\"\"\"\n",
    "    return np.exp(-(x - 2) ** 2) + np.exp(-(x - 6) ** 2 / 10) + 1 / (x ** 2 + 1)\n",
    "\n",
    "\n",
    "plt.plot(xs, f(xs))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def plot_bo(f, bo):\n",
    "    \"\"\"绘制贝叶斯优化过程\"\"\"\n",
    "    x = np.linspace(-2, 10, 10000)\n",
    "    mean, sigma = bo._gp.predict(x.reshape(-1, 1), return_std=True)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(x, f(x))\n",
    "    plt.plot(x, mean)\n",
    "    plt.fill_between(x, mean + sigma, mean - sigma, alpha=0.1)\n",
    "    plt.scatter(bo.space.params.flatten(), bo.space.target, c=\"red\", s=50, zorder=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 构造BO对象\n",
    "bo = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    verbose=0,\n",
    "    random_state=987234,\n",
    ")\n",
    "# 设置一个kappa值，小的值是偏向于exploitation，即在现有值附近探索，保证更大\n",
    "bo.maximize(n_iter=10, acq=\"ucb\", kappa=0.1)\n",
    "\n",
    "plot_bo(f, bo)\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    verbose=0,\n",
    "    random_state=987234,\n",
    ")\n",
    "# 设置新的较大kappa值，尽可能地探索未知的空间\n",
    "bo.maximize(n_iter=10, acq=\"ucb\", kappa=10)\n",
    "\n",
    "plot_bo(f, bo)\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    verbose=0,\n",
    "    random_state=987234,\n",
    ")\n",
    "\n",
    "# 换一个提取函数，换为EI方法。xi小的时候，偏向于exploitation\n",
    "bo.maximize(n_iter=10, acq=\"ei\", xi=1e-4)\n",
    "\n",
    "plot_bo(f, bo)\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    verbose=0,\n",
    "    random_state=987234,\n",
    ")\n",
    "# 偏大的时候，偏向于探索exploration\n",
    "bo.maximize(n_iter=10, acq=\"ei\", xi=1e-1)\n",
    "\n",
    "plot_bo(f, bo)\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    verbose=0,\n",
    "    random_state=987234,\n",
    ")\n",
    "# 常用的另一种方法，poi法。同样，小的xi篇exploition，大的偏exploration\n",
    "bo.maximize(n_iter=10, acq=\"poi\", xi=1e-4)\n",
    "\n",
    "plot_bo(f, bo)\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=f,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    verbose=0,\n",
    "    random_state=987234,\n",
    ")\n",
    "\n",
    "bo.maximize(n_iter=10, acq=\"poi\", xi=1e-1)\n",
    "\n",
    "plot_bo(f, bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯优化实例\n",
    "\n",
    "最后是一个用交叉验证和贝叶斯优化对机器学习模型SVR和RF调参的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Colours\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"Synthetic binary classification dataset.\"\"\"\n",
    "    data, targets = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=45,\n",
    "        n_informative=12,\n",
    "        n_redundant=7,\n",
    "        random_state=134985745,\n",
    "    )\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def svc_cv(C, gamma, data, targets):\n",
    "    \"\"\"SVC cross validation.\n",
    "    This function will instantiate a SVC classifier with parameters C and\n",
    "    gamma. Combined with data and targets this will in turn be used to perform\n",
    "    cross validation. The result of cross validation is returned.\n",
    "    Our goal is to find combinations of C and gamma that maximizes the roc_auc\n",
    "    metric.\n",
    "    \"\"\"\n",
    "    estimator = SVC(C=C, gamma=gamma, random_state=2)\n",
    "    cval = cross_val_score(estimator, data, targets, scoring='roc_auc', cv=4)\n",
    "    return cval.mean()\n",
    "\n",
    "\n",
    "def rfc_cv(n_estimators, min_samples_split, max_features, data, targets):\n",
    "    \"\"\"Random Forest cross validation.\n",
    "    This function will instantiate a random forest classifier with parameters\n",
    "    n_estimators, min_samples_split, and max_features. Combined with data and\n",
    "    targets this will in turn be used to perform cross validation. The result\n",
    "    of cross validation is returned.\n",
    "    Our goal is to find combinations of n_estimators, min_samples_split, and\n",
    "    max_features that minimzes the log loss.\n",
    "    \"\"\"\n",
    "    estimator = RFC(\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        random_state=2\n",
    "    )\n",
    "    cval = cross_val_score(estimator, data, targets,\n",
    "                           scoring='neg_log_loss', cv=4)\n",
    "    return cval.mean()\n",
    "\n",
    "\n",
    "def optimize_svc(data, targets):\n",
    "    \"\"\"Apply Bayesian Optimization to SVC parameters.\"\"\"\n",
    "\n",
    "    def svc_crossval(expC, expGamma):\n",
    "        \"\"\"Wrapper of SVC cross validation.\n",
    "        Notice how we transform between regular and log scale. While this\n",
    "        is not technically necessary, it greatly improves the performance\n",
    "        of the optimizer.\n",
    "        \"\"\"\n",
    "        C = 10 ** expC\n",
    "        gamma = 10 ** expGamma\n",
    "        return svc_cv(C=C, gamma=gamma, data=data, targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=svc_crossval,\n",
    "        pbounds={\"expC\": (-3, 2), \"expGamma\": (-4, -1)},\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(n_iter=10)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)\n",
    "\n",
    "\n",
    "def optimize_rfc(data, targets):\n",
    "    \"\"\"Apply Bayesian Optimization to Random Forest parameters.\"\"\"\n",
    "\n",
    "    def rfc_crossval(n_estimators, min_samples_split, max_features):\n",
    "        \"\"\"Wrapper of RandomForest cross validation.\n",
    "        Notice how we ensure n_estimators and min_samples_split are casted\n",
    "        to integer before we pass them along. Moreover, to avoid max_features\n",
    "        taking values outside the (0, 1) range, we also ensure it is capped\n",
    "        accordingly.\n",
    "        \"\"\"\n",
    "        return rfc_cv(\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=max(min(max_features, 0.999), 1e-3),\n",
    "            data=data,\n",
    "            targets=targets,\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=rfc_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (10, 250),\n",
    "            \"min_samples_split\": (2, 25),\n",
    "            \"max_features\": (0.1, 0.999),\n",
    "        },\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(n_iter=10)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data, targets = get_data()\n",
    "\n",
    "    print(Colours.yellow(\"--- Optimizing SVM ---\"))\n",
    "    optimize_svc(data, targets)\n",
    "\n",
    "    print(Colours.green(\"--- Optimizing Random Forest ---\"))\n",
    "    optimize_rfc(data, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
