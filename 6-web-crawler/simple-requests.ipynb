{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Requests\n",
    "\n",
    "在获取科研数据的过程中，一项重要工作就是数据爬取，为了更便利地获取数据，从这篇开始，简单地掌握一些爬虫技能。\n",
    "\n",
    "这部分内容主要参考了：[Python-crawler](https://github.com/Ehco1996/Python-crawler)和[Python3网络爬虫开发实战](https://python3webspider.cuiqingcai.com/)。\n",
    "\n",
    "概念性的内容，可直接参考《Python3网络爬虫开发实战》，这里直接从实战开始，从最简单的requests库的使用开始记录。\n",
    "\n",
    "## 安装requests库\n",
    "\n",
    "直接使用pip安装requests即可。\n",
    "\n",
    "``` python\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "然后检查一下是否安装了beautifulsoup4。\n",
    "\n",
    "``` python\n",
    "pip list\n",
    "```\n",
    "\n",
    "## requests基本使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">ç»å½</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å",
      "³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿",
      "è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#首先我们先导入requests这个包\n",
    "import requests\n",
    "\n",
    "#我们来吧百度的index页面的html源码抓取到本地，并用r变量保存\n",
    "#注意这里，网页前面的 http://一定要写出来，它并不能像真正的浏览器一样帮我们补全http协议\n",
    "r = requests.get(\"http://www.baidu.com\")\n",
    "\n",
    "#将下载到的内容打印一下：\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上面的抓取过程中，我们用到了requests库的**get方法**，这个方法是requests库中最常用的方法之一。他接受一个参数（url）并返回一个**HTTP response对象**。\n",
    "\n",
    "### get方法\n",
    "\n",
    "这个方法可以接收三个参数，其中第二个默认为None，params是字典或字节序列，作为参数增加到url中的，第三个可选，包括设置HTTP定制头headers等：def get(url, params=None, **kwargs)。\n",
    "\n",
    "get返回的是Response对象。是一个完整的对HTTP request的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def getHtmlText(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        # 如果状态码不是200 则应发HTTOError异常\n",
    "        r.raise_for_status()\n",
    "        # 设置正确的编码方式\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"Something Wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://www.baidu.com')\n",
    "print(r.cookies)\n",
    "for key, target_directory in r.cookies.items():\n",
    "    print(key + '=' + target_directory)\n",
    "\n",
    "requests.get('http://httpbin.org/cookies/set/number/123456789')\n",
    "r = requests.get('http://httpbin.org/cookies')\n",
    "print(r.text)\n",
    "\n",
    "s = requests.Session()\n",
    "s.get('http://httpbin.org/cookies/set/number/123456789')\n",
    "r = s.get('http://httpbin.org/cookies')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://www.baidu.com/')\n",
    "print(type(r))\n",
    "print(r.status_code)\n",
    "print(type(r.text))\n",
    "print(r.text)\n",
    "print(r.cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "content = 'Hello 123 4567 World_This is a Regex Demo'\n",
    "print(len(content))\n",
    "result = re.match('^Hello\\s\\d\\d\\d\\s\\d{4}\\s\\w{10}', content)\n",
    "print(result)\n",
    "print(result.group())\n",
    "print(result.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他类似request库的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# 以 Python 官网为例，我们来把这个网页抓下来：\n",
    "response = urllib.request.urlopen('https://www.python.org')\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "# 利用 type() 方法输出 Response 的类型。通过输出结果可以发现它是一个 HTTPResposne 类型的对象\n",
    "print(type(response))\n",
    "\n",
    "# 得到这个对象之后，我们把它赋值为 response 变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。\n",
    "print(response.status)\n",
    "print(response.getheaders())\n",
    "print(response.getheader('Server'))\n",
    "\n",
    "# 想给链接传递一些参数该怎么实现呢？我们首先看一下 urlopen() 函数的API：\n",
    "data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')\n",
    "response = urllib.request.urlopen('http://httpbin.org/post', data=data)\n",
    "print(response.read())\n",
    "\n",
    "# timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。\n",
    "response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)\n",
    "# 在这里我们设置了超时时间是 1 秒，程序 1 秒过后服务器依然没有响应，于是抛出了 URLError 异常，它属于 urllib.error 模块，错误原因是超时。\n",
    "print(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# 利用更强大的 Request 类来构建一个请求\n",
    "request = urllib.request.Request('https://python.org')\n",
    "response = urllib.request.urlopen(request)\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "url = 'http://httpbin.org/post'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    'Host': 'httpbin.org'\n",
    "}\n",
    "dict = {\n",
    "    'name': 'Germey'\n",
    "}\n",
    "data = bytes(urllib.parse.urlencode(dict), encoding='utf8')\n",
    "req = request.Request(url=url, data=data, headers=headers, method='POST')\n",
    "response = request.urlopen(req)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更强大的工具 Handler.我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情\n",
    "# 比调用 urlopen() 的对象的更普遍的对象，也就是 Opener。\n",
    "# 输入用户名和密码，认证成功之后才能查看页面\n",
    "from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener\n",
    "from urllib.error import URLError\n",
    "\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "url = 'http://localhost:5000/'\n",
    "\n",
    "p = HTTPPasswordMgrWithDefaultRealm()\n",
    "p.add_password(None, url, username, password)\n",
    "auth_handler = HTTPBasicAuthHandler(p)\n",
    "opener = build_opener(auth_handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(url)\n",
    "    html = result.read().decode('utf-8')\n",
    "    print(html)\n",
    "except URLError as e:\n",
    "    print(e.reason)\n",
    "\n",
    "# 在做爬虫的时候免不了要使用代理，如果要添加代理，可以这样做：\n",
    "from urllib.error import URLError\n",
    "from urllib.request import ProxyHandler, build_opener\n",
    "\n",
    "proxy_handler = ProxyHandler({\n",
    "    'http': 'http://127.0.0.1:9743',\n",
    "    'https': 'https://127.0.0.1:9743'\n",
    "})\n",
    "opener = build_opener(proxy_handler)\n",
    "try:\n",
    "    response = opener.open('https://www.baidu.com')\n",
    "    print(response.read().decode('utf-8'))\n",
    "except URLError as e:\n",
    "    print(e.reason)\n",
    "\n",
    "# Cookies 的处理就需要 Cookies 相关的 Handler 了。\n",
    "import http.cookiejar, urllib.request\n",
    "\n",
    "cookie = http.cookiejar.CookieJar()\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "response = opener.open('http://www.baidu.com')\n",
    "for item in cookie:\n",
    "    print(item.name+\"=\"+item.value)\n",
    "filename = 'cookies.txt'\n",
    "cookie = http.cookiejar.MozillaCookieJar(filename)\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "response = opener.open('http://www.baidu.com')\n",
    "cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, error\n",
    "try:\n",
    "    response = request.urlopen('http://cuiqingcai.com/index.htm')\n",
    "except error.URLError as e:\n",
    "    print(e.reason)\n",
    "\n",
    "\n",
    "\n",
    "from urllib import request,error\n",
    "try:\n",
    "    response = request.urlopen('http://cuiqingcai.com/index.htm')\n",
    "except error.HTTPError as e:\n",
    "    print(e.reason, e.code, e.headers, seq='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
