{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch运用\n",
    "\n",
    "通过示例进一步理解如何运用pytorch包，先从简单的lstm和cnn开始。\n",
    "\n",
    "## 基本神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_normalized_data():\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    if not os.path.exists('data/train.csv'):\n",
    "        print('Looking for data/train.csv')\n",
    "        print('You have not downloaded the data and/or not placed the files in the correct location.')\n",
    "        print('Please get the data from: https://www.kaggle.com/c/digit-recognizer')\n",
    "        print('Place train.csv in the folder data adjacent to the class folder')\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv('data/train.csv')\n",
    "    data = df.values.astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "\n",
    "    Xtrain = X[:-1000]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest = X[-1000:]\n",
    "    Ytest = Y[-1000:]\n",
    "\n",
    "    # normalize the data\n",
    "    mu = Xtrain.mean(axis=0)\n",
    "    std = Xtrain.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    Xtrain = (Xtrain - mu) / std\n",
    "    Xtest = (Xtest - mu) / std\n",
    "\n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 建立神经网络\n",
    "class Net(torch.nn.Module):  # 继承 torch 的 Module\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()  # 继承 __init__ 功能\n",
    "        # 定义每层用什么样的形式\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)  # 隐藏层线性输出\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  # 输出层线性输出\n",
    "\n",
    "    def forward(self, x):  # 这同时也是 Module 中的 forward 功能\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        x = F.relu(self.hidden(x))  # 激励函数(隐藏层的线性值)\n",
    "        x = self.predict(x)  # 输出值\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(1, 10, 1)\n",
    "print(net)  # net 的结构\n",
    "\n",
    "# 训练神经网络，optimizer 是训练的工具\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率\n",
    "loss_func = torch.nn.MSELoss()  # 预测值和真实值的误差计算公式 (均方差)\n",
    "\n",
    "plt.ion()  # 画图\n",
    "plt.show()\n",
    "for t in range(100):\n",
    "    prediction = net(x)  # 喂给 net 训练数据 x, 输出预测值\n",
    "\n",
    "    loss = loss_func(prediction, y)  # 计算两者的误差\n",
    "\n",
    "    optimizer.zero_grad()  # 清空上一步的残余更新参数值\n",
    "    loss.backward()  # 误差反向传播, 计算参数更新值\n",
    "    optimizer.step()  # 将参数更新值施加到 net 的 parameters 上\n",
    "    # 接着上面来\n",
    "    if t % 5 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用pytorch写一个神经网络的示例——来自课程《Mordern deep learning in Python》\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from util import get_normalized_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 第一步是load data，这里采用的是deep learning里的hello world，即识别手写数字的数据集，来自kaggle。\n",
    "# get the data,\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# 第二步是构建网络结构，使用pytorch构建神经网络要比直接通过numpy手写简单很多。和Keras类似，可以先构建一个sequential，然后逐层添加网络结构即可。\n",
    "model = torch.nn.Sequential()\n",
    "# 逐层添加网络即可，在pytorch中是采用add_module()函数。第一个参数是当前层的命名，可以取任何想要的名称。第二个参数就是该层。层要么是linear transformation，要么是activation\n",
    "# 比如第一层，Linear,参数D是输入的神经元个数，第二个参数500是输出的神经元个数\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# 第三步是构建loss函数，loss函数详情可参考http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# 第四步是设置优化函数，Adam是一种常用的优化算法，是一种改良的GD算法。算法需要神经网络的parameters作为参数。\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# 第五步是定义训练过程和预测过程，这部分也是相对最难掌握的一部分。这部分相对TensorFlow和Theano都会麻烦一些。\n",
    "\n",
    "\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    \"\"\"训练过程主要包括：包装输入输出到Variable变量，初始化优化函数，前向传播，反向传播，以及参数更新，详情见每步解释\"\"\"\n",
    "    # 为什么要包装变量到Variable：把Tensor包装到Variable中，它就会开始保存所有计算历史。因此每次运算都会稍微多一些cost；另一方面，在训练循环外部对Variable进行计算操作相对容易。不包装也是可以计算的，并且后面的pytorch版本有柯南高就不需要Variable的这一步了\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # pytorch的梯度计算是累计的，这对有些神经网络是比较好的，因此这里初始化为0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 直接调用model的前向函数即可得到输出\n",
    "    logits = model.forward(inputs)\n",
    "\n",
    "    # 然后计算loss\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # 接着就是进行反向传播计算\n",
    "    output.backward()\n",
    "\n",
    "    # 最后是更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    # argmax函数是给出axis维上数组中最大数的索引\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# 第六步定义各类超参数并开始训练过程\n",
    "# 首先是将numpy变量都设置为torch中的张量，注意要指定数据类型\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 32  # 每个batch的大小\n",
    "n_batches = Xtrain.size()[0] // batch_size  # batch的个数\n",
    "\n",
    "costs = []\n",
    "test_accuracies = []\n",
    "for i in range(epochs):\n",
    "    cost = 0.\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    Ypred = predict(model, Xtest)\n",
    "    acc = np.mean(Ytest == Ypred)\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, cost / n_batches, acc))\n",
    "\n",
    "    costs.append(cost / n_batches)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "# 第七步是将训练过程可视化,# EXERCISE: plot test cost + training accuracy too。一般都是把cost和accuracy都可视化出来\n",
    "# plot the results\n",
    "plt.plot(costs)\n",
    "plt.title('Training cost')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_accuracies)\n",
    "plt.title('Test accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM in Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"一个简单的pytorch的lstm模型的示例\"\"\"\n",
    "# Author: Robert Guthrie\n",
    "# 作者：Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd  # torch中自动计算梯度模块\n",
    "import torch.nn as nn             # 神经网络模块\n",
    "import torch.nn.functional as F   # 神经网络模块中的常用功能\n",
    "import torch.optim as optim       # 模型优化器模块\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 第一句代码生成了一个LSTM对象，LSTM类继承自RNNBase类，RNNBase类继承自Module类，\n",
    "# Module类是pytorch中完成一定网络功能的基类，可以通过继承该类定义自己的神经网络。\n",
    "# 自己实现神经网络时，一般要重写其forward方法。\n",
    "# Module实现了__call__方法，这意味着其可被当做可调用方法使用。比如下面有用到lstm()。\n",
    "# RNNBase的__init__和__forward方法都是要读一读的，理解这个及基本理解了循环网络的机制。\n",
    "# lstm单元输入和输出维度都是3\n",
    "lstm = nn.LSTM(3, 3)\n",
    "# 生成一个长度为5，每一个元素为1*3的序列作为输入，这里的数字3对应于上句中第一个3\n",
    "inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)]\n",
    "\n",
    "# 设置隐藏层维度，初始化隐藏层的数据。hidden变量是一个元组，其第一个元素是LSTM隐藏层输出，另一个元素维护隐藏层的状态。\n",
    "# torch.rand(1,1,3)就是生成了一个维度为(1,1,3)的以一定高斯分布生成的张量\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN in pytorch\n",
    "\n",
    "## 神经网络训练技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"用pytorch写神经网络时，进行dropout的示例。\n",
    "有dropout的神经网络的区别：\n",
    "一个是在神经网络结构中增加dropout层；\n",
    "另一个，注意dropout有两个mode，一是对train data的，一个是对test data的。在train data中进行dropout，但是在test data中是不用的\"\"\"\n",
    "\n",
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "# Note: is helpful to look at keras_example.py first\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_normalized_data\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "# get the data, same as Theano + Tensorflow examples\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# Note: no need to convert Y to indicator matrix\n",
    "\n",
    "\n",
    "# the model will be a sequence of layers\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "# ANN with layers [784] -> [500] -> [300] -> [10]\n",
    "# NOTE: the \"p\" is p_drop, not p_keep\n",
    "model.add_module(\"dropout1\", torch.nn.Dropout(p=0.2))\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout2\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout3\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# Note: no final softmax!\n",
    "# just like Tensorflow, it's included in cross-entropy function\n",
    "\n",
    "\n",
    "# define a loss function\n",
    "# other loss functions can be found here:\n",
    "# http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# Note: this returns a function!\n",
    "# e.g. use it like: loss(logits, labels)\n",
    "\n",
    "\n",
    "# define an optimizer\n",
    "# other optimizers can be found here:\n",
    "# http://pytorch.org/docs/master/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# define the training procedure\n",
    "# i.e. one step of gradient descent\n",
    "# there are lots of steps\n",
    "# so we encapsulate it in a function\n",
    "# Note: inputs and labels are torch tensors\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    # set the model to training mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.train()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # Backward\n",
    "    output.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # what's the difference between backward() and step()?\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# similar to train() but not doing the backprop step\n",
    "def get_cost(model, loss, inputs, labels):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# define the prediction procedure\n",
    "# also encapsulate these steps\n",
    "# Note: inputs is a torch tensor\n",
    "def predict(model, inputs):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# return the accuracy\n",
    "# labels is a torch tensor\n",
    "# to get back the internal numpy data\n",
    "# use the instance method .numpy()\n",
    "def score(model, inputs, labels):\n",
    "    predictions = predict(model, inputs)\n",
    "    return np.mean(labels.numpy() == predictions)\n",
    "\n",
    "\n",
    "### prepare for training loop ###\n",
    "\n",
    "# convert the data arrays into torch tensors\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "Ytest = torch.from_numpy(Ytest).long()\n",
    "\n",
    "# training parameters\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "n_batches = Xtrain.size()[0] // batch_size\n",
    "\n",
    "# things to keep track of\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# main training loop\n",
    "for i in range(epochs):\n",
    "    cost = 0\n",
    "    test_cost = 0\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    # we could have also calculated the train cost here\n",
    "    # but I wanted to show you that we could also return it\n",
    "    # from the train function itself\n",
    "    train_acc = score(model, Xtrain, Ytrain)\n",
    "    test_acc = score(model, Xtest, Ytest)\n",
    "    test_cost = get_cost(model, loss, Xtest, Ytest)\n",
    "\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, test_cost, test_acc))\n",
    "\n",
    "    # for plotting\n",
    "    train_costs.append(cost / n_batches)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_costs.append(test_cost)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(train_costs, label='Train cost')\n",
    "plt.plot(test_costs, label='Test cost')\n",
    "plt.title('Cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label='Train accuracy')\n",
    "plt.plot(test_accuracies, label='Test accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}