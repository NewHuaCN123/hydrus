{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch使用基础\n",
    "\n",
    "本文主要参考pytorch中文文档。基本思路是：\n",
    "\n",
    "- 首先简单认识pytorch的基本情况；\n",
    "- 其次，简单介绍神经网络，并给出torch的入门级实现；\n",
    "- 接下来，对pytorch的主要构成一一解析；\n",
    "- 最后是lstm等神经网络结构的实现和一些训练细节。\n",
    "\n",
    "## torch快速入门\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "# 构建一个 5x3 的矩阵, 未初始化的:\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x)\n",
    "# 构建一个随机初始化的矩阵:\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "# 获得 size:\n",
    "print(x.size())\n",
    "# 加法\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "\n",
    "print(torch.add(x, y))\n",
    "\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)\n",
    "\n",
    "# 可以用类似Numpy的索引来处理所有的张量！\n",
    "print(x[:, 1])\n",
    "\n",
    "# 改变大小: 如果你想要去改变tensor的大小, 可以使用 torch.view:\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "# 转换一个 Torch Tensor 为 NumPy 数组\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "# 查看 numpy 数组是如何改变的.a和b是绑定的\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# 看改变 np 数组之后 Torch Tensor 是如何自动改变的\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# 可以使用 .cuda 方法将 Tensors 在GPU上运行.\n",
    "# 只要在  CUDA 是可用的情况下, 我们可以运行这段代码\n",
    "if torch.cuda.is_available():\n",
    "    b = b.cuda()\n",
    "    print(b + b)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 创建 variable（变量）:\n",
    "\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "# variable（变量）的操作:\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "# y 由操作创建,所以它有 grad_fn 属性.\n",
    "\n",
    "print(y.grad_fn)\n",
    "\n",
    "# y 的更多操作\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n",
    "\n",
    "# 梯度 我们现在开始了解反向传播, out.backward() 与 out.backward(torch.Tensor([1.0])) 这样的方式一样\n",
    "\n",
    "out.backward()\n",
    "\n",
    "# 但因 d(out)/dx 的梯度\n",
    "# 你应该得到一个 4.5 的矩阵. 让我们推导出 out Variable “o”. 我们有 o=1/4∑izi, zi=3(xi+2)^2 和 zi∣∣xi=1=27. 因此, ∂o/∂xi=32(xi+2), 所以 ∂o∂xi∣∣xi=1=9/2=4.5.\n",
    "print(x.grad)\n",
    "\n",
    "# 你可以使用自动求导来做很多有趣的事情\n",
    "x = torch.randn(3)\n",
    "# torch.norm(input, p=2) → float\n",
    "#\n",
    "# 返回输入张量input 的p 范数。\n",
    "#\n",
    "# 参数：\n",
    "#     input (Tensor) – 输入张量\n",
    "#     p (float,optional) – 范数计算中的幂指数值\n",
    "x = Variable(x, requires_grad = True)\n",
    "\n",
    "y = x * 2\n",
    "print(y.data)\n",
    "print(y.data.norm())\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "# 上面整个用计算图的思路去理解就很简单了\n",
    "\n",
    "\"\"\" 神经网络可以使用 torch.nn 包构建.\n",
    " \n",
    " autograd 实现了反向传播功能, 但是直接用来写深度学习的代码在很多情况下还是稍显复杂,\n",
    "  torch.nn 是专门为神经网络设计的模块化接口. nn 构建于 Autograd 之上, \n",
    " 可用来定义和运行神经网络. nn.Module 是 nn 中最重要的类, \n",
    " 可把它看成是一个网络的封装, 包含网络各层定义以及 forward 方法, \n",
    "调用 forward(input) 方法, 可返回前向传播的结果.\n",
    "\n",
    "一个典型的神经网络训练过程如下:\n",
    "\n",
    "    定义具有一些可学习参数(或权重)的神经网络\n",
    "    迭代输入数据集\n",
    "    通过网络处理输入\n",
    "    计算损失(输出的预测值与实际值之间的距离)\n",
    "    将梯度传播回网络\n",
    "    更新网络的权重, 通常使用一个简单的更新规则: weight = weight - learning_rate * gradient\n",
    "\n",
    "\"\"\"\n",
    "# 定义一个网络:\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 卷积层 '1'表示输入图片为单通道, '6'表示输出通道数, '5'表示卷积核为5*5\n",
    "        # 核心\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 仿射层/全连接层: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #在由多个输入平面组成的输入信号上应用2D最大池化.\n",
    "        # (2, 2) 代表的是池化操作的步幅\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果大小是正方形, 则只能指定一个数字\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除批量维度外的所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 神经网络基础\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 跟着例子学习PyTorch\n",
    "\n",
    "# 先使用 numpy 实现网络。\n",
    "# Numpy 提供了一个n维的数组对象, 并提供了许多操纵这个数组对象的函数.\n",
    "#  Numpy 是科学计算的通用框架; Numpy 数组没有计算图, 也没有深度学习, 也没有梯度下降等方法实现的接口.\n",
    "# 但是我们仍然可以很容易地使用 numpy 生成随机数据 并将产生的数据传入双层的神经网络,\n",
    "#  并使用 numpy 来实现这个网络的正向传播和反向传播:\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# N 是一个batch的样本数量; D_in是输入维度;\n",
    "# H 是隐藏层向量的维度; D_out是输出维度.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机的输入输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重参数\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向计算, 算出y的预测值\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算并打印误差值\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 在反向传播中, 计算出误差关于w1和w2的导数\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "# Numpy 是一个伟大的框架, 但它不能利用 GPU 加速它数值计算.\n",
    "#  对于现代的深度神经网络, GPU 往往是提供 50倍或更大的加速,\n",
    "# 所以不幸的是, numpy 不足以满足现在深度学习的需求.\n",
    "# 介绍一下最基本的 PyTorch 概念: Tensor .\n",
    "# PyTorch Tensor 在概念上与 numpy 数组相同:\n",
    "# Tensor 是一个n维数组, PyTorch 也提供了很多能在这些 Tensor 上操作的函数.\n",
    "#  像 numpy 数组一样, PyTorch Tensor 也和numpy的数组对象一样不了解深度学习,计算图和梯度下降；\n",
    "# 它们只是科学计算的通用工具.\n",
    "# 然而不像 numpy, PyTorch Tensor 可以利用 GPU 加速他们的数字计算.\n",
    "#  要在 GPU 上运行 PyTorch 张量, 只需将其转换为新的数据类型.\n",
    "\n",
    "# 将 PyTorch Tensor 生成的随机数据传入双层的神经网络. 就像上面的 numpy 例子一样,\n",
    "# 我们需要手动实现网络的正向传播和反向传播:\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor  # 取消注释以在GPU上运行\n",
    "\n",
    "# N 批量大小; D_in是输入尺寸;\n",
    "# H是隐藏尺寸; D_out是输出尺寸.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 正向传递：计算预测y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算并打印loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 反向传播计算关于损失的w1和w2的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "# torch和numpy\n",
    "np_data = np.arange(6).reshape((2, 3))\n",
    "torch_data = torch.from_numpy(np_data)\n",
    "tensor2array = torch_data.numpy()\n",
    "print(\n",
    "    '\\nnumpy', np_data,\n",
    "    '\\ntorch tensor', torch_data,\n",
    "    '\\ntensor to array', tensor2array\n",
    ")\n",
    "\n",
    "# 进行计算的时候 numpy和torch的方式也有所不同\n",
    "data = np.array([[1, 2], [3, 4]])\n",
    "tensor = torch.FloatTensor(data)\n",
    "\n",
    "print(\n",
    "    '\\nnumpy:', data.dot(data),\n",
    "    '\\ntorch:', torch.mm(tensor, tensor)\n",
    ")\n",
    "\n",
    "# variable变量\n",
    "variable = Variable(tensor, requires_grad=True)\n",
    "\n",
    "t_out = torch.mean(tensor * tensor)\n",
    "v_out = torch.mean(variable * variable)\n",
    "\n",
    "print(t_out)\n",
    "print(v_out)\n",
    "\n",
    "v_out.backward()\n",
    "# v_out=1/4*sum(variable*variable) d(v_out)/d(variable)=variable/2\n",
    "print(variable.grad)\n",
    "\n",
    "print(variable)\n",
    "\n",
    "print(variable.data)\n",
    "\n",
    "print(variable.data.numpy())\n",
    "\n",
    "# fake data\n",
    "x = torch.linspace(-5, 5, 200)  # x data (tensor) ,shape=(100,1)\n",
    "x = Variable(x)\n",
    "x_np = x.data.numpy()  # 为了画图，转换为numpy的数据形式\n",
    "\n",
    "y_relu = F.relu(x).data.numpy()\n",
    "y_sigmoid = F.sigmoid(x).data.numpy()\n",
    "y_tanh = F.tanh(x).data.numpy()\n",
    "y_softplus = F.softplus(x).data.numpy()\n",
    "\n",
    "# plt.figure(1, figsize=(8, 6))\n",
    "# plt.subplot(221)\n",
    "# plt.plot(x_np, y_relu, c='red', label='relu')\n",
    "# plt.ylim((-1, 5))\n",
    "# plt.legend(loc='best')\n",
    "#\n",
    "# plt.subplot(222)\n",
    "# plt.plot(x_np, y_sigmoid, c='red', label='sigmoid')\n",
    "# plt.ylim((-0.2, 1.2))\n",
    "# plt.legend(loc='best')\n",
    "#\n",
    "# plt.subplot(223)\n",
    "# plt.plot(x_np, y_tanh, c='red', label='tanh')\n",
    "# plt.ylim((-1.2, 1.2))\n",
    "# plt.legend(loc='best')\n",
    "#\n",
    "# plt.subplot(224)\n",
    "# plt.plot(x_np, y_softplus, c='red', label='softplus')\n",
    "# plt.ylim((-0.2, 6))\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# 关系拟合\n",
    "# unsqueeze见文档（不知道什么用法的都去查文档，文档很清楚）\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor),shape=(100,1)\n",
    "y = x.pow(2) + 0.2 * torch.rand(x.size())  # noisy y data (tensor), shape=(100, 1)\n",
    "print(x)\n",
    "print(y)\n",
    "# 用 Variable 来修饰这些数据 tensor\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "\n",
    "# 画图\n",
    "# plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "# plt.show()\n",
    "\n",
    "# 建立神经网络\n",
    "class Net(torch.nn.Module):  # 继承 torch 的 Module\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()  # 继承 __init__ 功能\n",
    "        # 定义每层用什么样的形式\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)  # 隐藏层线性输出\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  # 输出层线性输出\n",
    "\n",
    "    def forward(self, x):  # 这同时也是 Module 中的 forward 功能\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        x = F.relu(self.hidden(x))  # 激励函数(隐藏层的线性值)\n",
    "        x = self.predict(x)  # 输出值\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(1, 10, 1)\n",
    "print(net)  # net 的结构\n",
    "\n",
    "# 训练神经网络，optimizer 是训练的工具\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率\n",
    "loss_func = torch.nn.MSELoss()  # 预测值和真实值的误差计算公式 (均方差)\n",
    "\n",
    "plt.ion()  # 画图\n",
    "plt.show()\n",
    "for t in range(100):\n",
    "    prediction = net(x)  # 喂给 net 训练数据 x, 输出预测值\n",
    "\n",
    "    loss = loss_func(prediction, y)  # 计算两者的误差\n",
    "\n",
    "    optimizer.zero_grad()  # 清空上一步的残余更新参数值\n",
    "    loss.backward()  # 误差反向传播, 计算参数更新值\n",
    "    optimizer.step()  # 将参数更新值施加到 net 的 parameters 上\n",
    "    # 接着上面来\n",
    "    if t % 5 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.pause(0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## torch.nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 利用pytorch写一个神经网络的示例——来自课程《Mordern deep learning in Python》\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from util import get_normalized_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 第一步是load data，这里采用的是deep learning里的hello world，即识别手写数字的数据集，来自kaggle。\n",
    "# get the data,\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# 第二步是构建网络结构，使用pytorch构建神经网络要比直接通过numpy手写简单很多。和Keras类似，可以先构建一个sequential，然后逐层添加网络结构即可。\n",
    "model = torch.nn.Sequential()\n",
    "# 逐层添加网络即可，在pytorch中是采用add_module()函数。第一个参数是当前层的命名，可以取任何想要的名称。第二个参数就是该层。层要么是linear transformation，要么是activation\n",
    "# 比如第一层，Linear,参数D是输入的神经元个数，第二个参数500是输出的神经元个数\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# 第三步是构建loss函数，loss函数详情可参考http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# 第四步是设置优化函数，Adam是一种常用的优化算法，是一种改良的GD算法。算法需要神经网络的parameters作为参数。\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# 第五步是定义训练过程和预测过程，这部分也是相对最难掌握的一部分。这部分相对TensorFlow和Theano都会麻烦一些。\n",
    "\n",
    "\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    \"\"\"训练过程主要包括：包装输入输出到Variable变量，初始化优化函数，前向传播，反向传播，以及参数更新，详情见每步解释\"\"\"\n",
    "    # 为什么要包装变量到Variable：把Tensor包装到Variable中，它就会开始保存所有计算历史。因此每次运算都会稍微多一些cost；另一方面，在训练循环外部对Variable进行计算操作相对容易。不包装也是可以计算的，并且后面的pytorch版本有柯南高就不需要Variable的这一步了\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # pytorch的梯度计算是累计的，这对有些神经网络是比较好的，因此这里初始化为0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 直接调用model的前向函数即可得到输出\n",
    "    logits = model.forward(inputs)\n",
    "\n",
    "    # 然后计算loss\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # 接着就是进行反向传播计算\n",
    "    output.backward()\n",
    "\n",
    "    # 最后是更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    # argmax函数是给出axis维上数组中最大数的索引\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# 第六步定义各类超参数并开始训练过程\n",
    "# 首先是将numpy变量都设置为torch中的张量，注意要指定数据类型\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 32  # 每个batch的大小\n",
    "n_batches = Xtrain.size()[0] // batch_size  # batch的个数\n",
    "\n",
    "costs = []\n",
    "test_accuracies = []\n",
    "for i in range(epochs):\n",
    "    cost = 0.\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    Ypred = predict(model, Xtest)\n",
    "    acc = np.mean(Ytest == Ypred)\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, cost / n_batches, acc))\n",
    "\n",
    "    costs.append(cost / n_batches)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "# 第七步是将训练过程可视化,# EXERCISE: plot test cost + training accuracy too。一般都是把cost和accuracy都可视化出来\n",
    "# plot the results\n",
    "plt.plot(costs)\n",
    "plt.title('Training cost')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_accuracies)\n",
    "plt.title('Test accuracies')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM in Torch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"一个简单的pytorch的lstm模型的示例\"\"\"\n",
    "# Author: Robert Guthrie\n",
    "# 作者：Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd  # torch中自动计算梯度模块\n",
    "import torch.nn as nn             # 神经网络模块\n",
    "import torch.nn.functional as F   # 神经网络模块中的常用功能\n",
    "import torch.optim as optim       # 模型优化器模块\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 第一句代码生成了一个LSTM对象，LSTM类继承自RNNBase类，RNNBase类继承自Module类，\n",
    "# Module类是pytorch中完成一定网络功能的基类，可以通过继承该类定义自己的神经网络。\n",
    "# 自己实现神经网络时，一般要重写其forward方法。\n",
    "# Module实现了__call__方法，这意味着其可被当做可调用方法使用。比如下面有用到lstm()。\n",
    "# RNNBase的__init__和__forward方法都是要读一读的，理解这个及基本理解了循环网络的机制。\n",
    "# lstm单元输入和输出维度都是3\n",
    "lstm = nn.LSTM(3, 3)\n",
    "# 生成一个长度为5，每一个元素为1*3的序列作为输入，这里的数字3对应于上句中第一个3\n",
    "inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)]\n",
    "\n",
    "# 设置隐藏层维度，初始化隐藏层的数据。hidden变量是一个元组，其第一个元素是LSTM隐藏层输出，另一个元素维护隐藏层的状态。\n",
    "# torch.rand(1,1,3)就是生成了一个维度为(1,1,3)的以一定高斯分布生成的张量\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out)\n",
    "    print(hidden)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 神经网络训练技巧\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"用pytorch写神经网络时，进行dropout的示例。\n",
    "有dropout的神经网络的区别：\n",
    "一个是在神经网络结构中增加dropout层；\n",
    "另一个，注意dropout有两个mode，一是对train data的，一个是对test data的。在train data中进行dropout，但是在test data中是不用的\"\"\"\n",
    "\n",
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "# Note: is helpful to look at keras_example.py first\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_normalized_data\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "# get the data, same as Theano + Tensorflow examples\n",
    "# no need to split now, the fit() function will do it\n",
    "Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "# get shapes\n",
    "_, D = Xtrain.shape\n",
    "K = len(set(Ytrain))\n",
    "\n",
    "# Note: no need to convert Y to indicator matrix\n",
    "\n",
    "\n",
    "# the model will be a sequence of layers\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "# ANN with layers [784] -> [500] -> [300] -> [10]\n",
    "# NOTE: the \"p\" is p_drop, not p_keep\n",
    "model.add_module(\"dropout1\", torch.nn.Dropout(p=0.2))\n",
    "model.add_module(\"dense1\", torch.nn.Linear(D, 500))\n",
    "model.add_module(\"relu1\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout2\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense2\", torch.nn.Linear(500, 300))\n",
    "model.add_module(\"relu2\", torch.nn.ReLU())\n",
    "model.add_module(\"dropout3\", torch.nn.Dropout(p=0.5))\n",
    "model.add_module(\"dense3\", torch.nn.Linear(300, K))\n",
    "# Note: no final softmax!\n",
    "# just like Tensorflow, it's included in cross-entropy function\n",
    "\n",
    "\n",
    "# define a loss function\n",
    "# other loss functions can be found here:\n",
    "# http://pytorch.org/docs/master/nn.html#loss-functions\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "# Note: this returns a function!\n",
    "# e.g. use it like: loss(logits, labels)\n",
    "\n",
    "\n",
    "# define an optimizer\n",
    "# other optimizers can be found here:\n",
    "# http://pytorch.org/docs/master/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# define the training procedure\n",
    "# i.e. one step of gradient descent\n",
    "# there are lots of steps\n",
    "# so we encapsulate it in a function\n",
    "# Note: inputs and labels are torch tensors\n",
    "def train(model, loss, optimizer, inputs, labels):\n",
    "    # set the model to training mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.train()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    # Backward\n",
    "    output.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # what's the difference between backward() and step()?\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# similar to train() but not doing the backprop step\n",
    "def get_cost(model, loss, inputs, labels):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "    # Forward\n",
    "    logits = model.forward(inputs)\n",
    "    output = loss.forward(logits, labels)\n",
    "\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# define the prediction procedure\n",
    "# also encapsulate these steps\n",
    "# Note: inputs is a torch tensor\n",
    "def predict(model, inputs):\n",
    "    # set the model to testing mode\n",
    "    # because dropout has 2 different modes!\n",
    "    model.eval()\n",
    "\n",
    "    inputs = Variable(inputs, requires_grad=False)\n",
    "    logits = model.forward(inputs)\n",
    "    return logits.data.numpy().argmax(axis=1)\n",
    "\n",
    "\n",
    "# return the accuracy\n",
    "# labels is a torch tensor\n",
    "# to get back the internal numpy data\n",
    "# use the instance method .numpy()\n",
    "def score(model, inputs, labels):\n",
    "    predictions = predict(model, inputs)\n",
    "    return np.mean(labels.numpy() == predictions)\n",
    "\n",
    "\n",
    "### prepare for training loop ###\n",
    "\n",
    "# convert the data arrays into torch tensors\n",
    "Xtrain = torch.from_numpy(Xtrain).float()\n",
    "Ytrain = torch.from_numpy(Ytrain).long()\n",
    "Xtest = torch.from_numpy(Xtest).float()\n",
    "Ytest = torch.from_numpy(Ytest).long()\n",
    "\n",
    "# training parameters\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "n_batches = Xtrain.size()[0] // batch_size\n",
    "\n",
    "# things to keep track of\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# main training loop\n",
    "for i in range(epochs):\n",
    "    cost = 0\n",
    "    test_cost = 0\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j * batch_size:(j + 1) * batch_size]\n",
    "        Ybatch = Ytrain[j * batch_size:(j + 1) * batch_size]\n",
    "        cost += train(model, loss, optimizer, Xbatch, Ybatch)\n",
    "\n",
    "    # we could have also calculated the train cost here\n",
    "    # but I wanted to show you that we could also return it\n",
    "    # from the train function itself\n",
    "    train_acc = score(model, Xtrain, Ytrain)\n",
    "    test_acc = score(model, Xtest, Ytest)\n",
    "    test_cost = get_cost(model, loss, Xtest, Ytest)\n",
    "\n",
    "    print(\"Epoch: %d, cost: %f, acc: %.2f\" % (i, test_cost, test_acc))\n",
    "\n",
    "    # for plotting\n",
    "    train_costs.append(cost / n_batches)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_costs.append(test_cost)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(train_costs, label='Train cost')\n",
    "plt.plot(test_costs, label='Test cost')\n",
    "plt.title('Cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label='Train accuracy')\n",
    "plt.plot(test_accuracies, label='Test accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}