# 并行基本概念

本文补充一些并行计算的基本概念，方便后续看到相关词汇时明白什么意思。

参考资料主要有：

- [Python Concurrency & Parallel Programming](https://realpython.com/learning-paths/python-concurrency-parallel-programming/)
- [python-parallel-programming-cookbook-cn](https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html)
- [Parallel-Programming-with-Python](https://github.com/Voidly/Parallel-Programming-with-Python)
- [pydata/parallel-tutorial](https://github.com/pydata/parallel-tutorial)
- [pyspark-tutorial](https://sparkbyexamples.com/pyspark-tutorial/)
- [PySpark Tutorial | PySpark Tutorial For Beginners | Apache Spark With Python Tutorial | Simplilearn](https://www.youtube.com/watch?v=5dARTeE6OpU)
- [PySpark Tutorial | Python Spark | Intellipaat](https://www.youtube.com/watch?v=T6PDeK9kdHY)
- [Spark Python Notebooks](https://github.com/jadianes/spark-py-notebooks)

以下是一些基本概念。

首先并行计算和并发不太一样，并发是多用户争用CPU，在纷争发生的阶段将被CPU调度器所控制，CPU调度器的功能是决定在一个特定的时刻，哪个worker更适合使用资源。并行更多是指多核环境中同时执行指定的任务，在这些任务中它们不需要并发地接触CPU。

另外还有一个词是分布式编程，是指在物理分离的计算机器(节点)之间通过消息交换数据来分享进程。

下面先补充一些并行编程的架构相关的基本概念，简单了解即可，知道有这些名词，更多得是对应高性能计算机的。然后后面的记录还是以自己电脑上的并行处理为主，后面再看情况是否需要进一步学习了解。

解决一个大问题的一般方法是将其拆分成若干个小的独立的问题，然后分别解，并行程序就是这样。用多个处理器同时工作，来完成同一个任务。每一个处理器都做自己独立的部分，计算过程中处理器之间可能会交换数据。

提高计算能力有两种思路：提高处理器的时钟速度；增加芯片上的核心数。前者会增加散热，所以有上限，后者更可行。所以限制计算机硬件提供的一般都是多核心架构，即一个芯片上同时放多个处理器。GPU制造商也逐渐引入了这种基于多处理核心的硬件架构。今天的计算机几乎都是各种多核异构的计算单元组成的，每个单元有多个处理核心。

## 并行计算的内存架构

根据指令的同时执行和数据的同时执行，计算机系统可以分成以下四类：

- 单处理器，单数据 single instruction single data(SISD)
- 单处理器，多数据 (SIMD)
- 多处理器，单数据 (MISD)
- 多处理器，多数据 multiple instructions multiple data(MIMD)

SISD 就是单CPU机器，在单一数据流上执行指令。在SISD中，指令被顺序地执行。对于每一个CPU时钟，CPU按照下面的顺序执行：

- Fetch: CPU 从一片内存区域中（寄存器）获得数据和指令
- Decode: CPU对指令进行解码
- Execute: 该执行在数据上执行，将结果保存在另一个寄存器中

execute完成之后，CPU就回到第一步执行下一个时钟循环。这种冯诺依曼体系架构的主要元素有 中心内存单元，CPU和I/O系统。

传统的单处理器计算机都是经典的SISD系统。

MISD中有n个处理器，每一个都有自己的控制单元，共享同一个内存单元。在每一个CPU时钟中，从内存获得得数据会被所有得处理器同时处理，每个处理器按照自己得控制单元发送的指令处理。在这种情况下，并行实际上是指令层面的并行，多个指令在相同的数据上操作。用这种架构的比较少。

SIMD 计算机包括多个独立的处理器，每一个都有自己的局部内存，有n个数据流，每个处理器处理一个，所有处理器同时处理每一步，不同的数据上执行相同的指令。很多超级计算机使用这种架构。

MIMD是最广泛应用的一类。n个处理器，n个指令流，n个数据流。每一个处理器都有自己的控制单元和局部内存，每一个处理器都在独立的控制单元分配的指令流下工作。架构是通过线程或进程层面的并行来实现的，即处理器一般是异步工作的。不过异步的算法较难。

## 内存管理

内存管理是并行架构获取数据的方式。不论处理单元多块，如果内存提供指令和数据的速度不行，那么系统性能就被限制了。制约内存的响应时间的主要因素是内存存取周期，即连续两次读或写操作所需间隔的最小时间。处理器的周期通常比内存周期短得多。处理器传送数据到内存时，内存依旧在一个周期中，其他任何设备都不能使用内存，内存必须先对上一个请求做出响应。

两种内存管理系统，一是共享内存系统，其有大量的虚拟内存空间，且各个处理器对内存中的数据和指令有平等的访问权限。另一种是分布式内存模型，其中，每个处理器都有自己专属的内存，其他处理器不能访问。两种的主要区别就是以处理器的角度来说，内存和虚拟内存体系的不同。

load R0,i 意味着将i内存单元的内容加载进R0寄存器，但内存管理方式的不同，处理器的处理方式也不同。在共享内存系统中，i代表的是内存的全局地址，对系统中所有处理器来说都指向同一块内存空间。在分布式内存存系统中，i是局部地址。程序员需要能准确区分共享内存和分布式内存。我主要关注科学计算，这里先对其有个概念即可。

总之，共享内存就是同步的；分布式内存没有cache一致性问题，但是要实现处理器间的通讯。

始结束最快的计算机都是基于大规模并行处理机器massively parallel processing（MPP）的架构的，是工作站集群的形式。

GPU能提供高性能计算，但是不能把它看作一个独立处理单元，因为其必须在CPU的配合下才能顺利工作。异构计算的程序首先让CPU通过多种方式计算和控制任务，将计算密集型和具有高并行性得到任务分配给GPU执行。CPU和GPU之间可以通过高速总线通讯。一般是使用CUDA等编程框架提供的库来操作内存。

## 并行编程模型

- 共享内存模型：所有任务共享一个内存空概念，对共享资源的读写是异步的。通过一些机制，如锁来控制共享内存的访问权。
- 多线程模型：单个处理器可以有多个执行流程，通常这类模型会用作共享内存框架中，所以线程间的同步控制比较重要。
- 分布式内存/消息传递模型：在分布式内存系统中应用。程序员需要确定并行和通过消息产生的数据交换。
- 数据并行模型：多个任务需要操作同一个数据结构，但每个任务操作的是数据的不同部分。在共享内存架构中，所有任务都通过共享内存来访问数据；在分布式内存架构中则会将数据分割并且保存到每个任务的局部内存中。为了实现这个模型，程序员必须指定数据的分配方式和对齐方式。现代的GPU在数据已对齐的情况下运行的效率非常高。

## 并行程序设计

并行算法设计是基于一系列操作的，在编程过程中必须执行这些操作来准确完成工作而不产生错误，并行算法大致操作如下：

- 任务分解 (Task decomposition)：第一阶段，将软件程序分解为可以在不同处理器执行的多个任务或一系列指令以实现并行性。
- 任务分配 (Task assignment)：向各个处理器之间分配工作。负载均衡是这个阶段的关键，所有处理器都应该保持工作状态，避免长时间的空闲。
- 聚合 (Agglomeration)：将小任务合并成大任务的过程。
- 映射 (Mapping)：在并行算法设计的映射阶段，会指定任务由哪个处理器处理。这阶段的目标是减少总体的执行时间。

## 并行程序性能

几个性能指标：加速比，效率和扩展性。

加速比用于衡量使用并行方式解决问题的收益。假设使用单个处理单元解决这个问题需要的时间为 TS ，使用 p 个相同的处理单元解决这个问题的时间为 TP ，那么加速比 S=TS/TP 。如果 S=p ，加速比为线性，也就是说执行速度随着处理器数量的增加而加快。当然，这只是一个理想状态。当 TS 为最佳串行算法的执行时间，加速比是绝对的，而当 TS 为并行算法在单个处理器上的执行时间，那么加速比是相对的。

下面概括了上述的情况：

- S=p 为线性加速比，也是理想加速比。
- S<p 为真实加速比
- S>p 为超线性加速比

在理想状态下，如果一个并行系统有 p 个处理单元，那么它能提供的加速比等于 p 。然而，这几乎是不可能达到的。处理单元空转和通讯通常会浪费一些时间。效率通常是用于评价处理器在执行任务时是否被充分利用的性能指标，它跟通讯和同步所耗费的时间作比较。

假设效率为 E ，可以通过 E=s/p 算出。拥有线性加速比的算法的效率 E=1；在其它情况下，E 会小于1。下面会定义三种情况：

- 当 E=1，为线性加速比。
- 当 E<1，为真实情况。
- 当 E<<1，可以确定这是一个有问题的低效并行算法。

伸缩性用于度量并行机器高效运行的能力，代表跟处理器数量成比例的计算能力 (执行速度)。如果问题的规模和处理器的数量同时增加，性能不会下降。在依靠各种因素叠加的可伸缩系统中，可以保持相同的效率或者有更高的效率。

## 并行Python

作为一种解释型的语言，Python的速度并不算慢。Python的一种常见应用场景是实现高级的逻辑。Python的解释器就是用C语言写的，即CPython。解释器将Python转换成一种中间语言，叫做Python字节码，类似于汇编语言，但是包含一些更高级的指令。当一个运行一个Python程序的时候，评估循环不断将Python字节码转换成机器码。解释型语言的好处是方便编程和调试，但是程序的运行速度慢。

- 其中的一种解决办法是，用C语言实现一些第三方的库，然后在Python中使用。个人认为比较适合很多模型的编写，模型可以用C语言来实现，然后提供相应的Python接口，比如VIC model。再比如Numpy。有三个概念使NumPy具有强大的功能：向量化/广播/索引编制。**用数组表达式替换显式循环**的做法通常称为向量化。通常，向量化数组运算通常比纯Python运算要快一个或两个（或更多）个数量级。在Python中遍历数组或任何数据结构时，涉及很多开销。NumPy中的矢量化操作在内部将循环委派给高度优化的C和Fortran函数，从而使Python代码更干净，更快。
- 另一种方法是使用即时编译器来替换Cpython，例如PyPy，PyPy对代码生成和Python的运行速度做了优化。这种个人见得不多，暂不考虑。
- **第三种方法**：Python提供了很多可以利用并行的模块，这里也重点记录这些并行编程的模块。

接下来，先补充两种基本概念：线程和进程，以及它们在Python中的表现。

- 进程是应用程序的一个执行实例，比如，在桌面上双击浏览器图标将会运行一个浏览器。
- 线程是一个控制流程，可以在进程内与其他活跃的线程同时执行。“控制流程”指的是顺序执行一些机器指令。

进程可以包含多个线程，所以开启一个浏览器，操作系统将创建一个进程，并开始执行这个进程的主线程。每一个线程将独立执行一系列的指令（通常就是一个函数），并且和其他线程并行执行。然而，**同一个进程内的线程可以共享一些地址空间和数据结构**。线程也被称作“轻量进程”，因为它和进程有许多共同点，比如都是可以和其他控制流程同时运行的控制流程，说它“轻量”是因为实现一个进程比线程要繁重的多。重申一遍，不同于进程，多个线程可以共享很多资源，特别是地址空间和数据结构等。

总结一下：

- 进程可以包含多个并行运行的线程。
- 通常，操作系统创建和管理线程比进程更能节省CPU的资源。线程用于一些小任务，进程用于繁重的任务——运行应用程序。
- 同一个进程下的线程共享地址空间和其他资源，进程之间相互独立。

以上是一些基本概念，后面会更多的记录python并行相关的技术知识。
