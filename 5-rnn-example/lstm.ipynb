{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM实践\n",
    "\n",
    "循环神经网络的变种LSTM模型也有很多种结构，从常规的lstm，到各类复杂结构，这里根据实际使用情况，以pytorch为主，也会有tf2的例子，逐步记录。\n",
    "\n",
    "## vanilla lstm\n",
    "\n",
    "常规的lstm模型。\n",
    "\n",
    "lstm模型的相关基础知识可以参考：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。\n",
    "\n",
    "这里简单总结下。首先RNN的结构如图所示：\n",
    "\n",
    "![](RNN-unrolled.png)\n",
    "\n",
    "RNN中A就是一个激活函数tanh。两个A之间的箭头意思是把第一个A输出的h传递给第二个A。也就是说，第二个A的输入是concatenate了上一个时段的输出和本时段的输入。\n",
    "\n",
    "那么LSTM就是将A进一步优化，引入了Cell State，通过几个gate来控制节点 来决定记忆哪些，忘记哪些，其中一个四门的形式的RNN就是LSTM。四门如下：\n",
    "\n",
    "![](Picture10.png)\n",
    "\n",
    "橙色是forget gate layer。\n",
    "\n",
    "黄色包括两小部分，是为了确定要记忆什么，第一小部分是input gate layer；第二小部分是初始记忆单元作为初始cell state。\n",
    "\n",
    "绿色是更新cell state，结合遗忘后剩下的历史cell state和刚黄色部分决定本次记忆的cell state 组成新的cell state\n",
    "\n",
    "蓝色就是最后利用新生成的cell state，结合上个时段的h，生成新的h。\n",
    "\n",
    "所以和RNN不同，LSTM不同时间段之间的A之间的联系包括h和cell state两种。\n",
    "\n",
    "还有一个内容比较重要，需要了解下，就是dropout。\n",
    "\n",
    "参考：[Dropout在RNN中的应用综述](https://lonepatient.top/2018/09/24/a-review-of-dropout-as-applied-to-rnns.html)，及原文[A review of Dropout as applied to RNNs](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b)\n",
    "\n",
    "前面已经说过了Dropout基本概念。\n",
    "\n",
    "RNN与仅前馈神经网络的不同之处在于**先前的状态反馈到网络中，允许网络保留先前状态**。因此，将标准dropout应用于RNN会**限制网络保留先前状态的能力**，从而阻碍其性能。 Bayer等人指出了将dropout应用于递归神经网络（RNN）的问题。如果将完整的输出权重向量设置为零，则“在每次前向传递期间导致RNN动态变化是非常显著的。”\n",
    "\n",
    "首先看下RNN的前向计算代码：\n",
    "\n",
    "```python\n",
    "class RNN:\n",
    "# ...\n",
    "    def step(self, x):\n",
    "        # update the hidden state\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # compute the output vector\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y\n",
    "rnn = RNN()\n",
    "y = rnn.step(x) # x is an input ve\n",
    "```\n",
    "\n",
    "作为克服应用于RNN的dropout性能问题的一种方法，Zaremba和Pham等仅将dropout应用于非循环连接（Dropout未应用于隐藏状态）。 “通过不对循环连接使用dropout，LSTM可以从dropout正则化中受益，而不会牺牲其宝贵的记忆能力。”\n",
    "\n",
    "![](79805875.jpg)\n",
    "\n",
    "即 Dropout仅适用于非循环连接（即仅应用于前馈虚线）。 粗线显示了LSTM中典型的信息流路径。 信息受到dropoutL + 1次的影响，其中L是网络的深度。\n",
    "\n",
    "Gal和Ghahramani（2015）分析了将Dropout应用于仅前馈RNN的部分，发现这种方法仍然导致过拟合。 他们提出了“**变分Dropout**”，通过**重复“输入，输出和循环层的每个时间步长相同的dropout掩码（在每个时间步骤丢弃相同的网络单元）**”，使用贝叶斯解释，他们看到了语言的改进 建模和情感分析任务超过’纯dropout’。\n",
    "\n",
    "![](14161715.jpg)\n",
    "\n",
    "即朴素dropout（a）在不同的时间步长使用不同的掩模，在循环层上没有dropout。 变分dropout（b）在每个时间步长使用相同的压差掩模，包括循环层（颜色表示Dropout掩模，实线表示dropout，虚线表示没有dropout的标准连接）。\n",
    "\n",
    "与Moon和Gal和Ghahramani一样，Semeniuta等人提出**将dropout应用于RNN的循环连接**，以便可以**对循环权重进行正则化以提高性能**。 Gal和Ghahramani使用**网络的隐藏状态**作为计算门值和小区更新以及使用dropout的子网络的输入来正则化子网络（上图b）。 Semeniuta等人的不同之处在于他们认为“整个架构以隐藏状态为关键部分并使整个网络正则化”。 这类似于Moon等人的概念（如上图a所示），但Semeniuta等人发现根据Moon等人直接丢弃先前的状态产生了混合结果，而将dropout应用于隐藏状态更新向量是一种更有原则的方法。\n",
    "\n",
    "![](52714967.jpg)\n",
    "\n",
    "上图是 Semeniuta等人论文中在LSTM网络的循环连接中三种类型的dropout的例证。 虚线箭头表示断开的连接。 为清楚起见，省略了输入连接。“ 注意Semeniuta等人如何。 （2016）将重复dropout应用于LSTM存储器单元的更新。\n",
    "\n",
    "设置权重dropout的代码关键部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setweights(self):\n",
    "    for name_w in self.weights:\n",
    "        raw_w = getattr(self.module, name_w + '_raw')\n",
    "        w = None\n",
    "        if self.variational:\n",
    "            mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n",
    "            if raw_w.is_cuda: mask = mask.cuda()\n",
    "            mask = torch.nn.functional.dropout(mask, p=self.dropout,\n",
    "             training=True)\n",
    "            w = mask.expand_as(raw_w) * raw_w\n",
    "        else:\n",
    "            w = torch.nn.functional.dropout(raw_w, p=self.dropout,\n",
    "         training=self.training)\n",
    "        setattr(self.module, name_w, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么接下来看看lstm和实际问题时如何联系起来的呢？\n",
    "\n",
    "径流预报中比较常见的是单输出问题，也就是N对1问题。每个时段都有多个输入值，也就是前面图中的x是个多维向量。输出h是标量值。\n",
    "\n",
    "那么具体实践是什么样呢？\n",
    "\n",
    "### LSTM in PyTorch\n",
    "\n",
    "pytorch中，LSTM类继承自RNNBase类，RNNBase类继承自Module类，Module类是pytorch中完成一定网络功能的基类，可以通过继承该类定义自己的神经网络。自己实现神经网络时，一般要重写其forward方法。\n",
    "\n",
    "Module实现了__call__方法，这意味着其可被当做可调用方法使用。比如下面有用到lstm()。\n",
    "\n",
    "pytorch 的LSTM单元接收的输入必须是三维张量：\n",
    "\n",
    "- 第一维反映序列seq的结构，即seq内的个数\n",
    "- 第二维是minibatch，即一次喂给网络的seq数目\n",
    "- 第三维是输入元素，每个元素具体是多少维的向量\n",
    "\n",
    "第一维和第二维和直观上的维度顺序不太一致，因为 pytorch 对tensor是sequence first排列维度的。\n",
    "\n",
    "- batch first: $a_1 a_2 a_3 a_4 a_5|b_1 b_2 b_3 b_4 b_5$\n",
    "- sequence first: $a_1 b_1| a_2 b_2| a_3 b_3| a_4 b_4| a_5 b_5$\n",
    "\n",
    "所以pytorch中不同seq同一时刻对应的输入单元在内存中毗邻，这样可以快速读取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.2682,  0.0304, -0.1526]]], grad_fn=<StackBackward>), tensor([[[-1.0766,  0.0972, -0.5498]]], grad_fn=<StackBackward>))\n",
      "tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.5370,  0.0346, -0.1958]]], grad_fn=<StackBackward>), tensor([[[-1.1552,  0.1214, -0.2974]]], grad_fn=<StackBackward>))\n",
      "tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.3947,  0.0391, -0.1217]]], grad_fn=<StackBackward>), tensor([[[-1.0727,  0.1104, -0.2179]]], grad_fn=<StackBackward>))\n",
      "tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.1854,  0.0740, -0.0979]]], grad_fn=<StackBackward>), tensor([[[-1.0530,  0.1836, -0.1731]]], grad_fn=<StackBackward>))\n",
      "tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"一个简单的pytorch的lstm模型的示例\"\"\"\n",
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd  # torch中自动计算梯度模块\n",
    "import torch.nn as nn             # 神经网络模块\n",
    "import torch.nn.functional as F   # 神经网络模块中的常用功能\n",
    "import torch.optim as optim       # 模型优化器模块\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "lstm = nn.LSTM(3, 3)\n",
    "# 生成一个长度为5，每一个元素为1*3的序列作为输入，这里的数字3对应于上句中第一个3\n",
    "inputs = [autograd.Variable(torch.randn((1, 3))) for _ in range(5)]\n",
    "\n",
    "# 设置隐藏层维度，初始化隐藏层的数据。hidden变量是一个元组，其第一个元素是LSTM隐藏层输出，另一个元素维护隐藏层的状态。\n",
    "# torch.rand(1,1,3)就是生成了一个维度为(1,1,3)的以一定高斯分布生成的张量\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM in Keras\n",
    "\n",
    "一个实例，代码来自博客 [How to Develop Multi-Step LSTM Time Series Forecasting Models for Power Usage](https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# fill missing values with a value at the same time one day ago\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if np.isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    "\n",
    "\n",
    "# load all data\n",
    "dataset = pd.read_csv('../data/household_power_consumption.txt', sep=';', header=0, low_memory=False,\n",
    "                      infer_datetime_format=True, parse_dates={'datetime': [0, 1]}, index_col=['datetime'])\n",
    "\n",
    "# mark all missing values\n",
    "dataset.replace('?', np.nan, inplace=True)\n",
    "# make dataset numeric\n",
    "dataset = dataset.astype('float32')\n",
    "# fill missing\n",
    "fill_missing(dataset.values)\n",
    "\n",
    "# add a column for for the remainder of sub metering\n",
    "values = dataset.values\n",
    "dataset['sub_metering_4'] = (values[:, 0] * 1000 / 60) - (values[:, 4] + values[:, 5] + values[:, 6])\n",
    "\n",
    "# save updated dataset\n",
    "dataset.to_csv('../data/household_power_consumption.csv')\n",
    "\n",
    "# resample minute data to total for each day\n",
    "# load the new file\n",
    "dataset = pd.read_csv('../data/household_power_consumption.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'],\n",
    "                      index_col=['datetime'])\n",
    "# resample data to daily\n",
    "daily_groups = dataset.resample('D')\n",
    "daily_data = daily_groups.sum()\n",
    "# summarize\n",
    "print(daily_data.shape)\n",
    "print(daily_data.head())\n",
    "# save\n",
    "daily_data.to_csv('../data/household_power_consumption_days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate multi-step lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[1:-328], data[-328:-6]\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train) / 7))\n",
    "    test = array(split(test, len(test) / 7))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col]) ** 2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=7):\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0] * train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end < len(data):\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "# train the model\n",
    "def build_model(train, n_input):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 70, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n",
    "\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = array(history)\n",
    "    data = data.reshape((data.shape[0] * data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, 0]\n",
    "    # reshape into [1, n_input, 1]\n",
    "    input_x = input_x.reshape((1, len(input_x), 1))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input):\n",
    "    # fit model\n",
    "    model = build_model(train, n_input)\n",
    "    # history is a list of weekly data\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = array(predictions)\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores\n",
    "\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('../data/household_power_consumption_days.csv', header=0, infer_datetime_format=True,\n",
    "                   parse_dates=['datetime'], index_col=['datetime'])\n",
    "# split into train and test\n",
    "train, test = split_dataset(dataset.values)\n",
    "# evaluate model and get scores\n",
    "n_input = 7\n",
    "score, scores = evaluate_model(train, test, n_input)\n",
    "# summarize scores\n",
    "summarize_scores('lstm', score, scores)\n",
    "# plot scores\n",
    "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
    "pyplot.plot(days, scores, marker='o', label='lstm')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacked lstm\n",
    "\n",
    "先对stacked lstm有个基本认识，参考：[Single Layer & Multi-layer Long Short-Term Memory (LSTM) Model with Intermediate Variables for Weather Forecasting](https://www.sciencedirect.com/science/article/pii/S187705091831439X)，一个典型的stacked lstm结构是这样的：\n",
    "\n",
    "![](Picture9.png)\n",
    "\n",
    "接下来参考[Stacked Long Short-Term Memory Networks](https://machinelearningmastery.com/stacked-long-short-term-memory-networks/)记录一些基本内容。\n",
    "\n",
    "原始的lstm就是由一个LSTM隐含层和一个标准前向输出层组成。 Stacked LSTM 是对该模型的扩充，它有多个LSTM隐含层，每层有多个记忆单元。\n",
    "\n",
    "### 为什么要增加深度\n",
    "\n",
    "神经网络的深度通常被认为是该方法在一系列具有挑战性的预测问题上取得成功的原因。更多的隐含层能让神经网络更深。更深的隐含层目前被认为能从之前的层中重新组合学习到的特征并创建新的更高层次的抽象特征。比如从线到面到体。\n",
    "\n",
    "一个足够深的神经网络结构能用来估计大部分函数。增加网络的深度提供了一种需要更少的神经元和更快的训练的解决方案。因此，增加深度是一种典型的优化方式。\n",
    "\n",
    "### Stacked LSTM Architecture\n",
    "\n",
    "因为LSTM是作用于序列数据，这意味着随着时间推移，层数的增加增加了输入观察的抽象级别。实际上就是将观察结果随时间分块或在不同的时间尺度上表示问题。\n",
    "\n",
    "引用一段话：\n",
    "\n",
    "\" ... building a deep RNN by stacking multiple recurrent hidden states on top of each other. This approach potentially allows the hidden state at each level to operate at different timescale\"\n",
    "\n",
    "— [How to Construct Deep Recurrent Neural Networks, 2013](https://arxiv.org/abs/1312.6026)\n",
    "\n",
    "在一些研究中，LSTM的深度比一个给定层的memory cells的个数对模型来说更重要。Stacked LSTMs 现在已经是序列预测问题的一种较稳定的技术。一个Stacked LSTM模型可以由一个LSTM模型和多个LSTM层构成。如下图所示，上面的LSTM层不输出一个值，而是输出一个序列到下一层LSTM。具体来说，每个输入时间步一个输出，而不是所有输入时间步一个输出。\n",
    "\n",
    "### Implement Stacked LSTMs in Keras\n",
    "\n",
    "在Keras中可以很容易的创建Stacked LSTM，每个LSTM memory cell 需要一个3D输入，当LSTM处理一个时间步长的输入序列时，每个存储单元将以2D数组的形式输出整个序列的单个值。\n",
    "\n",
    "首先是一个单层的LSTM的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25776a27f53e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Example of one output for whole sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# define model where LSTM is also output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Example of one output for whole sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model where LSTM is also output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, input_shape=(3,1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# input time steps\n",
    "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到输入数据有3个值。运行例子输出一个值，2D数组形式。\n",
    "\n",
    "为了堆叠LSTM层，需要修改LSTM层输出一个3D的数组作为下一层的输入。设置return_sequences=True，它就会输出一个序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of one output for each input time step\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model where LSTM is also output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, return_sequences=True, input_shape=(3,1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# input time steps\n",
    "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后就可以增加LSTM层了。比如：\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(..., return_sequences=True, input_shape=(...)))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n",
    "```\n",
    "\n",
    "或者更多层：\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(..., return_sequences=True, input_shape=(...)))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
