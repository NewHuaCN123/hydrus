{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境安装与基本概念\n",
    "\n",
    "使用一个工具，最先开始的一定就是安装了。本文件夹下主要针对pytorch进行介绍，也会补充一些tensorflow-keras的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## pytorch 与 tensorflow 安装\n",
    "\n",
    "### ubuntu18.04安装pytorch1.2\n",
    "\n",
    "#### 版本搭配  \n",
    "\n",
    "ubuntu18.04+cuda10.0+cudnn7.6.4+anaconda_python3.7+pytorch1.2\n",
    "\n",
    "``` Shell\n",
    "$ sudo lshw -C display | grep product\n",
    "product: GP102 [GeForce GTX 1080 Ti]\n",
    "```\n",
    "\n",
    "#### 驱动安装\n",
    "\n",
    "先检查GPU的兼容性，[官网](https://developer.nvidia.com/cuda-gpus)上查看compute capability，满足大于3或者3.5的要求即可。\n",
    "\n",
    "然后保证安装到Ubuntu上的包是最新的，因此\n",
    "\n",
    "``` Shell\n",
    "$ sudo apt update\n",
    "$ sudo apt upgrade #will ask you Y/n if you want to upgrade\n",
    "``` \n",
    "\n",
    "关于NVIDIA 的驱动：\n",
    "\n",
    "Ubuntu预装了GPU的通用驱动。这些驱动不是优化的。因此需要找到适合自己的GPU的最新的NVIDIA驱动。有几种选择：\n",
    "\n",
    "- Nvidia PPA：非常好的选择。通过使用PPA中包含的驱动达到开箱即用的效果。\n",
    "- Ubuntu Default recommended driver：Ubuntu能指出自己的电脑需要的NVIDIA驱动\n",
    "- Nouveau：NVIDIA驱动的开源实现版本。\n",
    "- Official NVIDIA site：和PPA的一样，但是不会自动更新，并且有时在更新、卸载或者安装中会报错。\n",
    "\n",
    "推荐的，也是比较好的方式是使用NVIDIA PPA来安装驱动。因为有最新的NVIDIA官方驱动，也在Ubuntu上测试过，且安装过程比较平滑。\n",
    "\n",
    "``` Shell\n",
    "$ sudo add-apt-repository ppa:graphics-drivers/ppa\n",
    "$ sudo apt-get update\n",
    "``` \n",
    "\n",
    "此语句将PPA库添加到Ubuntu的包系统内（Ubuntu是一种Debian Linux的发行版，因此使用的是Debian的dpkg包系统，该系统提供给Ubuntu应用来安装。高级包工具（APT）使得我们能很容易的在终端与dpkg交互）。\n",
    "\n",
    "接下来需要决定安装哪个版本的driver。去[PPA库网站](https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa)上，\n",
    "网页拉到最下面检查版本，发现很多版本都可以；也可以在[官网](https://www.nvidia.cn/Download/index.aspx?lang=cn)上检查，官网会给出一个版本；\n",
    "\n",
    "在命令行输入：\n",
    "\n",
    "``` Shell\n",
    "$ ubuntu-drivers devices\n",
    "``` \n",
    "\n",
    "可以看到给出了几个版本的驱动。\n",
    "\n",
    "结合三者我决定选择nvidia-driver-410，因为410是对应cuda10.0的版本。\n",
    "\n",
    "（**注意**：2019年10月24日看官网，支持的cuda版本已经为10.1，所以可以安装nvidia-driver-430，不过以内容下仍安装cuda10.0的安装写的）\n",
    "\n",
    "``` Shell\n",
    "$ sudo apt install nvidia-driver-410\n",
    "``` \n",
    "\n",
    "安装完毕之后需要重启电脑，直接命令行reboot即可，以使用新的显卡驱动：\n",
    "\n",
    "``` Shell\n",
    "$ reboot\n",
    "``` \n",
    "\n",
    "重启之后，在命令行键入：\n",
    "\n",
    "``` Shell\n",
    "$ nvidia-smi\n",
    "``` \n",
    "\n",
    "查看是否已成功安装，出现一些信息基本上就是安装成功了。\n",
    "\n",
    "如果安装的是430，问题也不大，即显示的信息里说CUDA版本是10.1的时候，\n",
    "根据别人的[经验](https://zhuanlan.zhihu.com/p/73787970 )，装cuda-10.0也是可以的。 \n",
    "\n",
    "为什么一定是10.0,？因为pytorch目前（2019年10月1日）支持的cuda是10.0。\n",
    "\n",
    "为了保证驱动正常，所以有必要暂停其升级，使用以下命令：\n",
    "\n",
    "``` Shell\n",
    "$ sudo apt-mark hold nvidia-driver-410\n",
    "nvidia-driver-410 set on hold.\n",
    "``` \n",
    "\n",
    "To reverse this operation run:\n",
    "\n",
    "``` Shell\n",
    "$ sudo apt-mark unhold nvidia-driver-410\n",
    "``` \n",
    "\n",
    "#### 安装CUDA\n",
    "\n",
    "首先check下gcc和g++的版本。\n",
    "\n",
    "``` Shell\n",
    "$ gcc --version\n",
    "gcc (Ubuntu 7.3.0–16ubuntu3) 7.3.0\n",
    "$ g++ --version\n",
    "g++ (Ubuntu 7.3.0–16ubuntu3) 7.3.0\n",
    "```\n",
    "\n",
    "安装cuda10.0和9.0不太一样。直接从[官网](https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=runfilelocal)下载\n",
    "（用deb安装时我的报错了，不知道什么错误，有可能是因为cuda和driver的版本不完全匹配，所以用runfile安装了）\n",
    "\n",
    "有提到要安装下kernel headers。（如果某个程序需要内核提供的一些功能，它就需要内核来编译程序，这个时候用的上kernel heads）。但几个也都没说。\n",
    "\n",
    "``` code\n",
    "$ sudo apt install linux-headers-$(uname -r)\n",
    "```\n",
    "\n",
    "执行命令即可安装，发现已经安装了。\n",
    "\n",
    "按照官网提示安装即可，注意因为之前已经安装过驱动了，所以这里提示问是否安装驱动时（Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 XXX.XX），选择no，其他的都可以选择yes或者默认值即可。\n",
    "\n",
    "``` code\n",
    "$ sudo ./cuda_10.0.130_410.48_linux.run\n",
    "```\n",
    "\n",
    "重启下看看自己没有黑屏。\n",
    "\n",
    "然后配置环境变量。\n",
    "\n",
    "``` code\n",
    "$ vim ~/.bashrc #打开配置文件\n",
    "$ export PATH=/usr/local/cuda-10.0/bin:$PATH\n",
    "$ export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:$LD_LIBRARY_PATH\n",
    "$ export CUDA_HOME=/usr/local/cuda-10.0\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "如果没有vim，安装即可，在命令行中输入：vim，没有安装的话会有提示，按照提示安装即可。\n",
    "\n",
    "最后验证cuda是否安装成功：\n",
    "\n",
    "``` code\n",
    "$ cd /usr/local/cuda-10.0/samples/1_Utilities/deviceQuery\n",
    "$ sudo make\n",
    "$ ./deviceQuery\n",
    "```\n",
    "\n",
    "#### 安装CUDNN\n",
    "\n",
    "官网下载最新版的对应cuda10.0版本的CUDNN。\n",
    "\n",
    "两种方法都有：\n",
    "\n",
    "- 下载tar压缩包的：cudnn安装较容易只需要把文件解压后拷贝进cuda根目录即可。\n",
    "- 下载三个deb文件的（the runtime library, the developer library, and the code samples library for Ubuntu 18.04）：\n",
    "\n",
    "``` code\n",
    "$ sudo dpkg -i libcudnn7_7.6.4.38-1+cuda10.0_amd64.deb\n",
    "$ sudo dpkg -i libcudnn7-dev_7.6.4.38-1+cuda10.0_amd64.deb\n",
    "$ sudo dpkg -i libcudnn7-doc_7.6.4.38-1+cuda10.0_amd64.deb \n",
    "```\n",
    "\n",
    "安装完成验证是否安装成功：\n",
    "Go to the MNIST example code: \n",
    "\n",
    "``` code\n",
    "$ cd /usr/src/cudnn_samples_v7/mnistCUDNN/\n",
    "```\n",
    "\n",
    "Compile the MNIST example:\n",
    "\n",
    "``` code\n",
    "$ sudo make clean && sudo make\n",
    "```\n",
    "\n",
    "Run the MNIST example:\n",
    "\n",
    "``` code\n",
    "$ ./mnistCUDNN \n",
    "```\n",
    "\n",
    "If your installation is successful, you should see Test passed!\n",
    "现在已经不需要将gcc和g++降到6.X版本就可以执行。 \n",
    "\n",
    "#### 安装python\n",
    "\n",
    "基础环境搭建好了之后，安装anaconda，现在（2019年10月1日）pytorch已经支持python3.7了。\n",
    "下载linux下的安装包，执行：\n",
    "\n",
    "``` code\n",
    "$ sh Anaconda3-2019.07-Linux-x86_64.sh   #后边的文件名称是你的安装包的名称\n",
    "```\n",
    "\n",
    "然后一路按照默认的设置安装即可。\n",
    "如果询问是否添加路径到环境变量或者是是否添加conda init，都选yes。\n",
    "\n",
    "安装完成以后，重启终端。\n",
    "先输入：\n",
    "\n",
    "``` code\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "\n",
    "再输入：\n",
    "\n",
    "``` code\n",
    "$ python\n",
    "```\n",
    "\n",
    "即可看到安装成功的anaconda。\n",
    "\n",
    "#### 安装pytorch\n",
    "\n",
    "官网目前（2019年11月1日）已经是1.3版本了，对应cuda是10.1，这里安装的是cuda10.0，对应的pytorch版本是1.2，因此需要找[以前的版本](https://pytorch.org/get-started/previous-versions/)安装。版本对应上即可。使用conda安装的时候，可能会出现没有权限的情况，给自己的用户赋予conda和anaconda相关文件夹权限即可，命令如下（以我的为例，owen是用户名;/home/owen/.conda是我的conda的根目录）：\n",
    "\n",
    "``` code\n",
    "sudo chown -R owen /home/owen/.conda\n",
    "sudo chown -R owen /home/owen/anaconda3\n",
    "```\n",
    "\n",
    "使用conda安装完毕后，检查是否安装成功：\n",
    "\n",
    "``` code\n",
    "$ python\n",
    "$ import torch\n",
    "```\n",
    "\n",
    "能导入即成功。\n",
    "\n",
    "可以先安装vscode，再测试下能不能使用GPU加速。\n",
    "安装vscode直接去官网即可。然后下载对应的python插件即可使用。\n",
    "\n",
    "``` code\n",
    "$ sudo dpkg -i code_1.38.1-1568209190_amd64.deb\n",
    "```\n",
    "\n",
    "测试代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8250, 1.4750, 1.3342],\n",
      "        [0.2639, 1.5897, 0.5670],\n",
      "        [0.3504, 1.0048, 0.9221],\n",
      "        [1.1656, 1.0448, 0.8609],\n",
      "        [0.5290, 0.7211, 0.2174]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "x = t.rand(5,3)\n",
    "y = t.rand(5,3)\n",
    "if t.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试成功之后，就可以自己写程序试试了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ubuntu16.04下安装TensorFlow-gpu\n",
    "\n",
    "通过前面的步骤，应该已经安装好了cuda和cudnn了，这里就不再重复了。可以重新检查下自己的cuda和cudnn版本。\n",
    "\n",
    "看看自己的显卡信息：\n",
    "\n",
    "``` Shell\n",
    "$ nvidia-smi\n",
    "NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1 \n",
    "...\n",
    "``` \n",
    "\n",
    "上面这个cuda version可能不是你安装的，因为这里是说这个驱动匹配的cuda version\n",
    "\n",
    "然后看看gcc和g++，比如我的一个版本：\n",
    "\n",
    "``` Shell\n",
    "$ gcc --version\n",
    "gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\n",
    "$ g++ --version\n",
    "g++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\n",
    "``` \n",
    "\n",
    "可以看看cuda版本，比如：\n",
    "\n",
    "```Shell\n",
    "$ cat  /usr/local/cuda/version.txt\n",
    "CUDA Version 10.0.130\n",
    "```\n",
    "\n",
    "还可以查看cudnn是否安装：\n",
    "\n",
    "```Shell\n",
    "$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n",
    "cat: /usr/local/cuda/include/cudnn.h: No such file or directory\n",
    "```\n",
    "\n",
    "如果结果是上面这样的，说明cudnn没有安装好，可以看看cudnn是否已经下载：\n",
    "\n",
    "```Shell\n",
    "$ cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\n",
    "#define CUDNN_MAJOR 7\n",
    "#define CUDNN_MINOR 5\n",
    "#define CUDNN_PATCHLEVEL 1\n",
    "...\n",
    "```\n",
    "\n",
    "如上所示，是7.5.1版本，说明cudnn已经下载了，但是还没往cuda里面放。如果你没有权限安装，那么就要通知管理员来安装了。\n",
    "\n",
    "### win10下安装TensorFlow-gpu\n",
    "\n",
    "#### 版本搭配\n",
    "\n",
    "win10+cuda10.0+cudnn7.6.4+anaconda_python3.7+tensorflow2.0\n",
    "\n",
    "#### 具体步骤\n",
    "\n",
    "第一步，因为现在不论是TensorFlow2.0还是pytorch1.2都支持的是cuda10.0，而不完全支持cuda10.1，版本不匹配的话会有问题，因此cuda选择10.0。\n",
    "如果电脑安装了visual studio 2019，那么安装cuda10.0时候会提示缺少visual studio 2017相关组件，因此需要首先安装visual studio 2017。\n",
    "直接在visual studio官网上找，拉到页面最下面，找旧版本，不能下载的话，加入微软的开发者计划即可。这步下载安装过程可能比较长。\n",
    "\n",
    "第二步: 去官网下载安装NVIDIA Driver\n",
    "Select the appropriate version and click search\n",
    "\n",
    "第三步: Install Anaconda (Python 3.7 version)\n",
    "\n",
    "第四步: Update Anaconda\n",
    "Open Anaconda Prompt to type the following command(s)\n",
    "\n",
    "```Shell\n",
    "conda update conda\n",
    "conda update --all\n",
    "```\n",
    "\n",
    "第五步: 下载安装CUDA Tookit 10.0\n",
    "\n",
    "Choose your version depending on your Operating System\n",
    "\n",
    "第六步: Download cuDNN Download\n",
    "Choose your version depending on your Operating System. Membership registration is required.\n",
    "\n",
    "下载之后进行解压。\n",
    "\n",
    "Add cuDNN into Environment PATH\n",
    "Add the following path in your Environment. Subjected to changes in your installation path.\n",
    "\n",
    "第七步: 安装TensorFlow\n",
    "\n",
    "```Shell\n",
    "$ pip install --upgrade pip\n",
    "$ pip install tensorflow-gpu\n",
    "```\n",
    "\n",
    "接下来看看在一个已经安装过cuda和cudnn的win10电脑上如何使用pipenv安装tensorflow。\n",
    "\n",
    "首先检查cuda和cudnn的版本：\n",
    "\n",
    "打开cmd，输入：\n",
    "\n",
    "```Shell\n",
    "$ nvcc -V\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2019 NVIDIA Corporation\n",
    "Built on Sun_Jul_28_19:12:52_Pacific_Daylight_Time_2019\n",
    "Cuda compilation tools, release 10.1, V10.1.243\n",
    "```\n",
    "\n",
    "可以看到cuda的版本是10.1，接下来看看cudnn，因为是cuda10.1，所以进入cuda10.1的文件夹:C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include，找到cudnn.h，文本编辑器打开就可以看到：\n",
    "\n",
    "```Shell\n",
    "...\n",
    "#define CUDNN_MAJOR 7\n",
    "#define CUDNN_MINOR 6\n",
    "#define CUDNN_PATCHLEVEL 4\n",
    "...\n",
    "```\n",
    "\n",
    "如上图所示，我的是7.6.4版本。\n",
    "\n",
    "接下来使用pipenv来安装tensorflow。首先建好一个项目，进入项目根目录。执行:\n",
    "\n",
    "```Shell\n",
    "$ pipenv isntall\n",
    "```\n",
    "\n",
    "这时候就会创建虚拟环境了。\n",
    "\n",
    "然后在这里面安装tensorflow。根据现在最新的tensorflow版本情况，可以使用pip install tensorflow 方便的安装，这里使用pipenv install来安装：\n",
    "\n",
    "```Shell\n",
    "$ pipenv install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络基本概念\n",
    "\n",
    "进入正式介绍之前，先简单补充一些使用库的过程中会遇到的一些概念（前提：已经了解了神经网络的基本概念）。\n",
    "\n",
    "神经网络基本概念可以参考：[深度学习 Deep Learning](https://space.bilibili.com/88461692/channel/detail?cid=26587)。这里对一些实践中会碰到的概念做些补充，逐渐补充完善中...\n",
    "\n",
    "### Loss and Loss Functions\n",
    "\n",
    "本节参考了：[Loss and Loss Functions for Training Deep Learning Neural Networks](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)。\n",
    "\n",
    "loss和loss函数很平常了，不过对于其在nn中的角色和如何选择合适的还是需要了解下。\n",
    "\n",
    "### 神经网络简单实现\n",
    "\n",
    "用numpy手写神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# 首先是标定神经网络的结构\n",
    "NN_ARCHITECTURE = [{\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "                   {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "                   {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "                   {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "                   {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"}]\n",
    "\n",
    "\n",
    "def init_layers(nn_architecture, seed=99):\n",
    "    \"\"\"然后对权重进行初始化，为了保证每次计算的结果一致，初始化种子使用同样的值\"\"\"\n",
    "    # 首先随机初始化种子\n",
    "    np.random.seed(seed)\n",
    "    # 初始化关键变量\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx+1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        # 对每一层，初始化W矩阵和向量b。要注意矩阵和向量的维度，因为\n",
    "        params_values['W'+str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size)*0.1\n",
    "        params_values['b'+str(layer_idx)\n",
    "                      ] = np.random.randn(layer_output_size, 1)*0.1\n",
    "    return params_values\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"sigmoid激活函数\"\"\"\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"sigmoid函数导数在反向传播时的作用\"\"\"\n",
    "    sig = sigmoid(Z)\n",
    "    return dA*sig*(1-sig)\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"relu激活函数\"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"relu函数导数\"\"\"\n",
    "    # copy=True意味着构建了一个数组dA的副本，即深拷贝\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    # 直接令z<=0时的dz为0即可，剩下的相当于dA*1\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \"\"\"单层前向传递函数\"\"\"\n",
    "    Z_curr = np.dot(W_curr, A_prev)+b_curr\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception(\"Non-supported activation function\")\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    \"\"\"所有层前向传递\"\"\"\n",
    "    # 构建一个变量存储后续反向传播所需的各层信息\n",
    "    memory = {}\n",
    "    # X是输入\n",
    "    A_curr = X\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # layer_idx从1开始计数\n",
    "        layer_idx = idx+1\n",
    "        A_prev = A_curr\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\"+str(layer_idx)]\n",
    "        b_curr = params_values[\"b\"+str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(\n",
    "            A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        # memory中存储中间变量\n",
    "        memory[\"A\"+str(idx)] = A_prev\n",
    "        memory[\"Z\"+str(layer_idx)] = Z_curr\n",
    "    return A_curr, memory\n",
    "\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    cost = -1/m*(np.dot(Y, np.log(Y_hat).T)+np.dot(1-Y, np.log(1-Y_hat).T))\n",
    "    # 为什么要除去多余的维度？因为之前的计算导致的\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \"\"\"计算一层误差反向传播，输入dA_curr表示的是∂J/∂a^(l)，其中，l是指当前层。\n",
    "    输出是损失函数对上一层l-1层神经元值以及本层l层权重和偏置的偏导\"\"\"\n",
    "    # m表示的是样本个数\n",
    "    m = A_prev.shape[1]\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    # 因为每个偏导都包含dZ_curr，因此先求它。dZ_curr表示的是∂J/∂Z^(l)，链式法则=∂J/∂a^(l) * f'(Z^(l))（对应项相乘）\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    # 接下来几项就是三个偏导，注意有样本的影响，维度和单一样本推导时有所不同\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T)/m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    \"\"\"完整反向传播算法，给出所有参数偏导组成的完整梯度\"\"\"\n",
    "    grads_values = {}\n",
    "    # 在反向循环开始之前，给出最后输出的误差，初始化反向传播算法\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    dA_prev = -(np.divide(Y, Y_hat)-np.divide(1-Y, 1-Y_hat))\n",
    "    # 因为是反向传播算法，因此循环顺序是从后往前\n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev+1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "\n",
    "        dA_curr = dA_prev\n",
    "\n",
    "        A_prev = memory[\"A\"+str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\"+str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\"+str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\"+str(layer_idx_curr)]\n",
    "\n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values['dW'+str(layer_idx_curr)] = dW_curr\n",
    "        grads_values['db'+str(layer_idx_curr)] = db_curr\n",
    "    return grads_values\n",
    "\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    \"\"\"更新参数\"\"\"\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values['W'+str(layer_idx)] -= learning_rate * \\\n",
    "            grads_values['dW'+str(layer_idx)]\n",
    "        params_values['b'+str(layer_idx)] -= learning_rate * \\\n",
    "            grads_values['db'+str(layer_idx)]\n",
    "    return params_values\n",
    "\n",
    "\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_accuracy_value(y_hat, y):\n",
    "    \"\"\"为了显示优化的过程，计算accuracy\"\"\"\n",
    "    y_hat = convert_prob_into_class(y_hat)\n",
    "    return (y_hat == y).all(axis=0).mean()\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n",
    "    \"\"\"整体训练过程\"\"\"\n",
    "    # 初始化神经网络参数\n",
    "    param_values = init_layers(nn_architecture, 2)\n",
    "    # 为了后续可视化优化过程，存储中间阶段优化结果\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    # 按epoch循环训练神经网络\n",
    "    for i in range(epochs):\n",
    "        # 每一步里面，首先前向传播\n",
    "        Y_hat, cashe = full_forward_propagation(\n",
    "            X, param_values, nn_architecture)\n",
    "        # 然后计算cost\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        # 然后反向传播\n",
    "        grads_values = full_backward_propagation(\n",
    "            Y_hat, Y, cashe, param_values, nn_architecture)\n",
    "        # 得到梯度之后，更新参数\n",
    "        param_values = update(param_values, grads_values,\n",
    "                              nn_architecture, learning_rate)\n",
    "        # 显示训练过程\n",
    "        if (i % 50 == 0):\n",
    "            if (verbose):\n",
    "                print(\n",
    "                    \"Iteration:{:05}-cost:{:.5f}-accuracy:{:.5f}\".format(i, cost, accuracy))\n",
    "            if (callback is not None):\n",
    "                callback(i, param_values)\n",
    "    return param_values\n",
    "\n",
    "\n",
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "X, y = make_moons(n_samples=N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "\n",
    "def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n",
    "    \"\"\"可视化数据集的函数\"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    plt.title(plot_name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    if(XX is not None and YY is not None and preds is not None):\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape),\n",
    "                     25, alpha=1, cmap=cm.Spectral)\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape),\n",
    "                     levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40,\n",
    "                cmap=plt.cm.Spectral, edgecolors='black')\n",
    "    plt.show()\n",
    "    if(file_name):\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "make_plot(X, y, \"Dataset\")\n",
    "# Training\n",
    "params_values = train(np.transpose(X_train), np.transpose(\n",
    "    y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 10000, 0.01)\n",
    "# Prediction\n",
    "Y_test_hat, _ = full_forward_propagation(\n",
    "    np.transpose(X_test), params_values, NN_ARCHITECTURE)\n",
    "# Accuracy achieved on the test set\n",
    "acc_test = get_accuracy_value(Y_test_hat, np.transpose(\n",
    "    y_test.reshape((y_test.shape[0], 1))))\n",
    "print(\"Test set accuracy: {:.2f} - David\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练可视化\n",
    "\n",
    "从这里开始补充一些除了基本概念之外也常需要了解的内容。首先看看一次简单训练过程中，如何进行loss的可视化等\n",
    "\n",
    "先看看下面程序的基本流程，首先是数据加载，然后是分割成训练集和测试集，然后构造模型，接下来就是训练和展示结果。\n",
    "\n",
    "这里首先说下fit函数的返回值会返回些什么内容。history是一个tensorflow中的keras的对象。其中有：\n",
    "\n",
    "- epoch: 全部epoch的编号，从0开始计数\n",
    "- history: loss和val_loss，都是各个epoch的值，分别对应训练集和验证集\n",
    "- model: 模型整理结构，也包括权重数据\n",
    "- params: 包括batch_size和epochs等各类模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0)  # list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证\n",
    "\n",
    "交叉验证是模型调优过程中，很重要的一个环节。这里参考了：[模型调优：交叉验证，超参数搜索(复习17)](https://blog.csdn.net/cymy001/article/details/79120355)以及[交叉验证](https://www.jiqizhixin.com/graph/technologies/de51a6aa-a6c6-43b1-880f-79c0b033bbae)来记录一点基本概念。\n",
    "\n",
    "用模型在测试集上进行性能评估前，通常是希望**尽可能利用手头现有的数据对模型进行调优**，甚至可以**粗略地估计测试结果**。\n",
    "\n",
    "交叉验证，有时亦称循环估计， 是一种统计学上将数据样本切割成较小子集的实用方法。于是可以**先在一个子集上做分析， 而其它子集则用来做后续对此分析的确认及验证**。 一开始的子集被称为训练集。而其它的子集则被称为验证集或测试集。交叉验证的目标是定义一个数据集到“测试”的模型在训练阶段，**以便减少像过拟合的问题**，得到该模型将如何衍生到一个独立的数据集的提示。\n",
    "\n",
    "交叉验证可以**保证所有数据都有被训练和验证的机会**，也尽最大可能让优化的模型性能表现的更加可信。\n",
    "\n",
    "K次交叉验证（k-fold cross-validation）是一种常用的方式，将**训练集分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练**。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常用的。\n",
    "\n",
    "交叉验证可用于**比较不同预测建模程序的性能**。注意它其实是用来判断模型性能以及选取参数的。因此确定好参数之后，就确定了模型，再重新用这个模型去训练预测就行了。\n",
    "\n",
    "可以使用scikit-learn调用Keras生成的模型。\n",
    "\n",
    "使用scikit-learn封装Keras的模型，使用scikit-learn对Keras的模型进行交叉验证，使用scikit-learn，利用网格搜索调整Keras模型的超参.\n",
    "\n",
    "首先，Keras在深度学习很受欢迎，但是只能做深度学习：Keras是最小化的深度学习库，目标在于快速搭建深度学习模型。现在已经可以在tensorflow2中直接使用keras接口了。\n",
    "\n",
    "另一方面，基于SciPy的scikit-learn，数值运算效率很高，适用于普遍的机器学习任务，提供很多机器学习工具，包括但不限于：\n",
    "\n",
    "- 使用K折验证模型\n",
    "- 快速搜索并测试超参\n",
    "\n",
    "Keras为scikit-learn封装了KerasClassifier和KerasRegressor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10)\n",
    "# evaluate using 10-fold cross validation\n",
    "# TODO： 这个StratifiedKFold已经废弃了，所以参考 from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(y=Y, n_folds=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来用pima的实例数据展示交叉验证。大部分过程前面以及提到过了，这里说明下model.evaluate 函数的作用。evaluate函数返回the loss value & metrics values for the model in test mode，在没有特别指定的情况下，返回的是test模式下的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 模型保存\n",
    "\n",
    "本节主要参考了：[用序列化保存模型](https://cnbeining.github.io/deep-learning-with-python-cn/4-advanced-multi-layer-perceptrons-and-keras/ch13-save-your-models-for-later-with-serialization.html)。使用保存点保存最好的模型，每轮效果变好就保存一下，还是很好的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint the weights when validation accuracy improves\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只保存最好的模型，每次如果效果变好就覆盖之前的权重文件，把保存的文件名改成固定的即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint the weights for best model on validation accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath = \"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,\n",
    "                             mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10,\n",
    "          callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型计算有时时间很长，所以需要保存相关计算结果，序列化是很好的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset serialize to JSON and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Fit the model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10, verbose=0)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存的模型自然也是可以重新加载的，保存的时候有model和weights两个部分，因此读取的时候也是两部分，并且在使用的时候还要注意加载的模型还必须compile之后，才能使用。这里以保存为yaml文件为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset serialize to YAML and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_yaml\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Fit the model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10, verbose=0)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# later...\n",
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "# load weights into new model loaded_model.load_weights(\"model.h5\") print(\"Loaded model from disk\")\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入保存的模型，保存点只保存权重，网络结构需要预先保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load and use weights from a checkpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
