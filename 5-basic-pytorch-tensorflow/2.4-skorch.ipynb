{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skorch\n",
    "\n",
    "简单记录如何使用一个兼容sklearn的包装了pytorch的神经网络库。安装可以使用conda：\n",
    "\n",
    "```Shell\n",
    "conda install skorch\n",
    "```\n",
    "\n",
    "主要参考官方文档：[skorch documentation](https://skorch.readthedocs.io/en/latest/?badge=latest)\n",
    "\n",
    "skorch 库的目的本身就是尽可能将pytorch和sklearn来拟合起来使用。它通过提供一个有sklearn接口的pytorch 包装来实现。\n",
    "\n",
    "skorch没有重复造轮子，如果对sklearn和pytorch都比较熟悉，那么语法都会是熟悉的语法。\n",
    "\n",
    "此外，skorch简化了训练循环，使许多样板代码过时了。一个简单的net.fit(x,y)就足够了。skorch开箱即用，可处理多种类型的数据，例如PyTorch张量，NumPy数组，Python dict等。如果还有其他数据，也很容易扩展skorch。\n",
    "\n",
    "下面先看一个例子。\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "一个简单的神经网络分类例子 NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9658346 , -2.1890705 ,  0.16985609, ..., -0.89645284,\n",
       "         0.3759244 , -1.0849651 ],\n",
       "       [-0.454767  ,  4.339768  , -0.48572844, ...,  2.9030426 ,\n",
       "        -0.9739298 ,  2.1753323 ],\n",
       "       [ 0.04121372, -2.457531  , -0.27141634, ...,  3.4025245 ,\n",
       "         5.5681396 ,  0.366057  ],\n",
       "       ...,\n",
       "       [ 1.5076263 , -2.0058584 , -0.21547978, ...,  3.68864   ,\n",
       "        -0.65711164,  1.3987011 ],\n",
       "       [-1.5917367 , -2.0708432 , -1.9618258 , ...,  1.0849729 ,\n",
       "         1.1306771 ,  2.101646  ],\n",
       "       [-1.4027424 ,  4.459072  ,  0.55552185, ...,  2.5708554 ,\n",
       "         0.5739863 ,  2.4207122 ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面可以看到将pytorch构造的神经网络给到NeuralNetClassifier对象，可以简单地调用sklearn地fit函数来实现训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7040\u001b[0m       \u001b[32m0.5050\u001b[0m        \u001b[35m0.6954\u001b[0m  0.1017\n",
      "      2        \u001b[36m0.6908\u001b[0m       \u001b[32m0.5100\u001b[0m        \u001b[35m0.6887\u001b[0m  0.0349\n",
      "      3        \u001b[36m0.6862\u001b[0m       \u001b[32m0.5800\u001b[0m        \u001b[35m0.6813\u001b[0m  0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.6772\u001b[0m       \u001b[32m0.5950\u001b[0m        \u001b[35m0.6750\u001b[0m  0.0454\n",
      "      5        \u001b[36m0.6673\u001b[0m       0.5900        \u001b[35m0.6659\u001b[0m  0.0439\n",
      "      6        \u001b[36m0.6594\u001b[0m       \u001b[32m0.6150\u001b[0m        \u001b[35m0.6572\u001b[0m  0.0369\n",
      "      7        \u001b[36m0.6510\u001b[0m       \u001b[32m0.6400\u001b[0m        \u001b[35m0.6480\u001b[0m  0.0389\n",
      "      8        \u001b[36m0.6378\u001b[0m       \u001b[32m0.6550\u001b[0m        \u001b[35m0.6410\u001b[0m  0.0339\n",
      "      9        \u001b[36m0.6330\u001b[0m       \u001b[32m0.6700\u001b[0m        \u001b[35m0.6324\u001b[0m  0.0389\n",
      "     10        \u001b[36m0.6216\u001b[0m       \u001b[32m0.6800\u001b[0m        \u001b[35m0.6212\u001b[0m  0.0464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5332069 , 0.46679318],\n",
       "       [0.60130286, 0.3986971 ],\n",
       "       [0.6090455 , 0.3909545 ],\n",
       "       ...,\n",
       "       [0.5826988 , 0.4173012 ],\n",
       "       [0.47568676, 0.5243132 ],\n",
       "       [0.5778703 , 0.42212975]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=F.relu):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = F.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "net.fit(X, y)\n",
    "y_proba = net.predict_proba(X)\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然 NeuralNetClassifier 提供了一个sklearn兼容地接口，那么也可以用pipeline，更多关于sklearn pipeline地内容可以参考 2-sklearn-example文件夹下内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6944\u001b[0m       \u001b[32m0.5500\u001b[0m        \u001b[35m0.6893\u001b[0m  0.0290\n",
      "      2        \u001b[36m0.6897\u001b[0m       \u001b[32m0.5700\u001b[0m        \u001b[35m0.6870\u001b[0m  0.0439\n",
      "      3        \u001b[36m0.6861\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6848\u001b[0m  0.0349\n",
      "      4        \u001b[36m0.6803\u001b[0m       0.6150        \u001b[35m0.6825\u001b[0m  0.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6795\u001b[0m       \u001b[32m0.6450\u001b[0m        \u001b[35m0.6800\u001b[0m  0.0329\n",
      "      6        \u001b[36m0.6770\u001b[0m       \u001b[32m0.6650\u001b[0m        \u001b[35m0.6774\u001b[0m  0.0419\n",
      "      7        \u001b[36m0.6665\u001b[0m       0.6550        \u001b[35m0.6727\u001b[0m  0.0445\n",
      "      8        0.6671       0.6500        \u001b[35m0.6683\u001b[0m  0.0374\n",
      "      9        \u001b[36m0.6630\u001b[0m       \u001b[32m0.6900\u001b[0m        \u001b[35m0.6636\u001b[0m  0.0417\n",
      "     10        0.6647       0.6750        \u001b[35m0.6591\u001b[0m  0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4878282 , 0.5121718 ],\n",
       "       [0.53851384, 0.46148616],\n",
       "       [0.56193954, 0.43806037],\n",
       "       ...,\n",
       "       [0.5192269 , 0.48077303],\n",
       "       [0.5744814 , 0.42551857],\n",
       "       [0.52439237, 0.47560763]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('net', net),\n",
    "])\n",
    "\n",
    "pipe.fit(X, y)\n",
    "y_proba = pipe.predict_proba(X)\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在skorch中可以很容易的执行GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7178\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7136\u001b[0m  0.0239\n",
      "      2        \u001b[36m0.7167\u001b[0m       0.5000        \u001b[35m0.7085\u001b[0m  0.0239\n",
      "      3        \u001b[36m0.7109\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.7049\u001b[0m  0.0279\n",
      "      4        \u001b[36m0.7082\u001b[0m       0.5075        \u001b[35m0.7016\u001b[0m  0.0242\n",
      "      5        \u001b[36m0.7032\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6974\u001b[0m  0.0249\n",
      "      6        \u001b[36m0.6941\u001b[0m       0.5149        \u001b[35m0.6937\u001b[0m  0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        0.6990       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6904\u001b[0m  0.0279\n",
      "      8        \u001b[36m0.6888\u001b[0m       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6876\u001b[0m  0.0299\n",
      "      9        \u001b[36m0.6826\u001b[0m       0.5299        \u001b[35m0.6849\u001b[0m  0.0309\n",
      "     10        0.6854       0.5299        \u001b[35m0.6824\u001b[0m  0.0349\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7153\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7193\u001b[0m  0.0399\n",
      "      2        \u001b[36m0.7134\u001b[0m       0.5000        \u001b[35m0.7162\u001b[0m  0.0294\n",
      "      3        0.7141       0.5000        \u001b[35m0.7132\u001b[0m  0.0309\n",
      "      4        \u001b[36m0.7069\u001b[0m       0.5000        \u001b[35m0.7102\u001b[0m  0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.7071       0.5000        \u001b[35m0.7079\u001b[0m  0.0329\n",
      "      6        \u001b[36m0.7011\u001b[0m       0.5000        \u001b[35m0.7056\u001b[0m  0.0324\n",
      "      7        0.7080       0.5000        \u001b[35m0.7031\u001b[0m  0.0279\n",
      "      8        \u001b[36m0.6964\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.7010\u001b[0m  0.0309\n",
      "      9        \u001b[36m0.6959\u001b[0m       0.5075        \u001b[35m0.6996\u001b[0m  0.0284\n",
      "     10        0.6982       0.5075        \u001b[35m0.6977\u001b[0m  0.0319\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7070\u001b[0m       \u001b[32m0.4701\u001b[0m        \u001b[35m0.7107\u001b[0m  0.0219\n",
      "      2        \u001b[36m0.6907\u001b[0m       0.4701        \u001b[35m0.7097\u001b[0m  0.0269\n",
      "      3        0.7120       \u001b[32m0.4851\u001b[0m        \u001b[35m0.7081\u001b[0m  0.0314\n",
      "      4        0.7020       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7064\u001b[0m  0.0319\n",
      "      5        0.7084       \u001b[32m0.5075\u001b[0m        \u001b[35m0.7051\u001b[0m  0.0329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        0.6978       0.5075        \u001b[35m0.7038\u001b[0m  0.0319\n",
      "      7        0.7034       0.5075        \u001b[35m0.7029\u001b[0m  0.0279\n",
      "      8        0.6937       0.5075        \u001b[35m0.7021\u001b[0m  0.0389\n",
      "      9        0.7053       0.5000        \u001b[35m0.7011\u001b[0m  0.0239\n",
      "     10        \u001b[36m0.6869\u001b[0m       0.5000        \u001b[35m0.7004\u001b[0m  0.0269\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7008\u001b[0m       \u001b[32m0.5821\u001b[0m        \u001b[35m0.6797\u001b[0m  0.0309\n",
      "      2        0.7104       \u001b[32m0.5970\u001b[0m        \u001b[35m0.6763\u001b[0m  0.0289\n",
      "      3        \u001b[36m0.6926\u001b[0m       \u001b[32m0.6045\u001b[0m        \u001b[35m0.6734\u001b[0m  0.0339\n",
      "      4        0.6985       \u001b[32m0.6194\u001b[0m        \u001b[35m0.6706\u001b[0m  0.0359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6774\u001b[0m       \u001b[32m0.6343\u001b[0m        \u001b[35m0.6687\u001b[0m  0.0272\n",
      "      6        0.6989       0.6269        \u001b[35m0.6661\u001b[0m  0.0199\n",
      "      7        0.6881       0.6343        \u001b[35m0.6637\u001b[0m  0.0269\n",
      "      8        0.6807       0.6194        \u001b[35m0.6616\u001b[0m  0.0219\n",
      "      9        0.6774       0.6269        \u001b[35m0.6597\u001b[0m  0.0269\n",
      "     10        \u001b[36m0.6730\u001b[0m       0.6269        \u001b[35m0.6582\u001b[0m  0.0255\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7284\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.7078\u001b[0m  0.0283\n",
      "      2        \u001b[36m0.7148\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.7059\u001b[0m  0.0279\n",
      "      3        0.7193       \u001b[32m0.5224\u001b[0m        \u001b[35m0.7039\u001b[0m  0.0329\n",
      "      4        \u001b[36m0.7084\u001b[0m       0.5224        \u001b[35m0.7026\u001b[0m  0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.7145       0.5224        \u001b[35m0.7007\u001b[0m  0.0373\n",
      "      6        0.7189       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6989\u001b[0m  0.0313\n",
      "      7        0.7128       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6973\u001b[0m  0.0305\n",
      "      8        0.7090       \u001b[32m0.5522\u001b[0m        \u001b[35m0.6959\u001b[0m  0.0180\n",
      "      9        \u001b[36m0.7064\u001b[0m       0.5373        \u001b[35m0.6946\u001b[0m  0.0239\n",
      "     10        0.7096       0.5373        \u001b[35m0.6931\u001b[0m  0.0293\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6930\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m0.7068\u001b[0m  0.0339\n",
      "      2        0.6976       0.4552        \u001b[35m0.7051\u001b[0m  0.0299\n",
      "      3        \u001b[36m0.6914\u001b[0m       \u001b[32m0.4776\u001b[0m        \u001b[35m0.7039\u001b[0m  0.0299\n",
      "      4        \u001b[36m0.6823\u001b[0m       \u001b[32m0.4851\u001b[0m        \u001b[35m0.7024\u001b[0m  0.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6727\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.7010\u001b[0m  0.0319\n",
      "      6        0.6831       0.5075        \u001b[35m0.6994\u001b[0m  0.0189\n",
      "      7        0.6914       0.5149        \u001b[35m0.6978\u001b[0m  0.0199\n",
      "      8        \u001b[36m0.6718\u001b[0m       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6964\u001b[0m  0.0239\n",
      "      9        0.6839       \u001b[32m0.5522\u001b[0m        \u001b[35m0.6949\u001b[0m  0.0219\n",
      "     10        \u001b[36m0.6676\u001b[0m       \u001b[32m0.5597\u001b[0m        \u001b[35m0.6934\u001b[0m  0.0304\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7096\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7082\u001b[0m  0.0219\n",
      "      2        0.7116       0.5000        \u001b[35m0.7061\u001b[0m  0.0264\n",
      "      3        \u001b[36m0.7075\u001b[0m       0.5000        \u001b[35m0.7044\u001b[0m  0.0329\n",
      "      4        0.7084       0.5000        \u001b[35m0.7025\u001b[0m  0.0289\n",
      "      5        \u001b[36m0.7051\u001b[0m       0.5000        \u001b[35m0.7011\u001b[0m  0.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.7005\u001b[0m       0.5000        \u001b[35m0.6995\u001b[0m  0.0299\n",
      "      7        \u001b[36m0.7004\u001b[0m       0.5000        \u001b[35m0.6978\u001b[0m  0.0209\n",
      "      8        \u001b[36m0.6992\u001b[0m       0.5000        \u001b[35m0.6962\u001b[0m  0.0259\n",
      "      9        \u001b[36m0.6946\u001b[0m       0.5000        \u001b[35m0.6947\u001b[0m  0.0275\n",
      "     10        0.6955       0.5000        \u001b[35m0.6936\u001b[0m  0.0319\n",
      "     11        \u001b[36m0.6898\u001b[0m       0.5000        \u001b[35m0.6923\u001b[0m  0.0289\n",
      "     12        0.6951       0.5000        \u001b[35m0.6909\u001b[0m  0.0330\n",
      "     13        0.6929       0.5000        \u001b[35m0.6899\u001b[0m  0.0289\n",
      "     14        0.6916       0.5000        \u001b[35m0.6890\u001b[0m  0.0279\n",
      "     15        0.6907       0.5000        \u001b[35m0.6880\u001b[0m  0.0239\n",
      "     16        0.6916       0.5000        \u001b[35m0.6871\u001b[0m  0.0284\n",
      "     17        \u001b[36m0.6874\u001b[0m       0.5000        \u001b[35m0.6861\u001b[0m  0.0289\n",
      "     18        \u001b[36m0.6868\u001b[0m       0.5000        \u001b[35m0.6850\u001b[0m  0.0279\n",
      "     19        \u001b[36m0.6857\u001b[0m       0.4925        \u001b[35m0.6842\u001b[0m  0.0289\n",
      "     20        \u001b[36m0.6847\u001b[0m       0.5000        \u001b[35m0.6835\u001b[0m  0.0319\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7349\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7252\u001b[0m  0.0239\n",
      "      2        0.7378       0.5000        \u001b[35m0.7204\u001b[0m  0.0269\n",
      "      3        \u001b[36m0.7237\u001b[0m       0.5000        \u001b[35m0.7177\u001b[0m  0.0319\n",
      "      4        0.7266       0.5000        \u001b[35m0.7149\u001b[0m  0.0284\n",
      "      5        \u001b[36m0.7218\u001b[0m       0.5000        \u001b[35m0.7129\u001b[0m  0.0259\n",
      "      6        \u001b[36m0.7169\u001b[0m       0.5000        \u001b[35m0.7105\u001b[0m  0.0219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m0.7162\u001b[0m       0.5000        \u001b[35m0.7084\u001b[0m  0.0274\n",
      "      8        \u001b[36m0.7125\u001b[0m       0.5000        \u001b[35m0.7066\u001b[0m  0.0325\n",
      "      9        \u001b[36m0.7093\u001b[0m       0.5000        \u001b[35m0.7053\u001b[0m  0.0279\n",
      "     10        0.7130       0.5000        \u001b[35m0.7041\u001b[0m  0.0239\n",
      "     11        \u001b[36m0.7063\u001b[0m       0.5000        \u001b[35m0.7029\u001b[0m  0.0239\n",
      "     12        \u001b[36m0.7044\u001b[0m       0.5000        \u001b[35m0.7014\u001b[0m  0.0225\n",
      "     13        0.7052       0.5000        \u001b[35m0.7006\u001b[0m  0.0319\n",
      "     14        \u001b[36m0.7012\u001b[0m       0.5000        \u001b[35m0.6997\u001b[0m  0.0170\n",
      "     15        \u001b[36m0.6988\u001b[0m       0.5000        \u001b[35m0.6987\u001b[0m  0.0299\n",
      "     16        \u001b[36m0.6985\u001b[0m       0.5000        \u001b[35m0.6980\u001b[0m  0.0229\n",
      "     17        0.6988       0.5000        \u001b[35m0.6974\u001b[0m  0.0304\n",
      "     18        0.6990       0.5000        \u001b[35m0.6966\u001b[0m  0.0299\n",
      "     19        \u001b[36m0.6953\u001b[0m       0.5000        \u001b[35m0.6961\u001b[0m  0.0289\n",
      "     20        0.7023       0.5000        \u001b[35m0.6955\u001b[0m  0.0219\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6994\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7040\u001b[0m  0.0239\n",
      "      2        0.6996       0.5000        \u001b[35m0.7023\u001b[0m  0.0359\n",
      "      3        \u001b[36m0.6963\u001b[0m       0.5000        \u001b[35m0.7009\u001b[0m  0.0359\n",
      "      4        \u001b[36m0.6909\u001b[0m       0.5000        \u001b[35m0.6995\u001b[0m  0.0229\n",
      "      5        0.6922       0.5000        \u001b[35m0.6989\u001b[0m  0.0319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        0.6937       \u001b[32m0.5075\u001b[0m        \u001b[35m0.6981\u001b[0m  0.0298\n",
      "      7        0.6923       0.5075        \u001b[35m0.6980\u001b[0m  0.0279\n",
      "      8        \u001b[36m0.6883\u001b[0m       0.5075        \u001b[35m0.6973\u001b[0m  0.0219\n",
      "      9        \u001b[36m0.6846\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6970\u001b[0m  0.0256\n",
      "     10        0.6889       0.5149        \u001b[35m0.6965\u001b[0m  0.0314\n",
      "     11        0.6912       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6958\u001b[0m  0.0479\n",
      "     12        0.6872       0.5224        \u001b[35m0.6955\u001b[0m  0.0299\n",
      "     13        \u001b[36m0.6820\u001b[0m       0.5149        \u001b[35m0.6953\u001b[0m  0.0479\n",
      "     14        \u001b[36m0.6800\u001b[0m       0.5149        \u001b[35m0.6950\u001b[0m  0.0699\n",
      "     15        0.6863       0.5149        \u001b[35m0.6947\u001b[0m  0.0678\n",
      "     16        0.6823       0.5224        \u001b[35m0.6941\u001b[0m  0.0399\n",
      "     17        0.6883       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6935\u001b[0m  0.0319\n",
      "     18        0.6822       0.5299        0.6936  0.0349\n",
      "     19        \u001b[36m0.6785\u001b[0m       0.5224        \u001b[35m0.6931\u001b[0m  0.0320\n",
      "     20        0.6865       0.5224        \u001b[35m0.6929\u001b[0m  0.0385\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7056\u001b[0m       \u001b[32m0.4925\u001b[0m        \u001b[35m0.6935\u001b[0m  0.0309\n",
      "      2        \u001b[36m0.7024\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6906\u001b[0m  0.0309\n",
      "      3        \u001b[36m0.6880\u001b[0m       0.5000        \u001b[35m0.6883\u001b[0m  0.0359\n",
      "      4        0.7026       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6859\u001b[0m  0.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.6935       0.5149        \u001b[35m0.6832\u001b[0m  0.0339\n",
      "      6        \u001b[36m0.6829\u001b[0m       0.5149        \u001b[35m0.6809\u001b[0m  0.0459\n",
      "      7        0.6906       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6792\u001b[0m  0.0259\n",
      "      8        \u001b[36m0.6773\u001b[0m       \u001b[32m0.5448\u001b[0m        \u001b[35m0.6775\u001b[0m  0.0309\n",
      "      9        0.6853       \u001b[32m0.5746\u001b[0m        \u001b[35m0.6755\u001b[0m  0.0339\n",
      "     10        0.6790       \u001b[32m0.5821\u001b[0m        \u001b[35m0.6737\u001b[0m  0.0259\n",
      "     11        0.6845       \u001b[32m0.5970\u001b[0m        \u001b[35m0.6717\u001b[0m  0.0319\n",
      "     12        \u001b[36m0.6714\u001b[0m       \u001b[32m0.6045\u001b[0m        \u001b[35m0.6699\u001b[0m  0.0269\n",
      "     13        0.6810       \u001b[32m0.6194\u001b[0m        \u001b[35m0.6681\u001b[0m  0.0329\n",
      "     14        \u001b[36m0.6670\u001b[0m       \u001b[32m0.6269\u001b[0m        \u001b[35m0.6665\u001b[0m  0.0389\n",
      "     15        \u001b[36m0.6653\u001b[0m       \u001b[32m0.6493\u001b[0m        \u001b[35m0.6646\u001b[0m  0.0299\n",
      "     16        \u001b[36m0.6588\u001b[0m       0.6418        \u001b[35m0.6630\u001b[0m  0.0259\n",
      "     17        \u001b[36m0.6576\u001b[0m       0.6418        \u001b[35m0.6612\u001b[0m  0.0279\n",
      "     18        0.6600       0.6493        \u001b[35m0.6596\u001b[0m  0.0289\n",
      "     19        0.6607       0.6418        \u001b[35m0.6580\u001b[0m  0.0299\n",
      "     20        0.6580       0.6343        \u001b[35m0.6565\u001b[0m  0.0239\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6917\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6879\u001b[0m  0.0299\n",
      "      2        0.6936       0.4851        \u001b[35m0.6868\u001b[0m  0.0230\n",
      "      3        \u001b[36m0.6896\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.6855\u001b[0m  0.0279\n",
      "      4        0.6936       \u001b[32m0.5448\u001b[0m        \u001b[35m0.6844\u001b[0m  0.0329\n",
      "      5        0.6912       0.5373        \u001b[35m0.6832\u001b[0m  0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.6926       0.5373        \u001b[35m0.6823\u001b[0m  0.0294\n",
      "      7        \u001b[36m0.6849\u001b[0m       0.5299        \u001b[35m0.6812\u001b[0m  0.0239\n",
      "      8        \u001b[36m0.6771\u001b[0m       0.5448        \u001b[35m0.6801\u001b[0m  0.0339\n",
      "      9        0.6925       \u001b[32m0.5597\u001b[0m        \u001b[35m0.6791\u001b[0m  0.0304\n",
      "     10        0.6839       0.5597        \u001b[35m0.6782\u001b[0m  0.0339\n",
      "     11        0.6792       \u001b[32m0.5746\u001b[0m        \u001b[35m0.6773\u001b[0m  0.0319\n",
      "     12        0.6812       0.5746        \u001b[35m0.6763\u001b[0m  0.0269\n",
      "     13        0.6820       0.5746        \u001b[35m0.6753\u001b[0m  0.0239\n",
      "     14        \u001b[36m0.6707\u001b[0m       \u001b[32m0.5896\u001b[0m        \u001b[35m0.6745\u001b[0m  0.0269\n",
      "     15        0.6748       \u001b[32m0.6045\u001b[0m        \u001b[35m0.6739\u001b[0m  0.0220\n",
      "     16        \u001b[36m0.6698\u001b[0m       \u001b[32m0.6194\u001b[0m        \u001b[35m0.6731\u001b[0m  0.0189\n",
      "     17        \u001b[36m0.6689\u001b[0m       \u001b[32m0.6418\u001b[0m        \u001b[35m0.6722\u001b[0m  0.0229\n",
      "     18        \u001b[36m0.6680\u001b[0m       0.6418        \u001b[35m0.6713\u001b[0m  0.0299\n",
      "     19        0.6741       0.6418        \u001b[35m0.6702\u001b[0m  0.0249\n",
      "     20        \u001b[36m0.6670\u001b[0m       0.6418        \u001b[35m0.6692\u001b[0m  0.0239\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7151\u001b[0m       \u001b[32m0.4552\u001b[0m        \u001b[35m0.7146\u001b[0m  0.0259\n",
      "      2        0.7164       0.4403        \u001b[35m0.7127\u001b[0m  0.0334\n",
      "      3        \u001b[36m0.7098\u001b[0m       0.4478        \u001b[35m0.7098\u001b[0m  0.0259\n",
      "      4        0.7149       0.4478        \u001b[35m0.7077\u001b[0m  0.0249\n",
      "      5        \u001b[36m0.7013\u001b[0m       0.4403        \u001b[35m0.7059\u001b[0m  0.0269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        \u001b[36m0.6956\u001b[0m       0.4478        \u001b[35m0.7039\u001b[0m  0.0229\n",
      "      7        0.7093       0.4552        \u001b[35m0.7024\u001b[0m  0.0249\n",
      "      8        \u001b[36m0.6933\u001b[0m       0.4403        \u001b[35m0.7009\u001b[0m  0.0284\n",
      "      9        \u001b[36m0.6930\u001b[0m       0.4552        \u001b[35m0.6997\u001b[0m  0.0219\n",
      "     10        \u001b[36m0.6894\u001b[0m       \u001b[32m0.4851\u001b[0m        \u001b[35m0.6974\u001b[0m  0.0259\n",
      "     11        \u001b[36m0.6894\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.6962\u001b[0m  0.0229\n",
      "     12        0.7003       0.5075        \u001b[35m0.6947\u001b[0m  0.0279\n",
      "     13        0.6937       0.5075        \u001b[35m0.6935\u001b[0m  0.0289\n",
      "     14        0.6903       0.5075        \u001b[35m0.6924\u001b[0m  0.0249\n",
      "     15        \u001b[36m0.6873\u001b[0m       0.5000        \u001b[35m0.6915\u001b[0m  0.0229\n",
      "     16        \u001b[36m0.6816\u001b[0m       0.5000        \u001b[35m0.6908\u001b[0m  0.0279\n",
      "     17        \u001b[36m0.6814\u001b[0m       0.5075        \u001b[35m0.6896\u001b[0m  0.0229\n",
      "     18        0.6881       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6886\u001b[0m  0.0209\n",
      "     19        \u001b[36m0.6760\u001b[0m       0.5224        \u001b[35m0.6879\u001b[0m  0.0219\n",
      "     20        0.6825       \u001b[32m0.5448\u001b[0m        \u001b[35m0.6869\u001b[0m  0.0289\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7052\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6973\u001b[0m  0.0244\n",
      "      2        \u001b[36m0.7043\u001b[0m       0.5000        \u001b[35m0.6939\u001b[0m  0.0319\n",
      "      3        0.7083       0.5000        \u001b[35m0.6911\u001b[0m  0.0269\n",
      "      4        \u001b[36m0.6966\u001b[0m       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6887\u001b[0m  0.0309\n",
      "      5        \u001b[36m0.6958\u001b[0m       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6866\u001b[0m  0.0269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        \u001b[36m0.6881\u001b[0m       \u001b[32m0.5522\u001b[0m        \u001b[35m0.6846\u001b[0m  0.0279\n",
      "      7        0.6906       \u001b[32m0.5672\u001b[0m        \u001b[35m0.6833\u001b[0m  0.0289\n",
      "      8        \u001b[36m0.6879\u001b[0m       0.5672        \u001b[35m0.6818\u001b[0m  0.0229\n",
      "      9        \u001b[36m0.6856\u001b[0m       0.5597        \u001b[35m0.6806\u001b[0m  0.0249\n",
      "     10        \u001b[36m0.6818\u001b[0m       \u001b[32m0.5821\u001b[0m        \u001b[35m0.6784\u001b[0m  0.0302\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6976\u001b[0m       \u001b[32m0.4925\u001b[0m        \u001b[35m0.6997\u001b[0m  0.0259\n",
      "      2        0.7081       0.4776        \u001b[35m0.6980\u001b[0m  0.0329\n",
      "      3        \u001b[36m0.6858\u001b[0m       0.4925        \u001b[35m0.6966\u001b[0m  0.0269\n",
      "      4        0.6979       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6948\u001b[0m  0.0359\n",
      "      5        0.6986       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6937\u001b[0m  0.0269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        0.6959       0.5224        \u001b[35m0.6928\u001b[0m  0.0209\n",
      "      7        0.6927       0.5299        \u001b[35m0.6921\u001b[0m  0.0299\n",
      "      8        0.6892       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6911\u001b[0m  0.0285\n",
      "      9        \u001b[36m0.6744\u001b[0m       0.5000        \u001b[35m0.6905\u001b[0m  0.0195\n",
      "     10        0.6889       0.5299        \u001b[35m0.6892\u001b[0m  0.0209\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7121\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.7093\u001b[0m  0.0359\n",
      "      2        0.7133       0.5075        \u001b[35m0.7063\u001b[0m  0.0349\n",
      "      3        \u001b[36m0.7020\u001b[0m       \u001b[32m0.5224\u001b[0m        \u001b[35m0.7032\u001b[0m  0.0299\n",
      "      4        \u001b[36m0.6986\u001b[0m       0.5149        \u001b[35m0.7011\u001b[0m  0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.6993       0.5149        \u001b[35m0.6986\u001b[0m  0.0319\n",
      "      6        0.7007       0.5149        \u001b[35m0.6962\u001b[0m  0.0239\n",
      "      7        \u001b[36m0.6875\u001b[0m       0.5149        \u001b[35m0.6936\u001b[0m  0.0244\n",
      "      8        0.6912       0.5075        \u001b[35m0.6920\u001b[0m  0.0219\n",
      "      9        0.6881       0.5149        \u001b[35m0.6901\u001b[0m  0.0229\n",
      "     10        \u001b[36m0.6859\u001b[0m       0.5224        \u001b[35m0.6882\u001b[0m  0.0229\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6853\u001b[0m       \u001b[32m0.5448\u001b[0m        \u001b[35m0.6798\u001b[0m  0.0172\n",
      "      2        \u001b[36m0.6824\u001b[0m       \u001b[32m0.5522\u001b[0m        \u001b[35m0.6774\u001b[0m  0.0359\n",
      "      3        0.6843       \u001b[32m0.5672\u001b[0m        \u001b[35m0.6747\u001b[0m  0.0215\n",
      "      4        \u001b[36m0.6718\u001b[0m       \u001b[32m0.6194\u001b[0m        \u001b[35m0.6718\u001b[0m  0.0289\n",
      "      5        \u001b[36m0.6647\u001b[0m       0.6119        \u001b[35m0.6699\u001b[0m  0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.6677       0.6119        \u001b[35m0.6669\u001b[0m  0.0319\n",
      "      7        0.6681       0.5970        \u001b[35m0.6644\u001b[0m  0.0335\n",
      "      8        \u001b[36m0.6633\u001b[0m       0.6045        \u001b[35m0.6616\u001b[0m  0.0349\n",
      "      9        \u001b[36m0.6628\u001b[0m       \u001b[32m0.6343\u001b[0m        \u001b[35m0.6586\u001b[0m  0.0309\n",
      "     10        0.6648       \u001b[32m0.6567\u001b[0m        \u001b[35m0.6559\u001b[0m  0.0349\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7141\u001b[0m       \u001b[32m0.4328\u001b[0m        \u001b[35m0.7044\u001b[0m  0.0269\n",
      "      2        \u001b[36m0.7099\u001b[0m       \u001b[32m0.4627\u001b[0m        \u001b[35m0.7003\u001b[0m  0.0249\n",
      "      3        \u001b[36m0.6985\u001b[0m       \u001b[32m0.4701\u001b[0m        \u001b[35m0.6991\u001b[0m  0.0259\n",
      "      4        \u001b[36m0.6953\u001b[0m       \u001b[32m0.4776\u001b[0m        \u001b[35m0.6980\u001b[0m  0.0220\n",
      "      5        \u001b[36m0.6915\u001b[0m       \u001b[32m0.4925\u001b[0m        \u001b[35m0.6961\u001b[0m  0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.6890\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.6948\u001b[0m  0.0288\n",
      "      7        \u001b[36m0.6887\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6938\u001b[0m  0.0339\n",
      "      8        \u001b[36m0.6828\u001b[0m       0.5149        \u001b[35m0.6928\u001b[0m  0.0325\n",
      "      9        \u001b[36m0.6739\u001b[0m       0.5149        \u001b[35m0.6921\u001b[0m  0.0314\n",
      "     10        0.6793       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6915\u001b[0m  0.0359\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7254\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m0.6971\u001b[0m  0.0263\n",
      "      2        \u001b[36m0.7236\u001b[0m       0.5075        \u001b[35m0.6931\u001b[0m  0.0209\n",
      "      3        \u001b[36m0.7137\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6897\u001b[0m  0.0199\n",
      "      4        \u001b[36m0.6908\u001b[0m       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6864\u001b[0m  0.0275\n",
      "      5        0.7043       0.5299        \u001b[35m0.6842\u001b[0m  0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.7049       0.5373        \u001b[35m0.6826\u001b[0m  0.0309\n",
      "      7        0.7004       \u001b[32m0.5672\u001b[0m        \u001b[35m0.6808\u001b[0m  0.0489\n",
      "      8        \u001b[36m0.6851\u001b[0m       \u001b[32m0.5896\u001b[0m        \u001b[35m0.6787\u001b[0m  0.0718\n",
      "      9        0.6853       \u001b[32m0.6045\u001b[0m        \u001b[35m0.6774\u001b[0m  0.0319\n",
      "     10        \u001b[36m0.6844\u001b[0m       \u001b[32m0.6119\u001b[0m        \u001b[35m0.6759\u001b[0m  0.0419\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7334\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m0.7345\u001b[0m  0.0399\n",
      "      2        \u001b[36m0.7331\u001b[0m       0.5000        \u001b[35m0.7267\u001b[0m  0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.7212\u001b[0m       0.5000        \u001b[35m0.7217\u001b[0m  0.0439\n",
      "      4        \u001b[36m0.7170\u001b[0m       0.5000        \u001b[35m0.7166\u001b[0m  0.0519\n",
      "      5        \u001b[36m0.7107\u001b[0m       0.5000        \u001b[35m0.7129\u001b[0m  0.0432\n",
      "      6        0.7147       0.5000        \u001b[35m0.7087\u001b[0m  0.0239\n",
      "      7        0.7136       \u001b[32m0.5075\u001b[0m        \u001b[35m0.7057\u001b[0m  0.0269\n",
      "      8        \u001b[36m0.7012\u001b[0m       0.5000        \u001b[35m0.7034\u001b[0m  0.0249\n",
      "      9        \u001b[36m0.6991\u001b[0m       0.5075        \u001b[35m0.7011\u001b[0m  0.0269\n",
      "     10        0.6992       \u001b[32m0.5149\u001b[0m        \u001b[35m0.6982\u001b[0m  0.0279\n",
      "     11        \u001b[36m0.6972\u001b[0m       0.5149        \u001b[35m0.6962\u001b[0m  0.0259\n",
      "     12        \u001b[36m0.6926\u001b[0m       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6943\u001b[0m  0.0274\n",
      "     13        \u001b[36m0.6895\u001b[0m       0.5149        \u001b[35m0.6929\u001b[0m  0.0289\n",
      "     14        \u001b[36m0.6859\u001b[0m       0.5000        \u001b[35m0.6908\u001b[0m  0.0239\n",
      "     15        \u001b[36m0.6826\u001b[0m       0.5075        \u001b[35m0.6893\u001b[0m  0.0229\n",
      "     16        0.6837       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6876\u001b[0m  0.0279\n",
      "     17        0.6846       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6864\u001b[0m  0.0269\n",
      "     18        0.6858       0.5149        \u001b[35m0.6843\u001b[0m  0.0294\n",
      "     19        0.6850       0.5373        \u001b[35m0.6827\u001b[0m  0.0289\n",
      "     20        0.6854       0.5373        \u001b[35m0.6815\u001b[0m  0.0279\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6991\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.7024\u001b[0m  0.0249\n",
      "      2        \u001b[36m0.6958\u001b[0m       0.5149        \u001b[35m0.7007\u001b[0m  0.0269\n",
      "      3        0.6983       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6986\u001b[0m  0.0249\n",
      "      4        0.6999       0.5224        \u001b[35m0.6968\u001b[0m  0.0239\n",
      "      5        \u001b[36m0.6898\u001b[0m       \u001b[32m0.5373\u001b[0m        \u001b[35m0.6943\u001b[0m  0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.6895\u001b[0m       0.5299        \u001b[35m0.6936\u001b[0m  0.0269\n",
      "      7        0.6898       0.5224        \u001b[35m0.6923\u001b[0m  0.0339\n",
      "      8        \u001b[36m0.6844\u001b[0m       0.5075        \u001b[35m0.6907\u001b[0m  0.0289\n",
      "      9        0.6914       0.5299        \u001b[35m0.6893\u001b[0m  0.0289\n",
      "     10        0.6851       0.5299        \u001b[35m0.6881\u001b[0m  0.0199\n",
      "     11        0.6893       0.5299        \u001b[35m0.6861\u001b[0m  0.0189\n",
      "     12        \u001b[36m0.6813\u001b[0m       0.5299        \u001b[35m0.6851\u001b[0m  0.0244\n",
      "     13        0.6844       0.5299        \u001b[35m0.6836\u001b[0m  0.0208\n",
      "     14        0.6882       0.5373        \u001b[35m0.6819\u001b[0m  0.0209\n",
      "     15        0.6823       0.5299        \u001b[35m0.6802\u001b[0m  0.0245\n",
      "     16        0.6860       0.5373        \u001b[35m0.6792\u001b[0m  0.0160\n",
      "     17        \u001b[36m0.6749\u001b[0m       0.5373        \u001b[35m0.6779\u001b[0m  0.0199\n",
      "     18        0.6786       0.5373        \u001b[35m0.6765\u001b[0m  0.0249\n",
      "     19        \u001b[36m0.6657\u001b[0m       \u001b[32m0.5448\u001b[0m        \u001b[35m0.6743\u001b[0m  0.0225\n",
      "     20        0.6768       0.5448        \u001b[35m0.6733\u001b[0m  0.0219\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7009\u001b[0m       \u001b[32m0.4925\u001b[0m        \u001b[35m0.6999\u001b[0m  0.0209\n",
      "      2        \u001b[36m0.6978\u001b[0m       0.4776        \u001b[35m0.6987\u001b[0m  0.0239\n",
      "      3        \u001b[36m0.6973\u001b[0m       0.4776        \u001b[35m0.6971\u001b[0m  0.0209\n",
      "      4        \u001b[36m0.6967\u001b[0m       0.4776        \u001b[35m0.6952\u001b[0m  0.0229\n",
      "      5        \u001b[36m0.6924\u001b[0m       0.4776        \u001b[35m0.6938\u001b[0m  0.0219\n",
      "      6        0.6947       0.4851        \u001b[35m0.6925\u001b[0m  0.0209\n",
      "      7        0.6934       0.4776        \u001b[35m0.6905\u001b[0m  0.0190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      8        \u001b[36m0.6820\u001b[0m       0.4851        \u001b[35m0.6888\u001b[0m  0.0259\n",
      "      9        0.6834       0.4925        \u001b[35m0.6874\u001b[0m  0.0209\n",
      "     10        \u001b[36m0.6778\u001b[0m       0.4925        \u001b[35m0.6862\u001b[0m  0.0209\n",
      "     11        0.6820       \u001b[32m0.5000\u001b[0m        \u001b[35m0.6849\u001b[0m  0.0189\n",
      "     12        \u001b[36m0.6734\u001b[0m       0.5000        \u001b[35m0.6837\u001b[0m  0.0209\n",
      "     13        0.6860       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6826\u001b[0m  0.0180\n",
      "     14        \u001b[36m0.6720\u001b[0m       0.5224        \u001b[35m0.6810\u001b[0m  0.0219\n",
      "     15        \u001b[36m0.6712\u001b[0m       \u001b[32m0.5597\u001b[0m        \u001b[35m0.6794\u001b[0m  0.0209\n",
      "     16        \u001b[36m0.6698\u001b[0m       \u001b[32m0.5821\u001b[0m        \u001b[35m0.6774\u001b[0m  0.0249\n",
      "     17        \u001b[36m0.6652\u001b[0m       \u001b[32m0.5970\u001b[0m        \u001b[35m0.6760\u001b[0m  0.0249\n",
      "     18        \u001b[36m0.6622\u001b[0m       \u001b[32m0.6343\u001b[0m        \u001b[35m0.6744\u001b[0m  0.0269\n",
      "     19        \u001b[36m0.6588\u001b[0m       \u001b[32m0.6418\u001b[0m        \u001b[35m0.6732\u001b[0m  0.0252\n",
      "     20        \u001b[36m0.6567\u001b[0m       0.6269        \u001b[35m0.6719\u001b[0m  0.0289\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6986\u001b[0m       \u001b[32m0.4179\u001b[0m        \u001b[35m0.7023\u001b[0m  0.0239\n",
      "      2        0.7007       0.4179        \u001b[35m0.7010\u001b[0m  0.0257\n",
      "      3        0.6996       \u001b[32m0.4627\u001b[0m        \u001b[35m0.6991\u001b[0m  0.0239\n",
      "      4        \u001b[36m0.6948\u001b[0m       \u001b[32m0.4701\u001b[0m        \u001b[35m0.6977\u001b[0m  0.0239\n",
      "      5        \u001b[36m0.6938\u001b[0m       \u001b[32m0.4925\u001b[0m        \u001b[35m0.6962\u001b[0m  0.0259\n",
      "      6        0.6939       \u001b[32m0.5224\u001b[0m        \u001b[35m0.6947\u001b[0m  0.0229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        0.6946       0.5149        \u001b[35m0.6934\u001b[0m  0.0279\n",
      "      8        \u001b[36m0.6890\u001b[0m       \u001b[32m0.5522\u001b[0m        \u001b[35m0.6921\u001b[0m  0.0229\n",
      "      9        0.6906       \u001b[32m0.5746\u001b[0m        \u001b[35m0.6907\u001b[0m  0.0269\n",
      "     10        \u001b[36m0.6883\u001b[0m       \u001b[32m0.5896\u001b[0m        \u001b[35m0.6895\u001b[0m  0.0259\n",
      "     11        0.6906       \u001b[32m0.6119\u001b[0m        \u001b[35m0.6880\u001b[0m  0.0219\n",
      "     12        \u001b[36m0.6850\u001b[0m       0.6119        \u001b[35m0.6867\u001b[0m  0.0219\n",
      "     13        0.6860       \u001b[32m0.6194\u001b[0m        \u001b[35m0.6853\u001b[0m  0.0229\n",
      "     14        \u001b[36m0.6819\u001b[0m       \u001b[32m0.6343\u001b[0m        \u001b[35m0.6840\u001b[0m  0.0308\n",
      "     15        0.6879       0.6343        \u001b[35m0.6830\u001b[0m  0.0239\n",
      "     16        0.6866       \u001b[32m0.6493\u001b[0m        \u001b[35m0.6818\u001b[0m  0.0269\n",
      "     17        0.6829       \u001b[32m0.6567\u001b[0m        \u001b[35m0.6803\u001b[0m  0.0257\n",
      "     18        \u001b[36m0.6785\u001b[0m       \u001b[32m0.6716\u001b[0m        \u001b[35m0.6787\u001b[0m  0.0229\n",
      "     19        0.6825       0.6567        \u001b[35m0.6777\u001b[0m  0.0249\n",
      "     20        0.6794       0.6567        \u001b[35m0.6766\u001b[0m  0.0269\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6836\u001b[0m       \u001b[32m0.5896\u001b[0m        \u001b[35m0.6836\u001b[0m  0.0279\n",
      "      2        \u001b[36m0.6807\u001b[0m       0.5896        \u001b[35m0.6813\u001b[0m  0.0256\n",
      "      3        \u001b[36m0.6710\u001b[0m       \u001b[32m0.5970\u001b[0m        \u001b[35m0.6791\u001b[0m  0.0239\n",
      "      4        0.6794       \u001b[32m0.6194\u001b[0m        \u001b[35m0.6772\u001b[0m  0.0219\n",
      "      5        0.6808       0.6194        \u001b[35m0.6756\u001b[0m  0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.6783       \u001b[32m0.6343\u001b[0m        \u001b[35m0.6742\u001b[0m  0.0259\n",
      "      7        0.6723       \u001b[32m0.6418\u001b[0m        \u001b[35m0.6723\u001b[0m  0.0199\n",
      "      8        \u001b[36m0.6569\u001b[0m       \u001b[32m0.6567\u001b[0m        \u001b[35m0.6700\u001b[0m  0.0249\n",
      "      9        0.6638       \u001b[32m0.6642\u001b[0m        \u001b[35m0.6681\u001b[0m  0.0244\n",
      "     10        \u001b[36m0.6542\u001b[0m       0.6642        \u001b[35m0.6662\u001b[0m  0.0209\n",
      "     11        0.6556       0.6642        \u001b[35m0.6643\u001b[0m  0.0219\n",
      "     12        0.6585       \u001b[32m0.6716\u001b[0m        \u001b[35m0.6623\u001b[0m  0.0190\n",
      "     13        \u001b[36m0.6495\u001b[0m       \u001b[32m0.6940\u001b[0m        \u001b[35m0.6598\u001b[0m  0.0229\n",
      "     14        0.6518       \u001b[32m0.7015\u001b[0m        \u001b[35m0.6569\u001b[0m  0.0229\n",
      "     15        \u001b[36m0.6409\u001b[0m       0.6866        \u001b[35m0.6548\u001b[0m  0.0199\n",
      "     16        0.6460       0.6866        \u001b[35m0.6524\u001b[0m  0.0239\n",
      "     17        \u001b[36m0.6389\u001b[0m       0.6866        \u001b[35m0.6508\u001b[0m  0.0229\n",
      "     18        \u001b[36m0.6364\u001b[0m       0.6866        \u001b[35m0.6485\u001b[0m  0.0239\n",
      "     19        0.6378       0.6940        \u001b[35m0.6464\u001b[0m  0.0239\n",
      "     20        0.6378       0.7015        \u001b[35m0.6436\u001b[0m  0.0219\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: num_units.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7417\u001b[0m       \u001b[32m0.5224\u001b[0m        \u001b[35m0.7115\u001b[0m  0.0249\n",
      "      2        \u001b[36m0.7233\u001b[0m       0.5224        \u001b[35m0.7060\u001b[0m  0.0229\n",
      "      3        0.7235       0.5224        \u001b[35m0.7016\u001b[0m  0.0199\n",
      "      4        0.7278       \u001b[32m0.5299\u001b[0m        \u001b[35m0.6975\u001b[0m  0.0189\n",
      "      5        \u001b[36m0.7107\u001b[0m       \u001b[32m0.5448\u001b[0m        \u001b[35m0.6929\u001b[0m  0.0219\n",
      "      6        \u001b[36m0.7020\u001b[0m       \u001b[32m0.5746\u001b[0m        \u001b[35m0.6889\u001b[0m  0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.6986\u001b[0m       0.5746        \u001b[35m0.6859\u001b[0m  0.0309\n",
      "      8        0.6993       \u001b[32m0.5896\u001b[0m        \u001b[35m0.6827\u001b[0m  0.0339\n",
      "      9        \u001b[36m0.6860\u001b[0m       0.5597        \u001b[35m0.6798\u001b[0m  0.0329\n",
      "     10        0.6890       0.5821        \u001b[35m0.6772\u001b[0m  0.0279\n",
      "     11        \u001b[36m0.6838\u001b[0m       0.5821        \u001b[35m0.6751\u001b[0m  0.0339\n",
      "     12        \u001b[36m0.6790\u001b[0m       \u001b[32m0.6269\u001b[0m        \u001b[35m0.6718\u001b[0m  0.0319\n",
      "     13        0.6793       0.6269        \u001b[35m0.6696\u001b[0m  0.0299\n",
      "     14        \u001b[36m0.6781\u001b[0m       \u001b[32m0.6418\u001b[0m        \u001b[35m0.6671\u001b[0m  0.0369\n",
      "     15        \u001b[36m0.6701\u001b[0m       \u001b[32m0.6567\u001b[0m        \u001b[35m0.6643\u001b[0m  0.0349\n",
      "     16        \u001b[36m0.6635\u001b[0m       \u001b[32m0.6716\u001b[0m        \u001b[35m0.6614\u001b[0m  0.0349\n",
      "     17        0.6706       \u001b[32m0.6866\u001b[0m        \u001b[35m0.6588\u001b[0m  0.0314\n",
      "     18        0.6647       \u001b[32m0.6940\u001b[0m        \u001b[35m0.6568\u001b[0m  0.0329\n",
      "     19        \u001b[36m0.6607\u001b[0m       \u001b[32m0.7015\u001b[0m        \u001b[35m0.6547\u001b[0m  0.0319\n",
      "     20        \u001b[36m0.6500\u001b[0m       0.7015        \u001b[35m0.6521\u001b[0m  0.0289\n",
      "0.705088321854789 {'lr': 0.02, 'max_epochs': 20, 'module__num_units': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "params = {\n",
    "    'lr': [0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "    'module__num_units': [10, 20],\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "接下来看看sktorch的基本用法。以一个回归器为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A toy regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 20), (1000, 1), -6.4901485, 6.154505)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X_regr, y_regr = make_regression(1000, 20, n_informative=10, random_state=0)\n",
    "X_regr = X_regr.astype(np.float32)\n",
    "y_regr = y_regr.astype(np.float32) / 100\n",
    "y_regr = y_regr.reshape(-1, 1)\n",
    "X_regr.shape, y_regr.shape, y_regr.min(), y_regr.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个简单的两层神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义和训练神经网络回归器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.6521\u001b[0m        \u001b[32m3.9025\u001b[0m  0.0718\n",
      "      2        \u001b[36m4.2853\u001b[0m        \u001b[32m3.2140\u001b[0m  0.0489\n",
      "      3        \u001b[36m2.4202\u001b[0m        \u001b[32m0.5702\u001b[0m  0.0454\n",
      "      4        \u001b[36m0.4799\u001b[0m        \u001b[32m0.2762\u001b[0m  0.0549\n",
      "      5        \u001b[36m0.2664\u001b[0m        0.4691  0.0449\n",
      "      6        0.3470        \u001b[32m0.1310\u001b[0m  0.0578\n",
      "      7        \u001b[36m0.1079\u001b[0m        \u001b[32m0.1244\u001b[0m  0.0409\n",
      "      8        \u001b[36m0.0805\u001b[0m        \u001b[32m0.0744\u001b[0m  0.0479\n",
      "      9        \u001b[36m0.0744\u001b[0m        0.1681  0.0568\n",
      "     10        0.1528        0.1326  0.0613\n",
      "     11        0.1552        0.1704  0.0564\n",
      "     12        0.1021        \u001b[32m0.0694\u001b[0m  0.0539\n",
      "     13        \u001b[36m0.0634\u001b[0m        \u001b[32m0.0530\u001b[0m  0.0463\n",
      "     14        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0342\u001b[0m  0.0569\n",
      "     15        0.0368        0.0375  0.0510\n",
      "     16        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0326\u001b[0m  0.0549\n",
      "     17        0.0396        0.0398  0.0399\n",
      "     18        0.0281        0.0368  0.0429\n",
      "     19        0.0417        0.0334  0.0549\n",
      "     20        \u001b[36m0.0232\u001b[0m        \u001b[32m0.0294\u001b[0m  0.0449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=RegressorModule(\n",
       "    (dense0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dense1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (output): Linear(in_features=10, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "net_regr = NeuralNetRegressor(\n",
    "    RegressorModule,\n",
    "    max_epochs=20,\n",
    "    lr=0.1,\n",
    "#     device='cuda',  # uncomment this to train with CUDA\n",
    ")\n",
    "net_regr.fit(X_regr, y_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后是执行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24164444],\n",
       "       [-1.5176816 ],\n",
       "       [-0.89105034],\n",
       "       [-0.2990004 ],\n",
       "       [-0.512405  ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net_regr.predict(X_regr[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with CNN\n",
    "\n",
    "接下来看看如何在skorch中使用CNN。首先导入并处理数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "X /= 255.0\n",
    "X.min(), X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52500, 784), (52500,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "assert(X_train.shape[0] + X_test.shape[0] == mnist.data.shape[0])\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y)\n",
    "        \n",
    "plot_example(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来用Pytorch 构造神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 98, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dim = X.shape[1]\n",
    "hidden_dim = int(mnist_dim/8)\n",
    "output_dim = len(np.unique(mnist.target))\n",
    "\n",
    "mnist_dim, hidden_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 1, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XCnn = X.reshape(-1, 1, 28, 28)\n",
    "XCnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52500, 1, 28, 28), (52500,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XCnn_train, XCnn_test, y_train, y_test = train_test_split(XCnn, y, test_size=0.25, random_state=42)\n",
    "XCnn_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout)\n",
    "        self.fc1 = nn.Linear(1600, 100) # 1600 = number channels * width * height\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten over channel, height and width = 1600\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        x = torch.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "cnn = NeuralNetClassifier(\n",
    "    Cnn,\n",
    "    max_epochs=2,\n",
    "    lr=0.002,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "cnn.fit(XCnn_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = cnn.predict(XCnn_test)\n",
    "accuracy_score(y_test, y_pred_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sktorch v.s. PyTorch\n",
    "\n",
    "还是以MNIST为例，一个标准的sktorch过程。更多内容参考源代码：https://github.com/skorch-dev/skorch/blob/master/examples/benchmarks/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNet\n",
    "\n",
    "NeuralNet 及其衍生出的类是用户的主要使用内容。它们包装了PyTorch的Module类，并提供了sklearn接口。定义自己的神经网络的过程就和Pytorch中一样，然后将它和一个pytorch的criterion一起传递给NeuralNet，然后就可以调用fit()函数和predict()函数了。最后形式如下所示：\n",
    "\n",
    "```Python\n",
    "class MyModule(torch.nn.Module):\n",
    "    ...\n",
    "\n",
    "net = NeuralNet(\n",
    "    module=MyModule,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    ")\n",
    "net.fit(X, y)\n",
    "y_pred = net.predict(X_valid)\n",
    "```\n",
    "\n",
    "这其中 sktorch做的事情有：\n",
    "\n",
    "- wraps the PyTorch Module in an sklearn interface\n",
    "- converts numpy.ndarrays to PyTorch Tensors\n",
    "- abstracts away the fit loop\n",
    "- takes care of batching the data\n",
    "\n",
    "除了前面提到的 module 和 criterion ，还有一些重要的参数和方法：\n",
    "\n",
    "比如 optimizer ，可以通过它指定pytorch中的优化算法，另外可以通过类似如下形式：\n",
    "\n",
    "```Python\n",
    "optimizer__param_groups=[\n",
    "    ('embedding.*', {'lr': 0.0}),\n",
    "    ('linear0.bias', {'lr': 1}),\n",
    "]\n",
    "```\n",
    "\n",
    "定义优化函数的参数。\n",
    "\n",
    "还有 max_epochs，batch_size 定义epoch和batch数目。\n",
    "\n",
    "train_split 可以执行训练集和验证集的分离。\n",
    "\n",
    "关于callbacks的更多基本内容可以参考以下环节，总的来说，在 NeuralNet 中，默认的会有几个有用的callbacks，这定义到了  get_default_callbacks() 方法中。除了这些方法，有时候也想提供自己的callback方法，这时候最简单的方法是传递所有自己的callbacks到callbacks参数中，形如：\n",
    "\n",
    "```Python\n",
    "net = NeuralNet(\n",
    "    module=MyModule,\n",
    "    callbacks=[\n",
    "        MyCallback1(...),\n",
    "        MyCallback2(...),\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "更好的办法是给callbacks名称，这样可以避免命名冲突：\n",
    "\n",
    "```Python\n",
    "net = NeuralNet(\n",
    "    module=MyModule,\n",
    "    callbacks=[\n",
    "        ('cb1', MyCallback1(...)),\n",
    "        ('cb2', MyCallback2(...)),\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "这样做还可以更方便的传递参数：\n",
    "\n",
    "```Python\n",
    "net.set_params(callbacks__cb1__foo=123, callbacks__cb2__bar=456)\n",
    "```\n",
    "\n",
    "另外需要注意的是自定义的 callbacks 会在 default callbacks 之后被调用，因此还是会调用默认的callbacks。不过有一个例外，那就是 PrintLog, 这个默认的callback函数总是会在最后调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks 提供了灵活的方法来定制 NeuralNet，而不需要继承它。callbacks 通常是和history属性打交道的。想要记录net的行为，要考虑使用net.history。如果想要写自己的callback，需要继承 Callback。一般要注意：\n",
    "\n",
    "- 应该从基类继承。\n",
    "- 应至少实现父类提供的一种 on_xxx 方法（比如 on_train_begin(net, X, y) ）。\n",
    "- 作为参数，方法首先获取NeuralNet实例，并在适当时获取本地数据（例如，当前批次中的数据）。\n",
    "\n",
    "接下来看例子，这是一个记住在哪一代 validation accuracy 达到某一个值的例子。主要有以下内容：\n",
    "\n",
    "- 在 __init__ 中设置一个 minimum accuracy\n",
    "- 在 initialize 中设置 the critical epoch\n",
    "- 每个epoch 之后，如果还没达到 critical accuracy，就检查是否达到了\n",
    "- 当训练结束， 发送一个twitter通知训练是否成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "\n",
    "\n",
    "def tweet(msg):\n",
    "    print(\"~\" * 60)\n",
    "    print(\"*tweet*\", msg, \"#skorch #pytorch\")\n",
    "    print(\"~\" * 60)\n",
    "\n",
    "\n",
    "class AccuracyTweet(Callback):\n",
    "    def __init__(self, min_accuracy):\n",
    "        self.min_accuracy = min_accuracy\n",
    "\n",
    "    def initialize(self):\n",
    "        self.critical_epoch_ = -1\n",
    "\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        if self.critical_epoch_ > -1:\n",
    "            return\n",
    "        # look at the validation accuracy of the last epoch\n",
    "        if net.history[-1, 'valid_acc'] >= self.min_accuracy:\n",
    "            self.critical_epoch_ = len(net.history)\n",
    "\n",
    "    def on_train_end(self, net, **kwargs):\n",
    "        if self.critical_epoch_ < 0:\n",
    "            msg = \"Accuracy never reached {} :(\".format(self.min_accuracy)\n",
    "        else:\n",
    "            msg = \"Accuracy reached {} at epoch {}!!!\".format(\n",
    "                self.min_accuracy, self.critical_epoch_)\n",
    "\n",
    "        tweet(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在初始化一个分类器 NeuralNetClassifier ，然后传入callback 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0);\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=15,\n",
    "    lr=0.02,\n",
    "    warm_start=True,\n",
    "    callbacks=[AccuracyTweet(min_accuracy=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 20), (1000,), 0.5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X, y = X.astype(np.float32), y.astype(np.int64)\n",
    "X.shape, y.shape, y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6954\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m0.6844\u001b[0m  0.0489\n",
      "      2        \u001b[36m0.6802\u001b[0m       0.5950        \u001b[35m0.6817\u001b[0m  0.0309\n",
      "      3        0.6839       0.6000        \u001b[35m0.6792\u001b[0m  0.0419\n",
      "      4        \u001b[36m0.6753\u001b[0m       0.5900        \u001b[35m0.6767\u001b[0m  0.0309\n",
      "      5        0.6769       0.5950        \u001b[35m0.6742\u001b[0m  0.0314\n",
      "      6        0.6774       \u001b[32m0.6050\u001b[0m        \u001b[35m0.6720\u001b[0m  0.0319\n",
      "      7        \u001b[36m0.6693\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6695\u001b[0m  0.0439\n",
      "      8        0.6694       \u001b[32m0.6300\u001b[0m        \u001b[35m0.6672\u001b[0m  0.0329\n",
      "      9        0.6703       \u001b[32m0.6400\u001b[0m        \u001b[35m0.6652\u001b[0m  0.0419\n",
      "     10        \u001b[36m0.6523\u001b[0m       \u001b[32m0.6550\u001b[0m        \u001b[35m0.6623\u001b[0m  0.0514\n",
      "     11        0.6641       \u001b[32m0.6650\u001b[0m        \u001b[35m0.6603\u001b[0m  0.0439\n",
      "     12        0.6524       0.6650        \u001b[35m0.6582\u001b[0m  0.0349\n",
      "     13        \u001b[36m0.6506\u001b[0m       \u001b[32m0.6700\u001b[0m        \u001b[35m0.6553\u001b[0m  0.0409\n",
      "     14        \u001b[36m0.6489\u001b[0m       0.6650        \u001b[35m0.6527\u001b[0m  0.0459\n",
      "     15        0.6505       \u001b[32m0.6750\u001b[0m        \u001b[35m0.6502\u001b[0m  0.0324\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "*tweet* Accuracy never reached 0.7 :( #skorch #pytorch\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (dense0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (dense1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (output): Linear(in_features=10, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到没有达到0.7，可以再试一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16        \u001b[36m0.6473\u001b[0m       0.6750        \u001b[35m0.6474\u001b[0m  0.0289\n",
      "     17        \u001b[36m0.6431\u001b[0m       \u001b[32m0.6800\u001b[0m        \u001b[35m0.6443\u001b[0m  0.0349\n",
      "     18        0.6461       \u001b[32m0.6900\u001b[0m        \u001b[35m0.6418\u001b[0m  0.0329\n",
      "     19        \u001b[36m0.6430\u001b[0m       0.6850        \u001b[35m0.6392\u001b[0m  0.0409\n",
      "     20        \u001b[36m0.6364\u001b[0m       \u001b[32m0.6950\u001b[0m        \u001b[35m0.6366\u001b[0m  0.0319\n",
      "     21        \u001b[36m0.6266\u001b[0m       \u001b[32m0.7000\u001b[0m        \u001b[35m0.6334\u001b[0m  0.0339\n",
      "     22        0.6316       0.7000        \u001b[35m0.6308\u001b[0m  0.0469\n",
      "     23        \u001b[36m0.6231\u001b[0m       0.7000        \u001b[35m0.6277\u001b[0m  0.0389\n",
      "     24        \u001b[36m0.6094\u001b[0m       0.7000        \u001b[35m0.6242\u001b[0m  0.0309\n",
      "     25        0.6250       \u001b[32m0.7050\u001b[0m        \u001b[35m0.6215\u001b[0m  0.0429\n",
      "     26        0.6180       \u001b[32m0.7150\u001b[0m        \u001b[35m0.6187\u001b[0m  0.0449\n",
      "     27        0.6186       0.7150        \u001b[35m0.6159\u001b[0m  0.0389\n",
      "     28        0.6144       0.7150        \u001b[35m0.6134\u001b[0m  0.0441\n",
      "     29        \u001b[36m0.5993\u001b[0m       0.7150        \u001b[35m0.6100\u001b[0m  0.0444\n",
      "     30        \u001b[36m0.5976\u001b[0m       0.7150        \u001b[35m0.6071\u001b[0m  0.0349\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "*tweet* Accuracy reached 0.7 at epoch 21!!! #skorch #pytorch\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (dense0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (dense1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (output): Linear(in_features=10, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert net.history[-1, 'valid_acc'] >= 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要获取最后一个valid_loss可以使用如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975562930107117"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.history[-1, 'batches', -1, 'valid_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，还可以使用 earlystopping 这个callback，如前所述，直接添加到callbacks参数中即可，这里没展示出效果，不过知道使用方法即可，更多参数信息可以参考：https://skorch.readthedocs.io/en/stable/callbacks.html?highlight=earlystop#skorch.callbacks.EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EarlyStopping\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.02,\n",
    "    warm_start=True,\n",
    "    callbacks=[AccuracyTweet(min_accuracy=0.7),\n",
    "              EarlyStopping(patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7545\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m0.7316\u001b[0m  0.0239\n",
      "      2        \u001b[36m0.7453\u001b[0m       \u001b[32m0.3800\u001b[0m        \u001b[35m0.7231\u001b[0m  0.0329\n",
      "      3        \u001b[36m0.7275\u001b[0m       0.3800        \u001b[35m0.7174\u001b[0m  0.0249\n",
      "      4        \u001b[36m0.7261\u001b[0m       \u001b[32m0.3850\u001b[0m        \u001b[35m0.7128\u001b[0m  0.0409\n",
      "      5        \u001b[36m0.7185\u001b[0m       \u001b[32m0.4000\u001b[0m        \u001b[35m0.7088\u001b[0m  0.0339\n",
      "      6        \u001b[36m0.7090\u001b[0m       0.3900        \u001b[35m0.7064\u001b[0m  0.0584\n",
      "      7        \u001b[36m0.7050\u001b[0m       0.4000        \u001b[35m0.7044\u001b[0m  0.0439\n",
      "      8        \u001b[36m0.7019\u001b[0m       \u001b[32m0.4300\u001b[0m        \u001b[35m0.7025\u001b[0m  0.0409\n",
      "      9        \u001b[36m0.6998\u001b[0m       \u001b[32m0.4450\u001b[0m        \u001b[35m0.7009\u001b[0m  0.0359\n",
      "     10        \u001b[36m0.6948\u001b[0m       \u001b[32m0.4650\u001b[0m        \u001b[35m0.6996\u001b[0m  0.0419\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "*tweet* Accuracy never reached 0.7 :( #skorch #pytorch\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (dense0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (dense1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (output): Linear(in_features=10, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset 提供了有关数据处理的类和函数。\n",
    "\n",
    "CVSplit 类负责执行NeuralNet的内部交叉验证，它严格遵守sklearn标准。\n",
    "\n",
    "CVSplit需要的第一个参数是cv，它的工作原理类似于在sklearn GridSearchCV， cross_val_score()等等中的cv参数。这里补充一个简短说明：\n",
    "\n",
    "- None：使用默认的3-fold交叉验证。\n",
    "- 整数：指定的折叠次数(Stratified)KFold，\n",
    "- float：表示要包含在验证拆分中的数据集的比例（例如0.2，表示20％）。\n",
    "- object：用作交叉验证生成器的对象。\n",
    "- 迭代器：生成训练，验证集。\n",
    "\n",
    "此外，CVSplit 使用一个 stratified 参数来确定是否应进行分层拆分（仅对离散目标有意义），以及一个random_state参数用于交叉验证拆分具有随机成分的情况。\n",
    "\n",
    "和sklearn的交叉验证的一个区别是skorch仅进行一次拆分。在sklearn中，如果5-flod交叉验证，会对模型的不同折叠组合进行5次训练。而对于神经网络来说，通常并不希望这样，因为训练需要很多时间。因此，**skorch只进行一次拆分**。\n",
    "\n",
    "注意，CVSplit 在fit函数执行时会自动调用，如后面训练RandomDataset时所示，是有valid_loss的。\n",
    "\n",
    "如果您希望进行完整的kfold交叉验证，则仍然可以将skorch与sklearn函数结合使用，就像使用其他任何sklearn兼容的估算器一样。只需记住设置 train_split=None，以便将整个数据集用于训练。通常代码形式如下：\n",
    "\n",
    "```Python\n",
    "net = NeuralNetClassifier(\n",
    "    module=MyModule,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(net, X, y, cv=5)\n",
    "```\n",
    "\n",
    "在pytorch中 有 Dataset和DataLoader，skorch 默认使用了pytorch的 DataLoader，当调用fit函数时也支持pytorch的Dataset。那么如何在skorch中使用pytorch的Dataset？比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = torch.randn(128, 10)\n",
    "        self.Y = torch.randn(128, 10)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomDataset 可以直接传入fit函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.5074\u001b[0m        \u001b[32m1.0854\u001b[0m  0.0768\n",
      "      2        \u001b[36m1.5047\u001b[0m        \u001b[32m1.0838\u001b[0m  0.0030\n",
      "      3        \u001b[36m1.5021\u001b[0m        \u001b[32m1.0823\u001b[0m  0.0030\n",
      "      4        \u001b[36m1.4995\u001b[0m        \u001b[32m1.0808\u001b[0m  0.0030\n",
      "      5        \u001b[36m1.4969\u001b[0m        \u001b[32m1.0793\u001b[0m  0.0030\n",
      "      6        \u001b[36m1.4944\u001b[0m        \u001b[32m1.0778\u001b[0m  0.0030\n",
      "      7        \u001b[36m1.4918\u001b[0m        \u001b[32m1.0763\u001b[0m  0.0040\n",
      "      8        \u001b[36m1.4893\u001b[0m        \u001b[32m1.0748\u001b[0m  0.0050\n",
      "      9        \u001b[36m1.4867\u001b[0m        \u001b[32m1.0733\u001b[0m  0.0060\n",
      "     10        \u001b[36m1.4842\u001b[0m        \u001b[32m1.0718\u001b[0m  0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hust2\\.conda\\envs\\hydrus\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNet\n",
    "import torch.nn as nn\n",
    "\n",
    "train_ds = RandomDataset()\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)\n",
    "\n",
    "net = NeuralNet(MyModule, criterion=torch.nn.MSELoss)\n",
    "net.fit(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了支持更多的数据格式，skorch提供了自己的Dataset，它可以兼容：\n",
    "\n",
    "- numpy.ndarrays\n",
    "- PyTorch Tensors\n",
    "- scipy sparse CSR matrices\n",
    "- pandas DataFrames or Series\n",
    "\n",
    "数据可以以dict形式传入fit函数，字典的key会被当做forward函数的参数，下面是一个例子，这样可以很简便地处理不同类型地输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading\n",
    "\n",
    "skorch 提供了很多方法来保存模型。首先，可以使用python的oickle保存模型，这会直接保存整个模型，包括超参数。比如：\n",
    "\n",
    "```Python\n",
    "net = NeuralNet(\n",
    "    module=MyModule,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    ('my-features', get_features()),\n",
    "    ('net', net),\n",
    "])\n",
    "model.fit(X, y)\n",
    "\n",
    "# saving\n",
    "with open('some-file.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# loading\n",
    "with open('some-file.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "```\n",
    "\n",
    "这尤其在pipeline等时候有用。这种方式的缺点时当你修改了一些代码时，很容易报错。\n",
    "\n",
    "因此，skorch还提供了第二种保存模型的方法--调用 NeuralNet 中的 save_params() 和 load_params() 方法。这时候保存module的 state_dict，比如module的 weights 和 bias。形如：\n",
    "\n",
    "```Python\n",
    "net = NeuralNet(\n",
    "    module=MyModule,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    ('my-features', get_features()),\n",
    "    ('net', net),\n",
    "])\n",
    "model.fit(X, y)\n",
    "\n",
    "net.save_params(f_params='some-file.pkl')\n",
    "\n",
    "new_net = NeuralNet(\n",
    "    module=MyModule,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    ")\n",
    "new_net.initialize()  # This is important!\n",
    "new_net.load_params(f_params='some-file.pkl')\n",
    "```\n",
    "\n",
    "除了model的参数，history和optimizer state也可以通过包括f_history 和 f_optimizer  关键字到 save_params()和load_params() 来保存。形如：\n",
    "\n",
    "```Python\n",
    "net = NeuralNet(\n",
    "    module=MyModule\n",
    "    criterion=torch.nn.NLLLoss,\n",
    ")\n",
    "\n",
    "net.fit(X, y, epochs=2) # Train for 2 epochs\n",
    "\n",
    "net.save_params(\n",
    "    f_params='model.pkl', f_optimizer='opt.pkl', f_history='history.json')\n",
    "\n",
    "new_net = NeuralNet(\n",
    "    module=MyModule\n",
    "    criterion=torch.nn.NLLLoss,\n",
    ")\n",
    "new_net.initialize() # This is important!\n",
    "new_net.load_params(\n",
    "    f_params='model.pkl', f_optimizer='opt.pkl', f_history='history.json')\n",
    "\n",
    "new_net.fit(X, y, epochs=2) # Train for another 2 epochs\n",
    "```\n",
    "\n",
    "此外，skorch还提供了 Checkpoint, TrainEndCheckpoint, and LoadInitState 等callbacks来处理模型训练中的保存和加载模型。下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "\n",
    "X, y = make_classification(1000, 10, n_informative=5, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "class MyModule(nn.Sequential):\n",
    "    def __init__(self, num_units=10):\n",
    "        super().__init__(\n",
    "            nn.Linear(10, num_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_units, 10),\n",
    "            nn.Linear(10, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后创建两个不同的checkpoint callbacks并配置它们来保存模型参数，optimizer和history到一个exp1的文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5629\u001b[0m       \u001b[32m0.8750\u001b[0m        \u001b[35m0.3956\u001b[0m     +  0.0409\n",
      "      2        \u001b[36m0.3058\u001b[0m       \u001b[32m0.9350\u001b[0m        \u001b[35m0.2583\u001b[0m     +  0.0439\n",
      "      3        \u001b[36m0.2270\u001b[0m       0.9000        0.2924        0.0339\n",
      "      4        \u001b[36m0.1990\u001b[0m       0.7850        0.4077        0.0379\n",
      "      5        0.2270       \u001b[32m0.9550\u001b[0m        \u001b[35m0.1998\u001b[0m     +  0.0374\n",
      "      6        \u001b[36m0.1914\u001b[0m       0.9300        0.2043        0.0435\n",
      "      7        \u001b[36m0.1911\u001b[0m       0.8300        0.3399        0.0406\n",
      "      8        0.1974       0.9350        0.2017        0.0364\n",
      "      9        \u001b[36m0.1793\u001b[0m       0.9400        \u001b[35m0.1932\u001b[0m     +  0.0460\n",
      "     10        \u001b[36m0.1783\u001b[0m       0.9450        \u001b[35m0.1884\u001b[0m     +  0.0400\n"
     ]
    }
   ],
   "source": [
    "# First run\n",
    "\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "cp = Checkpoint(dirname='exp1')\n",
    "train_end_cp = TrainEndCheckpoint(dirname='exp1')\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule, lr=0.5, callbacks=[cp, train_end_cp]\n",
    ")\n",
    "\n",
    "_ = net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认的Checkpoint 会观察 valid_loss ，并在改善的时候保存模型。上面的+表示的就是这个。\n",
    "\n",
    "可以通过LoadInitState来加载checkpoint点来继续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "     11        \u001b[36m0.1696\u001b[0m       \u001b[32m0.8650\u001b[0m        \u001b[35m0.3954\u001b[0m     +  0.0269\n",
      "     12        0.2051       \u001b[32m0.9250\u001b[0m        \u001b[35m0.2246\u001b[0m     +  0.0289\n",
      "     13        \u001b[36m0.1607\u001b[0m       \u001b[32m0.9300\u001b[0m        \u001b[35m0.1902\u001b[0m     +  0.0359\n",
      "     14        0.1624       \u001b[32m0.9350\u001b[0m        \u001b[35m0.1766\u001b[0m     +  0.0389\n",
      "     15        \u001b[36m0.1500\u001b[0m       0.9350        0.1866        0.0329\n",
      "     16        0.1505       \u001b[32m0.9400\u001b[0m        0.2206        0.0409\n",
      "     17        0.1766       0.9400        \u001b[35m0.1750\u001b[0m     +  0.0309\n",
      "     18        \u001b[36m0.1500\u001b[0m       0.9100        0.2561        0.0339\n",
      "     19        0.1539       \u001b[32m0.9600\u001b[0m        \u001b[35m0.1711\u001b[0m     +  0.0339\n",
      "     20        \u001b[36m0.1493\u001b[0m       0.9500        \u001b[35m0.1683\u001b[0m     +  0.0439\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import LoadInitState\n",
    "\n",
    "cp = Checkpoint(dirname='exp1')\n",
    "load_state = LoadInitState(cp)\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule, lr=0.1, callbacks=[cp, load_state]\n",
    ")\n",
    "\n",
    "_ = net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，训练是从第11代开始的，也就是第10代结束的时候。创建了一个带 fn_prefix 参数的新的Checkpoint callback，fn_prefix参数的值  'from_train_end_' 是将要保存的文件名的前缀，这样可以确保不会覆盖之前的checkpoint保存文件。\n",
    "\n",
    "还可以开始新的实验，改变存储的文件夹即可，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.5812\u001b[0m       \u001b[32m0.8850\u001b[0m        \u001b[35m0.4392\u001b[0m     +  0.1278\n",
      "      2        \u001b[36m0.3166\u001b[0m       \u001b[32m0.9050\u001b[0m        \u001b[35m0.2971\u001b[0m     +  0.0484\n",
      "      3        \u001b[36m0.2023\u001b[0m       \u001b[32m0.9250\u001b[0m        \u001b[35m0.2483\u001b[0m     +  0.0399\n",
      "      4        \u001b[36m0.1783\u001b[0m       \u001b[32m0.9300\u001b[0m        \u001b[35m0.2178\u001b[0m     +  0.0429\n",
      "      5        \u001b[36m0.1546\u001b[0m       \u001b[32m0.9400\u001b[0m        \u001b[35m0.2147\u001b[0m     +  0.0479\n",
      "      6        0.1570       0.9350        0.2343        0.0409\n",
      "      7        \u001b[36m0.1537\u001b[0m       0.9300        \u001b[35m0.2048\u001b[0m     +  0.0364\n",
      "      8        0.1570       \u001b[32m0.9450\u001b[0m        \u001b[35m0.1971\u001b[0m     +  0.0404\n",
      "      9        \u001b[36m0.1409\u001b[0m       \u001b[32m0.9500\u001b[0m        \u001b[35m0.1923\u001b[0m     +  0.0569\n",
      "     10        \u001b[36m0.1389\u001b[0m       0.9400        0.1927        0.0409\n"
     ]
    }
   ],
   "source": [
    "cp = Checkpoint(dirname='exp2')\n",
    "load_state = LoadInitState(cp)\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule, lr=0.5,\n",
    "    callbacks=[cp, load_state],\n",
    "    module__num_units=20,\n",
    ")\n",
    "\n",
    "_ = net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这会将模型存储到'exp2'目录中。由于这是第一次运行，因此LoadInitState回调不执行任何操作。如果我们再次运行以上脚本，则LoadInitState 回调将从检查点加载模型。\n",
    "\n",
    "在上面的运行中，最后一个 checkpoint 是在第6个时刻创建的，我们可以加载该 checkpoint 来进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MyModule, lr=0.5, module__num_units=20,\n",
    ")\n",
    "net.initialize()\n",
    "net.load_params(checkpoint=cp)\n",
    "\n",
    "y_pred = net.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "一个 NeuralNet 对象使用了一个History对象来记录训练过程，存储在history 属性中。在很多用例中，history 用来打印每个epoch的训练过程，形如下：\n",
    "\n",
    "```Python\n",
    "net.fit(X, y)\n",
    "\n",
    "# prints\n",
    "  epoch    train_loss    valid_acc    valid_loss     dur\n",
    "-------  ------------  -----------  ------------  ------\n",
    "      1        0.7111       0.5100        0.6894  0.1345\n",
    "      2        0.6928       0.5500        0.6803  0.0608\n",
    "      3        0.6833       0.5650        0.6741  0.0620\n",
    "      4        0.6763       0.5850        0.6674  0.0594\n",
    "```\n",
    "\n",
    "所有信息可以通过 net.history 来获取。保存训练相关数据的最好方式就是通过利用history。\n",
    "\n",
    "通常History是一系列dicts的list，list中每个item对应每一个epoch，每个dict的key对应每列。因此，比如当想要获取最后一个epoch 的 train_loss 时，应该调用 net.history[-1]['train_loss']。为了使history 信息获取更方便，可以使用这种形式：net.history[-1, 'train_loss'].用逗号来分割。\n",
    "\n",
    "还有，History 在batches key下存储了每个epoch下的每个batch。所以比如想获取第7个epoch的第3个batch可以使用：net.history[7, 'batches', 3, 'train_loss'].\n",
    "\n",
    "以下是一些例子，备用：\n",
    "\n",
    "```Python\n",
    "# history of a fitted neural net\n",
    "history = net.history\n",
    "# get current epoch, a dict\n",
    "history[-1]\n",
    "# get train losses from all epochs, a list of floats\n",
    "history[:, 'train_loss']\n",
    "# get train and valid losses from all epochs, a list of tuples\n",
    "history[:, ('train_loss', 'valid_loss')]\n",
    "# get current batches, a list of dicts\n",
    "history[-1, 'batches']\n",
    "# get latest batch, a dict\n",
    "history[-1, 'batches', -1]\n",
    "# get train losses from current batch, a list of floats\n",
    "history[-1, 'batches', :, 'train_loss']\n",
    "# get train and valid losses from current batch, a list of tuples\n",
    "history[-1, 'batches', :, ('train_loss', 'valid_loss')]\n",
    "```\n",
    "\n",
    "history 就是dicts的list，所以可以写入。skorch 提供了一些函数来使之更容易。比如 new_epoch()函数，它会增加一个新的epoch 字典到list最后。还有，有 new_batch() 可以向当前epoch增加 新的batches。\n",
    "\n",
    "为了向当前epoch增加新的item，可以使用这种形式：history.record('foo', 123) 。如果想给当前batch增加值：history.record_batch('bar', 456).\n",
    "\n",
    "以下是一些例子：\n",
    "\n",
    "```Python\n",
    "# history of a fitted neural net\n",
    "history = net.history\n",
    "# add new epoch row\n",
    "history.new_epoch()\n",
    "# add an entry to current epoch\n",
    "history.record('my-score', 123)\n",
    "# add a batch row to the current epoch\n",
    "history.new_batch()\n",
    "# add an entry to the current batch\n",
    "history.record_batch('my-batch-score', 456)\n",
    "# overwrite entry of current batch\n",
    "history.record_batch('my-batch-score', 789)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
