{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch使用快速入门\n",
    "\n",
    "本文主要参考[DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)。完全参照官方文档，并结合官方的示例：[pytorch/tutorials/beginner_source](https://github.com/pytorch/tutorials/tree/master/beginner_source)，主要涉及一些基本的概念和使用pytorch的方法：\n",
    "\n",
    "- pytorch的基本数据类型\n",
    "- autograd的基本概念\n",
    "- torch.nn包介绍\n",
    "- 数据前处理\n",
    "\n",
    "最后一部分最为重要，因为这是在自己运用pytorch时最关键的。\n",
    "\n",
    "## pytorch 基本数据类型\n",
    "\n",
    "pytorch作为NumPy的替代品，可以利用GPU的性能进行计算；可作为一个高灵活性、速度快的深度学习平台。\n",
    "\n",
    "Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。因此经常看到把numpy的数组包装为tensor再运算。tensor的操作和numpy中的数组操作类似，不再赘述，详见官网。下面列举一些简单例子。首先pytorch的导入是import torch，因为torch一直都是那个torch，一开始是别的语言写的，现在在python下，所以就叫pytorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor是pytorch的基本数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.4489e-39, 9.6429e-39, 8.4490e-39],\n",
       "        [9.6429e-39, 9.2755e-39, 1.0286e-38],\n",
       "        [9.0919e-39, 8.9082e-39, 9.2755e-39],\n",
       "        [8.4490e-39, 1.0194e-38, 9.0919e-39],\n",
       "        [8.4490e-39, 1.0745e-38, 1.0102e-38]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个 5x3 的矩阵, 未初始化的:\n",
    "x = torch.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中的一些基本运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4562, 0.4429, 0.9242],\n",
       "        [0.7533, 0.0493, 0.3528],\n",
       "        [0.1749, 0.8484, 0.0523],\n",
       "        [0.3759, 0.8893, 0.2466],\n",
       "        [0.0922, 0.4198, 0.3831]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个随机初始化的矩阵:\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到torch中的size也是torch中的类，包装了python的list，自然地，加减运算的对象也都是torch的tensor了。运算可以使用运算符，也可以使用函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1712, 0.7336, 1.7592],\n",
       "        [0.8079, 0.6690, 0.8303],\n",
       "        [0.2682, 1.0335, 0.6529],\n",
       "        [0.8095, 1.1712, 0.6900],\n",
       "        [0.9412, 1.1830, 1.3106]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加法\n",
    "y = torch.rand(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1712, 0.7336, 1.7592],\n",
       "        [0.8079, 0.6690, 0.8303],\n",
       "        [0.2682, 1.0335, 0.6529],\n",
       "        [0.8095, 1.1712, 0.6900],\n",
       "        [0.9412, 1.1830, 1.3106]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1712, 0.7336, 1.7592],\n",
       "        [0.8079, 0.6690, 0.8303],\n",
       "        [0.2682, 1.0335, 0.6529],\n",
       "        [0.8095, 1.1712, 0.6900],\n",
       "        [0.9412, 1.1830, 1.3106]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy和tensor之间有很多类似的地方，比如索引，形状改变等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4429, 0.0493, 0.8484, 0.8893, 0.4198])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用类似Numpy的索引来处理所有的张量！\n",
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([[ 0.8189,  0.4966,  0.2559,  0.6062],\n",
      "        [ 0.1519,  0.3242,  0.7626,  0.2940],\n",
      "        [ 1.1054,  1.0715, -1.5740,  1.4157],\n",
      "        [ 0.0329,  0.0652,  0.5474, -1.2912]])\n",
      "tensor([ 0.8189,  0.4966,  0.2559,  0.6062,  0.1519,  0.3242,  0.7626,  0.2940,\n",
      "         1.1054,  1.0715, -1.5740,  1.4157,  0.0329,  0.0652,  0.5474, -1.2912])\n",
      "tensor([[ 0.8189,  0.4966,  0.2559,  0.6062,  0.1519,  0.3242,  0.7626,  0.2940],\n",
      "        [ 1.1054,  1.0715, -1.5740,  1.4157,  0.0329,  0.0652,  0.5474, -1.2912]])\n"
     ]
    }
   ],
   "source": [
    "# 改变大小: 如果你想要去改变tensor的大小, 可以使用 torch.view:\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # -1就是让pytorch自己根据其他的维度去判断这里该是几维\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于和numpy的紧密联系，因此pytorch的张量和numpy数组可以很方便的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意numpy的array和torch的tensor转换后，数据是绑定的，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy的array： [1. 1. 1. 1. 1.]\n",
      "array转为torch的tensor： tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 看改变 np 数组之后 Torch Tensor 是如何自动改变的\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(\"numpy的array：\",a)\n",
    "print(\"array转为torch的tensor：\",b)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用 .cuda 方法将 Tensors 在GPU上运行.\n",
    "# 只要在  CUDA 是可用的情况下, 我们可以运行这段代码\n",
    "if torch.cuda.is_available():\n",
    "    b = b.cuda()\n",
    "    print(b + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。\n",
    "\n",
    "张量要在GPU上计算，需要主动从CPU移动到GPU上。张量可以使用.to方法移动到任何设备（device）上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8521])\n",
      "-0.8521100282669067\n",
      "tensor([0.1479], device='cuda:0')\n",
      "tensor([0.1479], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "# 当GPU可用时,我们可以运行以下代码\n",
    "# 我们将使用`torch.device`来将tensor移入和移出GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor\n",
    "    x = x.to(device)                       # 或者使用`.to(\"cuda\")`方法\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # `.to`也能在移动时改变dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd自动求导\n",
    "\n",
    "PyTorch中，所有神经网络的核心是 autograd 包。\n",
    "\n",
    "autograd 包为**张量上的所有操作**提供了**自动求导机制**。它是一个在运行时定义（define-by-run）的框架，这意味着**反向传播是根据代码如何运行来决定的**，并且每次迭代可以是不同的.\n",
    "\n",
    "**torch.Tensor** 是这个包的核心类。如果设置它的属性 **.requires_grad** 为 True，那么它将会**追踪对于该张量的所有操作**。当完成计算后可以通过**调用 .backward()，来自动计算所有的梯度**。这个张量的所有梯度将会自动累加到.grad属性.\n",
    "\n",
    "为了阻止跟踪历史，可以使用 with torch.no_grad(): 包裹代码块，在评价模型时这很有用，因为模型可能有有参数 requires_grad=True 的能训练的参数，但是这时候我们不需要梯度。\n",
    "\n",
    "还有一个类对于autograd的实现非常重要：**Function**。Tensor 和 Function 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个tensor有 .grad_fn 属性，该属性指向一个创建Tensor的Function。用户创建的tensor的grad_fn 是None，即新建一个tensor，它是没有grad_fn的。\n",
    "\n",
    "如果想计算微分，可以在Tensor上调用.backward()。如果Tensor是一个标量（只含一个数据），就不需要给backward()指定参数，不过当元素较多时，需要制定一个gradient参数，这是一个有匹配shape的tensor。\n",
    "\n",
    "上面这段话还是有点晦涩，所以看例子。注意现在的版本已经没有Varaible了，所以可以不考虑它了。先构造一个tensor，设置requires_grad=True来跟踪计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor可以做计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意y是一个计算的结果，所以如前面文字所述，它有grad_fn，指向创建它的Function。那么看下这个grad_fn是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000020A18EE5088>\n"
     ]
    }
   ],
   "source": [
    "# y 由操作创建,所以它有 grad_fn 属性.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着做更多运算，注意看看grad_fn发生了什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y 的更多操作\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到grad_fn已经是多层函数了。tensor调用.requires_grad_( ... )可以就地改变tensor的requires_grad 属性，requires_grad 如果不指定，默认是False。例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad_(True)\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x20a18ef4c48>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = (a * a).sum()\n",
    "b.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子可以看出，默认的tensor由于没有设置requires_grad，其grad_fn是None，而设置了之后，再计算，b就有grad_fn了。\n",
    "\n",
    "接下来，看看Gradients，看看如何反向传播。开头的文字描述中有说到，执行backward就会进行反向传播计算了，看看前面的out变量。因为out现在是一个标量，所以out.backward() 与 out.backward(torch.Tensor([1.0])) 这样的方式一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看x的梯度是什么样， 即 d(out)/dx ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考下这个 4.5 的矩阵是怎么计算的。 x = [[1., 1.], [1., 1.]]，y = x + 2 = [[3., 3.], [3., 3.]]，z = y * y * 3 = [[27., 27.], [27., 27.]]，out = 27.\n",
    "\n",
    "简单推导下 out 和 x 的梯度. 微积分的链式法则，\n",
    "$$x.grad=\\frac{d(out)}{dx} = \\frac{d(out)}{dz} \\frac{dz}{dy} \\frac{dy}{dx}$$\n",
    "那么接下来计算每个部分，首先明确各个部分的实际数学表达（z和x的关系直接表达了）：\n",
    "$$out=\\frac{1}{4}\\sum_i z_i, z_i=3(x_i+2)^2$$\n",
    "\n",
    "这里直接看单个的元素的链式法则，$\\frac{\\partial{out}}{\\partial{z_i}}=\\frac{1}{4}, \\frac{\\partial z_i}{\\partial x_i}=6(x_i+2)$. 因此, $\\frac{\\partial o}{\\partial x_i} = \\frac 3 2(x_i+2)$, 所以 $\\frac{\\partial o}{\\partial x_i}|_{x_i=1}=4.5$\n",
    "\n",
    "关于矩阵微分的直接运算，如果感兴趣，可以参考：[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/)。本文目的不是了解数学基础，所以就不多说了，简而言之，数学上，对于一个向量函数$\\vec{y}=f(\\vec{x})$，$\\vec{y}$ 相对于 $\\vec{x}$ 的梯度就是一个雅可比矩阵。所以，torch.autograd 就是一个计算 vector-Jacobian product 的引擎。即给定任意向量 $v=(v_1 \\ v_2  ... \\ v_m)^T$，计算点积$v^T \\cdot J$。如果$v$正好是标量函数$l=g(\\vec y)$的梯度，即$v=({\\frac{\\partial{l}}{\\partial{y_1}} ... \\frac{\\partial{l}}{\\partial{y_m}}})^T$，那么根据链式法则， vector-Jacobian product 就是$l$关于$x$的梯度：\n",
    "$$J^T \\cdot v = \\begin{bmatrix}\n",
    " \\frac{\\partial y_1}{\\partial{x_1}}  \\ ... \\ \\frac{\\partial{y_m}}{\\partial{x_1}} \\\\ \n",
    "  \\vdots \\ \\ddots \\ \\vdots \\\\\n",
    " \\frac{\\partial{y_1}}{\\partial{x_n}}  \\ ... \\ \\frac{\\partial{y_m}}{\\partial{x_n}} \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial{y_1}} \\\\ \n",
    " \\vdots \\\\ \n",
    " \\frac{\\partial{l}}{\\partial{y_m} }\n",
    "\\end{bmatrix} \\dots \\begin{bmatrix}\n",
    " \\frac{\\partial{l}}{\\partial{x_1}} \\\\ \n",
    " \\vdots \\\\ \n",
    " \\frac{\\partial{l}}{\\partial{x_m}} \n",
    "\\end{bmatrix}$$\n",
    "注意，$J^T \\cdot v $给出的结果是列向量，前面提到的$v^T \\cdot J$ 给出的是行向量，没有本质区别。有了公式，就使得将external gradients给到非标量输出的模型变得容易。\n",
    "\n",
    "这里对vector-Jacobian product 的说明还有点晦涩. 所以接下来用其他例子进一步理解下 vector 和 Jacobian 分别是什么，external gradients又是什么意思。\n",
    "\n",
    "参考的blog有：[Pytorch中的vector-Jacobian product](https://juejin.im/post/5de5fbaae51d4523855e6dcc)和[详解Pytorch 自动微分里的（vector-Jacobian product）](https://zhuanlan.zhihu.com/p/65609544)\n",
    "\n",
    "$Y=G(X)$，Y和X都是向量，Y对X求导就是雅可比矩阵。如上面的例子，$Y=X^2$，求导的雅可比矩阵就是：\n",
    "\n",
    "$$ J = \\begin{bmatrix}\n",
    " 2x_1 \\ \\ \\ 0 \\ \\ \\ 0   \\\\ \n",
    " 0 \\ \\ \\ 2x_2 \\ \\ \\ 0  \\\\ \n",
    " 0 \\ \\ \\ 0 \\ \\ \\ 2x_3 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "注意$y_1=f_1(x_1,x_2,x_3)=x_1^2$，它是关于(x_1,x_2,x_3)的函数，而不仅仅只是关于$x_1$。而d(Y)对每一个分量$x_i$的导数是各个分量函数$y_j, j=1,2,3$对$x_i$的偏导数沿某一方向v的累积，一般默认的v是v=(1,1,1)，当然也可以传入一个向量来指定方向，这个vector就是所谓vector-Jacobian 中的vector的含义。\n",
    "\n",
    "而官方文档中说的可easy feed external gradient，可以将其理解为容易得到$x_i$的偏导数向量在v方向上的投影，或者各个分量函数关于$x_i$偏导的权重。v一旦确定，关于每个$x_i$的权重就确定了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4438, 0.8266, 1.9605], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1969, 0.6833, 3.8435], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones(3))  # 向量反向传播，要指定参数，一般指定默认方向，就是[1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8875, 1.6532, 3.9210])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  # 因为是默认方向，所以结果就是2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，看看Jacobian矩阵，手算验证下，见注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(2., requires_grad=True)\n",
      "tensor(3., requires_grad=True)\n",
      "tensor([ 8., 18., 33.], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "x1=torch.tensor(1, requires_grad=True, dtype = torch.float)\n",
    "x2=torch.tensor(2, requires_grad=True, dtype = torch.float)\n",
    "x3=torch.tensor(3, requires_grad=True, dtype = torch.float)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "y=torch.randn(3) # 定义y变量\n",
    "y[0]=x1**2+2*x2+x3 # 每个值定义不同的函数\n",
    "y[1]=x1+x2**3+x3**2\n",
    "y[2]=2*x1+x2**2+x3**3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad  # [2x_1 1 2] 代入数据求和就是5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.grad  # [2 3_x2^2 2x_2] 代入数据求和就是18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.grad  # [1 2x_3 3x_3^2] 代入数据求和就是34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不一样的vector看一下，这时候就可以使用公式$J^T \\cdot v $了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=torch.tensor(1, requires_grad=True, dtype = torch.float)\n",
    "x2=torch.tensor(2, requires_grad=True, dtype = torch.float)\n",
    "x3=torch.tensor(3, requires_grad=True, dtype = torch.float)\n",
    "y=torch.randn(3)\n",
    "y[0]=x1**2+2*x2+x3\n",
    "y[1]=x1+x2**3+x3**2\n",
    "y[2]=2*x1+x2**2+x3**3\n",
    "v=torch.tensor([3,2,1],dtype=torch.float)\n",
    "y.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad  # y对x_1偏导和v点积之后：[6x_1 2 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.grad  # y对x_2偏导和v点积之后：[6 6x_2^2 2x_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.grad  # y对x_3偏导和v点积之后：[3 4x_3 3x_3^2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在再看下官方的例子。这个例子是说，随机生成一个向量，乘2，然后求二范式，循环乘2多次直至使y的2范式大于1000得到最终得y。指定vector，backward就知道往哪算了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1124, -0.8276,  0.8662], requires_grad=True)\n",
      "tensor([ 0.2247, -1.6553,  1.7324], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.2247, -1.6553,  1.7324])\n",
      "tensor(2.4065)\n",
      "tensor([ 115.0533, -847.5005,  886.9653], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x * 2\n",
    "print(y)\n",
    "print(y.data)\n",
    "print(y.data.norm()) # the L2 norm (a.k.a Euclidean norm) of the tensor\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y是向量不是标量，所以torch.autograd 需要指定一个向量，变量才能执行backward。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最开始前面的文档说了可以通过用 with torch.no_grad() 包装代码块停止autograd 跟踪.requires_grad=True的tensors，下面看看具体操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者也可以使用.detach()来获得一个新的Tensor，与之前的tensor有相同的内容，只是没有了gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理解torch.nn\n",
    "\n",
    "神经网络可以使用 **torch.nn** 包构建. \n",
    " \n",
    "autograd 实现了反向传播功能, 但是直接用来写深度学习的代码在很多情况下还是稍显复杂,torch.nn 是专门为神经网络设计的模块化接口. nn 构建于 Autograd 之上, 可用来定义和运行神经网络. nn依赖于autograd来定义模型，并作微分计算。nn.Module 是 nn 中最重要的类, 可把它看成是一个网络的封装, 包含网络各层定义以及 forward 方法, \n",
    "调用 forward(input) 方法, 可返回前向传播的结果. 比如手写数字识别CNN的结构：\n",
    "\n",
    "![](mnist.png)\n",
    "\n",
    "这是一个简单的前向网络，获取输入，层层前进，得到结果。\n",
    "\n",
    "一个典型的神经网络训练过程如下:\n",
    "\n",
    "- 定义具有一些可学习参数(或权重)的神经网络\n",
    "- 迭代输入数据集\n",
    "- 通过网络处理输入\n",
    "- 计算损失(输出的预测值与实际值之间的距离)\n",
    "- 将梯度传播回网络\n",
    "- 更新网络的权重, 通常使用一个简单的更新规则: weight = weight - learning_rate * gradient\n",
    "\n",
    "### 定义网络\n",
    "\n",
    "定义上图中的网络，关于CNN更多基本概念，参考后面的5-cnn-example文件夹中的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"注意定义的层都是利用nn中有的层进行定义的\"\"\"\n",
    "        # 首先定义构造函数，继承父类nn.Module 的构造函数\n",
    "        super(Net, self).__init__()\n",
    "        # 卷积层 '1'表示输入图片为单通道channel, '6'表示输出通道数channel, '3'表示卷积核为3*3\n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # 仿射运算: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward中利用function中的函数调用定义的网络各层进行前向计算\"\"\"\n",
    "        # 其中(2, 2)表示池化操作窗口(2, 2) \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果size是square , 则只能指定一个数字\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除 batch dimension 外的所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只需要计算forward函数，backward函数会使用autograd自动定义。在forward函数中可以使用任意的tensor运算。模型可以学习的参数由net.parameters()返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在尝试一个随机的32\\*32的输入，看看输出什么。注意torch.nn仅支持输入时mini-batch的，因此我们用单个例子来试也需要转换成mini－batch形式的。也可以使用input.unsqueeze(0)来增加一个假的batch层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0433,  0.1366, -0.2172, -0.1387,  0.0304, -0.0763,  0.0972, -0.1844,\n",
      "         -0.0249,  0.1643]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)  # nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有参数gradient缓存置零，随机方向反向传播，试试看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单回顾下目前为止的class：\n",
    "\n",
    "- torch.Tensor：一个支持autograd运算（比如backward()）的多维数组，并持有关于tensor的gradient；\n",
    "- nn.Module：神经网络模块。方便封装参数，并方便移入GPU，加载，输出等；\n",
    "- nn.Parameter：一类Tensor，当作为Module的属性时，会被自动注册为parameter；\n",
    "- autograd.Function：实现autograd的forward和backward定义。每个Tensor运算创建至少一个Function节点，该节点关联到创建Tensor的functions，并编码它的历史。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "loss function接受输入对（output和target），并计算两者之间的距离。在nn包下有一些不同的loss functions。一个比较简单的是MSELoss，它计算mean-squared error。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor(0.5429, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(target.shape)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在反向跟踪loss，使用.grad_fn属性，计算图是这样的：\n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "      \n",
    "调用loss.backward()时，整个计算图都会根据loss做微分，graph中所有有requires_grad=True 属性的Tensors都有累积gradient的.grad Tensor 。 个人理解，每个requires_grad=True 的节点都是知道累计到它这的梯度的tensor。用.grad_fn.next_functions[0][0]可以上溯看function："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7efd94f1bb10>\n",
      "<AddmmBackward object at 0x7efd94f1bd10>\n",
      "<AccumulateGrad object at 0x7efd94f1bb10>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "为了反向传播，需要使用loss.backward()。您需要清除现有的梯度，否则梯度将积累到现有的梯度。关于为什么需要清楚梯度，这里解释的不是太清楚。因此，参考一些blog做说明：[PyTorch中在反向传播前为什么要手动将梯度清零？](https://www.zhihu.com/question/303070254)，[PyTorch中的梯度累加](https://www.cnblogs.com/lart/p/11628696.html)，首先手动清零就说明默认的是pytoch会对梯度进行累加。关于累加的意思，要更好地理解就需要重新了解下pytorch的一些基本特性，这里参考：[PyTorch经验指南：技巧与陷阱](https://www.jiqizhixin.com/articles/2018-07-30-2) 对pytorch的内容再做次回顾。\n",
    "首先了解补充下关于计算图的内容，计算图(Computation Graph)是现代深度学习框架如PyTorch和TensorFlow等的核心，其为高效自动求导算法——反向传播(Back Propogation)提供了理论支持，了解计算图在实际写程序过程中会有极大的帮助。可以阅读：[Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)。\n",
    "\n",
    "计算图示一种很好地思考数学表达的方式。比如：$e=(a+b)*(b+1)$ 中有三个运算。引入两个中间变量，现在有：$c=a+b$, $d=b+1$, $e=c*d$\n",
    "\n",
    "现在将输入及等式转为节点，构建一个计算图：\n",
    "\n",
    "![](tree-def.png)\n",
    "\n",
    "有了计算图，逐个节点计算可以很容易前向计算。现在看计算图中的微分。关键是**理解边（edge）上的微分**。比如a变化，那它如何影响c？c会怎么变化？这就是c关于a的偏微分。这里的例子就是利用求导法则计算，比如：\n",
    "$$\\frac{\\partial}{\\partial a}(a+b)=\\frac{\\partial a}{\\partial a}+\\frac{\\partial b}{\\partial a}=1$$\n",
    "\n",
    "所有边的情况如下图：\n",
    "\n",
    "![](Picture3.png)\n",
    "\n",
    "那关于不直接相邻的节点如何互相影响呢？比如e关于a的变化？这就可以用链式法则。比如a变化1，c就变化1；c变化1，e就变化2，所以e相对于a的变化就是1×2。\n",
    "\n",
    "更一般地就是将一个节点到另一个节点所有edge上的影响总和起来。比如e对于b，$\\frac{\\partial e}{\\partial b}=1*2+1*3$, “sum over paths” 就是链式法则的另一种形式。\n",
    "\n",
    "再比如，\n",
    "\n",
    "![](Picture4.png)\n",
    "$\\frac{\\partial Z}{\\partial X}=\\alpha \\delta +\\alpha \\epsilon +\\alpha \\zeta +\\beta \\delta +\\beta \\epsilon +\\beta \\zeta +\\gamma \\delta +\\gamma \\epsilon +\\gamma \\zeta$\n",
    "\n",
    "一种更好地表达是：\n",
    "$\\frac{\\partial Z}{\\partial X}=(\\alpha +\\beta +\\gamma)(\\delta +\\epsilon +\\zeta)$\n",
    "\n",
    "这就是“forward-mode differentiation”和“reverse-mode differentiation”的由来。它们是通过分解路径来高效计算和的算法。它们不是显式地对所有路径求和，而是**通过在每个节点合并路径来更有效地计算相同的和**。关键是这两种算法都**只接触每条边一次*。\n",
    "\n",
    "Forward-mode differentiation 从输入开始到最后末端。每个节点将所有进入它的路径都加和起来，每个路径都是一个输入怎么影响它的，加那和就是将每个节点对它的影响整合起来。\n",
    "\n",
    "![](Picture5.png)\n",
    "\n",
    "Reverse-mode differentiation 相似：\n",
    "\n",
    "![](Picture6.png)\n",
    "\n",
    "Forward-mode differentiation 是跟踪一个输入如何影响每个节点，Reverse-mode differentiation 就是追踪每个节点如何影响一个输出。也就是forward-mode differentiation **将运算符 $\\frac{\\partial}{\\partial X}$ 作用到每个节点**，而 reverse mode differentiation 是**将运算符 $\\frac{\\partial Z}{\\partial}$ 作用到每个节点**。\n",
    "\n",
    "现在问题是为什么要关心reverse-mode differentiation，看起来和forward-mode没有本质区别。回到前面的e关于b变化的例子。\n",
    "\n",
    "forward-mode 是这样的：\n",
    "\n",
    "![](Picture7.png)\n",
    "\n",
    "reverse-mode 是这样的：\n",
    "\n",
    "![](Picture8.png)\n",
    "\n",
    "注意 reverse-mode differentiation 计算了e关于“*所有*”节点的微分。比如同时得到了$\\frac{\\partial e}{\\partial a}$和$\\frac{\\partial e}{\\partial b}$。而Forward-mode differentiation只能计算输出相对一个输入的变化。 这点区别十分重要，比如100万个输入，前向模式要计算100万次，而反向模式只用一次！！\n",
    "\n",
    "在神经网络的训练中，cost是参数的函数。我们想计算cost相对于所有参数的微分以用于梯度下降。如果有百万参数，那么reverse-mode differentiation就非常有用了，在神经网络中，这就是backpropagation 。\n",
    "\n",
    "（当有很多输出时，计算输出关于输入的变化，这时候用forward-mode differentiation更好）\n",
    "\n",
    "了解了计算图后，看看在pytorch中底层就采用了计算图的autograd。参考：[pytorch-book/chapter03-tensor_and_autograd/Autograd.ipynb](https://github.com/chenyuntc/pytorch-book/blob/master/chapter03-tensor_and_autograd/Autograd.ipynb)。用户会自己创建叶子节点，计算图的目标是计算根节点，利用链式法则，即刚说的reverse-mode differentiation，可以很容易的求得各节点的梯度。\n",
    "\n",
    "在PyTorch实现中，autograd会随着用户的操作，记录生成当前tensor的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作Function，每一个tensor在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播过程中，autograd沿着这个图从当前tensor（根节点）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个tensor的梯度，这些函数的函数名通常以Backward结尾。\n",
    "\n",
    "PyTorch使用的是动态图，它的计算图在**每次前向传播时都是从头开始构建**，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。\n",
    "\n",
    "tensor的requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都是True。这其实很好理解，对于$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$，x.requires_grad = True，当需要计算$\\partial z \\over \\partial x$时，根据链式法则，$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$，自然也需要求$ \\frac{\\partial z}{\\partial y}$，所以y.requires_grad会被自动标为True.\n",
    "\n",
    "绝大多数函数都可以使用autograd实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办? 那就需要写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形， 它接收参数，计算并返回结果。具体等用到的时候再查。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "通常来说，当必须处理图像、文本、音频或视频数据时，可以使用python标准库将数据**加载到numpy数组**里。然后将这个数组**转化成torch.*Tensor**。\n",
    "\n",
    "- 对于图片，有Pillow，OpenCV等包可以使用；\n",
    "- 对于音频，有scipy和librosa等包可以使用；\n",
    "- 对于文本，不管是原生python的或者是基于Cython的文本，可以使用NLTK和SpaCy。\n",
    "\n",
    "特别对于视觉方面，我们创建了一个包，名字叫torchvision，其中包含了针对Imagenet、CIFAR10、MNIST等常用数据集的数据加载器（data loaders），还有对图片数据变形的操作，即torchvision.datasets和torch.utils.data.DataLoader。\n",
    "\n",
    "但是像网络上下载的纯数据，想要顺利地输入到自己定义的模型中，可能这一步就要花些时间来处理了。\n",
    "\n",
    "在训练中，想要利用GPU需要做一些处理：\n",
    "\n",
    "与将一个张量传递给GPU一样，可以这样**将神经网络转移到GPU上**。\n",
    "\n",
    "如果我们有cuda可用的话，让我们首先定义第一个设备为可见cuda设备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后这些方法将递归遍历所有模块，并将它们的参数和缓冲区转换为CUDA张量。请记住，我们不得不将**输入和目标**在**每一步都送入GPU**（inputs, labels = inputs.to(device), labels.to(device)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "可以展示模型训练时的一些统计指标以对训练过程有基本的认识。Pytorch现在整合了TensorBoard，它是一个设计来可视化神经网络训练过程结果的工具。有了它可以可视化更丰富的内容。\n",
    "\n",
    "接下来是一个例子，首先读取数据并做合适的变换。接着设置tensorboard并将数据写入tensorboard，然后就可以使用tensorboard了，包括查看训练数据，跟踪模型训练性能，以及评估训练后的模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是数据的前处理，接下来构建网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在开始设置tensorboard，从torch.utils导入，并定义SummaryWriter，这是向tensorboard写数据的关键对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意现在创建了新的文件夹runs/fashion_mnist_experiment_1。接下来把数据写入TensorBoard，使用make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在在本文件夹下打开命令行并运行：\n",
    "\n",
    "```Shell\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "然后就可以导航到 https://localhost:6006 查看交互式图形了。\n",
    "\n",
    "tensorboard的一个亮点是能可视化复杂的模型结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行完上面命令后，刷新TensorBoard，就会看到网页导航栏有“GRAPHS”。点击进入，就能看到模型结构，双击每个节点，就能看到内部的结构。\n",
    "\n",
    "接下来看看如何通过add_embedding方法来可视化高维数据为低维表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将loss记录进TensorBoard，并通过plot_classes_preds函数看模型预测。首先是一些辅助函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后训练模型，并使用add_scalar函数将结果写入TensorBoard，而不是输出到控制台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在在SCALARS标签下可以看到训练过程中loss的变化。最后可以看看如何评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        _, class_preds_batch = torch.max(output, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_preds = torch.cat(class_preds)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后在新的标签页可以看到结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跟着例子学习PyTorch\n",
    "\n",
    "通过例子了解pytorch的基本概念。\n",
    "\n",
    "PyTorch的核心是提供两个主要功能：\n",
    "\n",
    "- n维张量，类似于numpy，但可以在GPU上运行；\n",
    "- 自动区分以构建和训练神经网络。\n",
    "\n",
    "### 张量\n",
    "\n",
    "先使用 numpy 实现网络（更具体地可以参考:[numpy部分的手写神经网络](https://github.com/OuyangWenyu/hydrus/blob/master/3-numpy-examples/neural_network.py)），这里也做些记录，详见nn_basic.ipynb\n",
    "\n",
    "Numpy 提供了一个n维的数组对象, 并提供了许多操纵这个数组对象的函数。Numpy 是科学计算的通用框架; Numpy 数组没有计算图, 也没有深度学习, 也没有梯度下降等方法实现的接口。但是可以很容易地使用 numpy 生成随机数据，并将产生的数据传入双层的神经网络,并实现这个网络的正向传播和反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N是批尺寸参数；D_in是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算并显示loss（损失）\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy 是一个伟大的框架, 但它不能利用 GPU 加速它数值计算，而对于现代的深度神经网络, GPU 往往是提供 50倍或更大的加速,所以 numpy 不足以满足现在深度学习的需求。\n",
    "\n",
    "PyTorch提供了Tensor，其在概念上与 numpy 数组相同，也是一个n维数组, 不过PyTorch 提供了很多能在这些 Tensor 上操作的函数。\n",
    "\n",
    "像 numpy 数组一样, PyTorch Tensor 也和numpy的数组对象一样不了解深度学习,计算图和梯度下降，它们只是科学计算的通用工具；\n",
    "\n",
    "然而不像 numpy, PyTorch Tensor 可以利用 GPU 加速他们的数字计算。\n",
    "\n",
    "要在 GPU 上运行 PyTorch 张量, 只需将其转换为新的数据类型.\n",
    "\n",
    "将 PyTorch Tensor 生成的随机数据传入双层的神经网络. 就像上面的 numpy 例子一样,\n",
    "我们需要手动实现网络的正向传播和反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N是批尺寸大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求导\n",
    "\n",
    "对于小型的两层网络而言，手动实现反向传递并不重要，但对于大型的复杂网络而言，这变得非常麻烦。\n",
    "\n",
    "幸运的是，我们可以使用**自动微分** 来自动计算神经网络中的反向传播。PyTorch中的 autograd软件包提供了这个功能。使用autograd时，您的网络正向传递将定义一个 计算图；图中的节点为张量，图中的边为从输入张量产生输出张量的函数。通过该图进行反向传播，可以轻松计算梯度。\n",
    "\n",
    "这听起来很复杂，在实践中非常简单。**每个张量代表计算图中的一个节点**。如果 x是一个张量，并且有 x.requires_grad=True，那么x.grad就是另一个张量，代表着x相对于某个标量值的梯度。\n",
    "\n",
    "通过使用PyTorch张量和autograd来实现网络就不再需要手动实现网络的反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N是批尺寸大小；D_in是输入维度；\n",
    "# H是隐藏层维度；D_out是输出维度 \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据，将requires_grad置为False，意味着我们不需要在反向传播时候计算这些值的梯度\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 产生随机权重tensor，将requires_grad设置为True，意味着我们希望在反向传播时候计算这些值的梯度\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：使用tensor的操作计算预测值y。\n",
    "    # 由于w1和w2有requires_grad=True,涉及这些张量的操作将让PyTorch构建计算图，从而允许自动计算梯度。\n",
    "    # 由于我们不再手工实现反向传播，所以不需要保留中间值的引用。\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    # loss是一个形状为(1,)的张量\n",
    "    # loss.item()是这个张量对应的python数值\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 使用autograd计算反向传播,这个调用将计算loss对所有requires_grad=True的tensor的梯度。\n",
    "    # 这次调用后，w1.grad和w2.grad将分别是loss对w1和w2的梯度张量。\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段构建计算图，\n",
    "    # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 反向传播之后手动将梯度置零，原因在后面第三节nn到底是什么一节中简单补充说明\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在底层，每一个原始的**自动求导运算**实际上是**两个在Tensor上运行的函数**。其中，**forward**函数计算从输入Tensors获得的输出Tensors。而**backward**函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。\n",
    "\n",
    "在PyTorch中，可以很容易地通过定义**torch.autograd.Function**的子类并实现**forward和backward函数**，来**定义自己的自动求导运算**。然后，我们可以通过**构造实例**并**像调用函数一样调用它**，并传递包含输入数据的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "神经网络是计算图的一个子类。计算图接收输入数据，数据被路由到对数据执行处理的节点，并可能被这些节点变换。在深度学习中，神经网络中的**神经元（节点）通常利用参数或可微函数转换数据**，这样可以优化参数以通过梯度下降将损失最小化。\n",
    "\n",
    "PyTorch 使用一种称之为 imperative / eager 的范式，即**每一行代码都要求构建一个图**，以定义完整计算图的一个部分。即使完整的计算图还没有构建好，我们也**可以独立地执行这些作为组件的小计算图**，这种动态计算图被称为「define-by-run」方法。\n",
    "\n",
    "![](01.gif)\n",
    "\n",
    "TensorFlow、Caffe 和 CNTK 等大多数框架都使用静态计算图，开发者**必须建立或定义一个神经网络，并重复使用相同的结构来执行模型训练**。改变网络的模式就意味着我们必须从头开始设计并定义相关的模块。\n",
    "\n",
    "但 PyTorch 使用的技术为**自动微分（automatic differentiation）**。在这种机制下，系统会有一个 **Recorder 来记录我们执行的运算**，然后再**反向计算对应的梯度**。这种技术在构建神经网络的过程中十分强大，因为我们可以通过计算前向传播过程中参数的微分来节省时间。\n",
    "\n",
    "Autograd 会**维护一个图并记录对变量执行的所有运算**。这会产生一个有向无环图，其中叶结点为输入向量，根结点为输出向量。通过从根结点到叶结点追踪图的路径，我们可以轻易地使用链式法则自动计算梯度。\n",
    "\n",
    "回来看模型的训练过程，在使用 loss.backward() 收集一系列新的梯度并做反向传播之前，有必要手动地将由 net.zero_grad() 优化的参数梯度归零。默认情况下，PyTorch 会累加梯度，梯度累加就是，**每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加**，累加一定次数后，**根据累加的梯度更新网络参数**，然后清空梯度，进行下一次循环。\n",
    "\n",
    "这样的理由：\n",
    "\n",
    "一个是借助梯度累加，避免同时计算多个损失时存储多个计算图。在PyTorch中，multi-task任务一个标准的train from scratch流程为：\n",
    "\n",
    "```python\n",
    "for idx, data in enumerate(train_loader):\n",
    "    xs, ys = data\n",
    "    pred1 = model1(xs)\n",
    "    pred2 = model2(xs)\n",
    "\n",
    "    loss1 = loss_fn1(pred1, ys)\n",
    "    loss2 = loss_fn2(pred2, ys)\n",
    "\n",
    "    ******\n",
    "    loss = loss1 + loss2\n",
    "    optmizer.zero_grad()\n",
    "    loss.backward()\n",
    "    ++++++\n",
    "    optmizer.step()\n",
    "```\n",
    "\n",
    "从PyTorch的设计原理上来说，在每次进行前向计算得到pred时，会产生一个用于梯度回传的计算图，这张图储存了进行back propagation需要的中间结果，当调用了 **.backward()** 后，会从内存中将这张图进行释放。\n",
    "\n",
    "上述代码执行到 ****** 时，内存中是包含了两张计算图的，而随着求和得到loss，这两张图进行了合并，而且大小的变化可以忽略。\n",
    "\n",
    "执行到 ++++++ 时，得到对应的grad值并且释放内存。这样，训练时必须存储两张计算图，而如果loss的来源组成更加复杂，内存消耗会更大。\n",
    "\n",
    "为了减小每次的内存消耗，借助梯度累加，又有$\\partial (l_1+l_2)/\\partial (x)=\\partial (l_1)/\\partial (x)+\\partial (l_2)/\\partial (x)$，所以有：\n",
    "\n",
    "```python\n",
    "for idx, data in enumerate(train_loader):\n",
    "    xs, ys = data\n",
    "\n",
    "    optmizer.zero_grad()\n",
    "\n",
    "    # 计算d(l1)/d(x)\n",
    "    pred1 = model1(xs) #生成graph1\n",
    "    loss = loss_fn1(pred1, ys)\n",
    "    loss.backward()  #释放graph1\n",
    "\n",
    "    # 计算d(l2)/d(x)\n",
    "    pred2 = model2(xs)#生成graph2\n",
    "    loss2 = loss_fn2(pred2, ys)\n",
    "    loss.backward()  #释放graph2\n",
    "\n",
    "    # 使用d(l1)/d(x)+d(l2)/d(x)进行优化\n",
    "    optmizer.step()\n",
    "```\n",
    "\n",
    "可以从代码中看出，利用梯度累加，可以在最多保存一张计算图的情况下进行multi-task任务的训练。\n",
    "\n",
    "另一个理由是，**在单次迭代中没有足够资源来计算所有需要的梯度时，这种做法非常便利**。在内存大小不够的情况下叠加多个batch的grad作为一个大batch进行迭代，因为二者得到的梯度是等价的。一定条件下，batchsize越大训练效果越好，梯度累加则实现了batchsize的变相扩大，如果accumulation_steps为8，则batchsize '变相' 扩大了8倍，是解决显存受限的一个不错的trick，使用时需要注意，学习率也要适当放大。\n",
    "\n",
    "综上可知，这种梯度累加的思路是对内存的极大友好，是由FAIR的设计理念出发的。\n",
    "\n",
    "总之，自己使用的时候记得调用下zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0093, -0.0126,  0.0087, -0.0158, -0.0098,  0.0159])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多敢于loss function和modules的信息可以参考文档：[TORCH.NN](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "### 更新权重\n",
    "\n",
    "最简单的更新规则就是随机梯度下降（SGD）：weight = weight - learning_rate * gradient ， pytorch中有很多不同的更新规则，比如Nesterov-SGD, Adam, RMSProp, etc.  torch.optim 包中实现了这些算法，可以很容易的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
