{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch使用快速入门\n",
    "\n",
    "本文主要参考[DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)。完全参照官方文档，并结合官方的示例：[pytorch/tutorials/beginner_source](https://github.com/pytorch/tutorials/tree/master/beginner_source)，主要涉及一些基本的概念和使用pytorch的方法：\n",
    "\n",
    "- pytorch的基本数据类型\n",
    "- autograd的基本概念\n",
    "- 理解torch.nn包\n",
    "- 实例：训练一个分类器\n",
    "\n",
    "最后一部分最为重要，因为这是在自己运用pytorch时最关键的。\n",
    "\n",
    "## pytorch 基本数据类型\n",
    "\n",
    "pytorch作为NumPy的替代品，可以利用GPU的性能进行计算；可作为一个高灵活性、速度快的深度学习平台。\n",
    "\n",
    "Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。因此经常看到把numpy的数组包装为tensor再运算。tensor的操作和numpy中的数组操作类似，不再赘述，详见官网。下面列举一些简单例子。首先pytorch的导入是import torch，因为torch一直都是那个torch，一开始是别的语言写的，现在在python下，所以就叫pytorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor是pytorch的基本数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.4489e-39, 9.6429e-39, 8.4490e-39],\n",
       "        [9.6429e-39, 9.2755e-39, 1.0286e-38],\n",
       "        [9.0919e-39, 8.9082e-39, 9.2755e-39],\n",
       "        [8.4490e-39, 1.0194e-38, 9.0919e-39],\n",
       "        [8.4490e-39, 1.0745e-38, 1.0102e-38]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个 5x3 的矩阵, 未初始化的:\n",
    "x = torch.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中的一些基本运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4562, 0.4429, 0.9242],\n",
       "        [0.7533, 0.0493, 0.3528],\n",
       "        [0.1749, 0.8484, 0.0523],\n",
       "        [0.3759, 0.8893, 0.2466],\n",
       "        [0.0922, 0.4198, 0.3831]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个随机初始化的矩阵:\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到torch中的size也是torch中的类，包装了python的list，自然地，加减运算的对象也都是torch的tensor了。运算可以使用运算符，也可以使用函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1712, 0.7336, 1.7592],\n",
       "        [0.8079, 0.6690, 0.8303],\n",
       "        [0.2682, 1.0335, 0.6529],\n",
       "        [0.8095, 1.1712, 0.6900],\n",
       "        [0.9412, 1.1830, 1.3106]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加法\n",
    "y = torch.rand(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1712, 0.7336, 1.7592],\n",
       "        [0.8079, 0.6690, 0.8303],\n",
       "        [0.2682, 1.0335, 0.6529],\n",
       "        [0.8095, 1.1712, 0.6900],\n",
       "        [0.9412, 1.1830, 1.3106]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1712, 0.7336, 1.7592],\n",
       "        [0.8079, 0.6690, 0.8303],\n",
       "        [0.2682, 1.0335, 0.6529],\n",
       "        [0.8095, 1.1712, 0.6900],\n",
       "        [0.9412, 1.1830, 1.3106]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy和tensor之间有很多类似的地方，比如索引，形状改变等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4429, 0.0493, 0.8484, 0.8893, 0.4198])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用类似Numpy的索引来处理所有的张量！\n",
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([[ 0.8189,  0.4966,  0.2559,  0.6062],\n",
      "        [ 0.1519,  0.3242,  0.7626,  0.2940],\n",
      "        [ 1.1054,  1.0715, -1.5740,  1.4157],\n",
      "        [ 0.0329,  0.0652,  0.5474, -1.2912]])\n",
      "tensor([ 0.8189,  0.4966,  0.2559,  0.6062,  0.1519,  0.3242,  0.7626,  0.2940,\n",
      "         1.1054,  1.0715, -1.5740,  1.4157,  0.0329,  0.0652,  0.5474, -1.2912])\n",
      "tensor([[ 0.8189,  0.4966,  0.2559,  0.6062,  0.1519,  0.3242,  0.7626,  0.2940],\n",
      "        [ 1.1054,  1.0715, -1.5740,  1.4157,  0.0329,  0.0652,  0.5474, -1.2912]])\n"
     ]
    }
   ],
   "source": [
    "# 改变大小: 如果你想要去改变tensor的大小, 可以使用 torch.view:\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # -1就是让pytorch自己根据其他的维度去判断这里该是几维\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于和numpy的紧密联系，因此pytorch的张量和numpy数组可以很方便的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意numpy的array和torch的tensor转换后，数据是绑定的，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy的array： [1. 1. 1. 1. 1.]\n",
      "array转为torch的tensor： tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 看改变 np 数组之后 Torch Tensor 是如何自动改变的\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(\"numpy的array：\",a)\n",
    "print(\"array转为torch的tensor：\",b)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用 .cuda 方法将 Tensors 在GPU上运行.\n",
    "# 只要在  CUDA 是可用的情况下, 我们可以运行这段代码\n",
    "if torch.cuda.is_available():\n",
    "    b = b.cuda()\n",
    "    print(b + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。\n",
    "\n",
    "张量要在GPU上计算，需要主动从CPU移动到GPU上。张量可以使用.to方法移动到任何设备（device）上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8521])\n",
      "-0.8521100282669067\n",
      "tensor([0.1479], device='cuda:0')\n",
      "tensor([0.1479], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "# 当GPU可用时,我们可以运行以下代码\n",
    "# 我们将使用`torch.device`来将tensor移入和移出GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor\n",
    "    x = x.to(device)                       # 或者使用`.to(\"cuda\")`方法\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # `.to`也能在移动时改变dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd自动求导\n",
    "\n",
    "PyTorch中，所有神经网络的核心是 autograd 包。\n",
    "\n",
    "autograd 包为**张量上的所有操作**提供了**自动求导机制**。它是一个在运行时定义（define-by-run）的框架，这意味着**反向传播是根据代码如何运行来决定的**，并且每次迭代可以是不同的.\n",
    "\n",
    "**torch.Tensor** 是这个包的核心类。如果设置它的属性 **.requires_grad** 为 True，那么它将会**追踪对于该张量的所有操作**。当完成计算后可以通过**调用 .backward()，来自动计算所有的梯度**。这个张量的所有梯度将会自动累加到.grad属性.\n",
    "\n",
    "为了阻止跟踪历史，可以使用 with torch.no_grad(): 包裹代码块，在评价模型时这很有用，因为模型可能有有参数 requires_grad=True 的能训练的参数，但是这时候我们不需要梯度。\n",
    "\n",
    "还有一个类对于autograd的实现非常重要：**Function**。Tensor 和 Function 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个tensor有 .grad_fn 属性，该属性指向一个创建Tensor的Function。用户创建的tensor的grad_fn 是None，即新建一个tensor，它是没有grad_fn的。\n",
    "\n",
    "如果想计算微分，可以在Tensor上调用.backward()。如果Tensor是一个标量（只含一个数据），就不需要给backward()指定参数，不过当元素较多时，需要制定一个gradient参数，这是一个有匹配shape的tensor。\n",
    "\n",
    "上面这段话还是有点晦涩，所以看例子。注意现在的版本已经没有Varaible了，所以可以不考虑它了。先构造一个tensor，设置requires_grad=True来跟踪计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor可以做计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意y是一个计算的结果，所以如前面文字所述，它有grad_fn，指向创建它的Function。那么看下这个grad_fn是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000020A18EE5088>\n"
     ]
    }
   ],
   "source": [
    "# y 由操作创建,所以它有 grad_fn 属性.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着做更多运算，注意看看grad_fn发生了什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y 的更多操作\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到grad_fn已经是多层函数了。tensor调用.requires_grad_( ... )可以就地改变tensor的requires_grad 属性，requires_grad 如果不指定，默认是False。例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad_(True)\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x20a18ef4c48>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = (a * a).sum()\n",
    "b.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子可以看出，默认的tensor由于没有设置requires_grad，其grad_fn是None，而设置了之后，再计算，b就有grad_fn了。\n",
    "\n",
    "接下来，看看Gradients，看看如何反向传播。开头的文字描述中有说到，执行backward就会进行反向传播计算了，看看前面的out变量。因为out现在是一个标量，所以out.backward() 与 out.backward(torch.Tensor([1.0])) 这样的方式一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看x的梯度是什么样， 即 d(out)/dx ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思考下这个 4.5 的矩阵是怎么计算的。 x = [[1., 1.], [1., 1.]]，y = x + 2 = [[3., 3.], [3., 3.]]，z = y * y * 3 = [[27., 27.], [27., 27.]]，out = 27.\n",
    "\n",
    "简单推导下 out 和 x 的梯度. 微积分的链式法则，\n",
    "$$x.grad=\\frac{d(out)}{dx} = \\frac{d(out)}{dz} \\frac{dz}{dy} \\frac{dy}{dx}$$\n",
    "那么接下来计算每个部分，首先明确各个部分的实际数学表达（z和x的关系直接表达了）：\n",
    "$$out=\\frac{1}{4}\\sum_i z_i, z_i=3(x_i+2)^2$$\n",
    "\n",
    "这里直接看单个的元素的链式法则，$\\frac{\\partial{out}}{\\partial{z_i}}=\\frac{1}{4}, \\frac{\\partial z_i}{\\partial x_i}=6(x_i+2)$. 因此, $\\frac{\\partial o}{\\partial x_i} = \\frac 3 2(x_i+2)$, 所以 $\\frac{\\partial o}{\\partial x_i}|_{x_i=1}=4.5$\n",
    "\n",
    "关于矩阵微分的直接运算，如果感兴趣，可以参考：[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/)。本文目的不是了解数学基础，所以就不多说了，简而言之，数学上，对于一个向量函数$\\vec{y}=f(\\vec{x})$，$\\vec{y}$ 相对于 $\\vec{x}$ 的梯度就是一个雅可比矩阵。所以，torch.autograd 就是一个计算 vector-Jacobian product 的引擎。即给定任意向量 $v=(v_1 \\ v_2  ... \\ v_m)^T$，计算点积$v^T \\cdot J$。如果$v$正好是标量函数$l=g(\\vec y)$的梯度，即$v=({\\frac{\\partial{l}}{\\partial{y_1}} ... \\frac{\\partial{l}}{\\partial{y_m}}})^T$，那么根据链式法则， vector-Jacobian product 就是$l$关于$x$的梯度：\n",
    "$$J^T \\cdot v = \\begin{bmatrix}\n",
    " \\frac{\\partial y_1}{\\partial{x_1}}  \\ ... \\ \\frac{\\partial{y_m}}{\\partial{x_1}} \\\\ \n",
    "  \\vdots \\ \\ddots \\ \\vdots \\\\\n",
    " \\frac{\\partial{y_1}}{\\partial{x_n}}  \\ ... \\ \\frac{\\partial{y_m}}{\\partial{x_n}} \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial{y_1}} \\\\ \n",
    " \\vdots \\\\ \n",
    " \\frac{\\partial{l}}{\\partial{y_m} }\n",
    "\\end{bmatrix} \\dots \\begin{bmatrix}\n",
    " \\frac{\\partial{l}}{\\partial{x_1}} \\\\ \n",
    " \\vdots \\\\ \n",
    " \\frac{\\partial{l}}{\\partial{x_m}} \n",
    "\\end{bmatrix}$$\n",
    "注意，$J^T \\cdot v $给出的结果是列向量，前面提到的$v^T \\cdot J$ 给出的是行向量，没有本质区别。有了公式，就使得将external gradients给到非标量输出的模型变得容易。\n",
    "\n",
    "这里对vector-Jacobian product 的说明还有点晦涩. 所以接下来用其他例子进一步理解下 vector 和 Jacobian 分别是什么，external gradients又是什么意思。\n",
    "\n",
    "参考的blog有：[Pytorch中的vector-Jacobian product](https://juejin.im/post/5de5fbaae51d4523855e6dcc)和[详解Pytorch 自动微分里的（vector-Jacobian product）](https://zhuanlan.zhihu.com/p/65609544)\n",
    "\n",
    "$Y=G(X)$，Y和X都是向量，Y对X求导就是雅可比矩阵。如上面的例子，$Y=X^2$，求导的雅可比矩阵就是：\n",
    "\n",
    "$$ J = \\begin{bmatrix}\n",
    " 2x_1 \\ \\ \\ 0 \\ \\ \\ 0   \\\\ \n",
    " 0 \\ \\ \\ 2x_2 \\ \\ \\ 0  \\\\ \n",
    " 0 \\ \\ \\ 0 \\ \\ \\ 2x_3 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "注意$y_1=f_1(x_1,x_2,x_3)=x_1^2$，它是关于(x_1,x_2,x_3)的函数，而不仅仅只是关于$x_1$。而d(Y)对每一个分量$x_i$的导数是各个分量函数$y_j, j=1,2,3$对$x_i$的偏导数沿某一方向v的累积，一般默认的v是v=(1,1,1)，当然也可以传入一个向量来指定方向，这个vector就是所谓vector-Jacobian 中的vector的含义。\n",
    "\n",
    "而官方文档中说的可easy feed external gradient，可以将其理解为容易得到$x_i$的偏导数向量在v方向上的投影，或者各个分量函数关于$x_i$偏导的权重。v一旦确定，关于每个$x_i$的权重就确定了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4438, 0.8266, 1.9605], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1969, 0.6833, 3.8435], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones(3))  # 向量反向传播，要指定参数，一般指定默认方向，就是[1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8875, 1.6532, 3.9210])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  # 因为是默认方向，所以结果就是2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，看看Jacobian矩阵，手算验证下，见注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(2., requires_grad=True)\n",
      "tensor(3., requires_grad=True)\n",
      "tensor([ 8., 18., 33.], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "x1=torch.tensor(1, requires_grad=True, dtype = torch.float)\n",
    "x2=torch.tensor(2, requires_grad=True, dtype = torch.float)\n",
    "x3=torch.tensor(3, requires_grad=True, dtype = torch.float)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "y=torch.randn(3) # 定义y变量\n",
    "y[0]=x1**2+2*x2+x3 # 每个值定义不同的函数\n",
    "y[1]=x1+x2**3+x3**2\n",
    "y[2]=2*x1+x2**2+x3**3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad  # [2x_1 1 2] 代入数据求和就是5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.grad  # [2 3_x2^2 2x_2] 代入数据求和就是18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.grad  # [1 2x_3 3x_3^2] 代入数据求和就是34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不一样的vector看一下，这时候就可以使用公式$J^T \\cdot v $了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=torch.tensor(1, requires_grad=True, dtype = torch.float)\n",
    "x2=torch.tensor(2, requires_grad=True, dtype = torch.float)\n",
    "x3=torch.tensor(3, requires_grad=True, dtype = torch.float)\n",
    "y=torch.randn(3)\n",
    "y[0]=x1**2+2*x2+x3\n",
    "y[1]=x1+x2**3+x3**2\n",
    "y[2]=2*x1+x2**2+x3**3\n",
    "v=torch.tensor([3,2,1],dtype=torch.float)\n",
    "y.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad  # y对x_1偏导和v点积之后：[6x_1 2 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.grad  # y对x_2偏导和v点积之后：[6 6x_2^2 2x_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.grad  # y对x_3偏导和v点积之后：[3 4x_3 3x_3^2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在再看下官方的例子。这个例子是说，随机生成一个向量，乘2，然后求二范式，循环乘2多次直至使y的2范式大于1000得到最终得y。指定vector，backward就知道往哪算了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1124, -0.8276,  0.8662], requires_grad=True)\n",
      "tensor([ 0.2247, -1.6553,  1.7324], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.2247, -1.6553,  1.7324])\n",
      "tensor(2.4065)\n",
      "tensor([ 115.0533, -847.5005,  886.9653], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x * 2\n",
    "print(y)\n",
    "print(y.data)\n",
    "print(y.data.norm()) # the L2 norm (a.k.a Euclidean norm) of the tensor\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y是向量不是标量，所以torch.autograd 需要指定一个向量，变量才能执行backward。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最开始前面的文档说了可以通过用 with torch.no_grad() 包装代码块停止autograd 跟踪.requires_grad=True的tensors，下面看看具体操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者也可以使用.detach()来获得一个新的Tensor，与之前的tensor有相同的内容，只是没有了gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理解torch.nn\n",
    "\n",
    "神经网络可以使用 **torch.nn** 包构建. \n",
    " \n",
    "autograd 实现了反向传播功能, 但是直接用来写深度学习的代码在很多情况下还是稍显复杂,torch.nn 是专门为神经网络设计的模块化接口. nn 构建于 Autograd 之上, 可用来定义和运行神经网络. nn依赖于autograd来定义模型，并作微分计算。nn.Module 是 nn 中最重要的类, 可把它看成是一个网络的封装, 包含网络各层定义以及 forward 方法, \n",
    "调用 forward(input) 方法, 可返回前向传播的结果. 比如手写数字识别CNN的结构：\n",
    "\n",
    "![](mnist.png)\n",
    "\n",
    "这是一个简单的前向网络，获取输入，层层前进，得到结果。\n",
    "\n",
    "一个典型的神经网络训练过程如下:\n",
    "\n",
    "- 定义具有一些可学习参数(或权重)的神经网络\n",
    "- 迭代输入数据集\n",
    "- 通过网络处理输入\n",
    "- 计算损失(输出的预测值与实际值之间的距离)\n",
    "- 将梯度传播回网络\n",
    "- 更新网络的权重, 通常使用一个简单的更新规则: weight = weight - learning_rate * gradient\n",
    "\n",
    "### 定义网络\n",
    "\n",
    "定义上图中的网络，关于CNN更多基本概念，参考后面的5-cnn-example文件夹中的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"注意定义的层都是利用nn中有的层进行定义的\"\"\"\n",
    "        # 首先定义构造函数，继承父类nn.Module 的构造函数\n",
    "        super(Net, self).__init__()\n",
    "        # 卷积层 '1'表示输入图片为单通道channel, '6'表示输出通道数channel, '3'表示卷积核为3*3\n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # 仿射运算: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward中利用function中的函数调用定义的网络各层进行前向计算\"\"\"\n",
    "        # 其中(2, 2)表示池化操作窗口(2, 2) \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果size是square , 则只能指定一个数字\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除 batch dimension 外的所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只需要计算forward函数，backward函数会使用autograd自动定义。在forward函数中可以使用任意的tensor运算。模型可以学习的参数由net.parameters()返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在尝试一个随机的32\\*32的输入，看看输出什么。注意torch.nn仅支持输入时mini-batch的，因此我们用单个例子来试也需要转换成mini－batch形式的。也可以使用input.unsqueeze(0)来增加一个假的batch层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0082,  0.0089, -0.1283,  0.1316,  0.0470, -0.0448,  0.0229, -0.0607,\n",
      "          0.0747, -0.0102]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)  # nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有参数gradient缓存置零，随机方向反向传播，试试看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单回顾下目前为止的class：\n",
    "\n",
    "- torch.Tensor：一个支持autograd运算（比如backward()）的多维数组，并持有关于tensor的gradient；\n",
    "- nn.Module：神经网络模块。方便封装参数，并方便移入GPU，加载，输出等；\n",
    "- nn.Parameter：一类Tensor，当作为Module的属性时，会被自动注册为parameter；\n",
    "- autograd.Function：实现autograd的forward和backward定义。每个Tensor运算创建至少一个Function节点，该节点关联到创建Tensor的functions，并编码它的历史。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "loss function接受输入对（output和target），并计算两者之间的距离。在nn包下有一些不同的loss functions。一个比较简单的是MSELoss，它计算mean-squared error。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor(0.6770, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(target.shape)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在反向跟踪loss，使用.grad_fn属性，计算图是这样的：\n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "      \n",
    "调用loss.backward()时，整个计算图都会根据loss做微分，graph中所有有requires_grad=True 属性的Tensors都有累积gradient的.grad Tensor 。 个人理解，每个requires_grad=True 的节点都是知道累计到它这的梯度的tensor。用.grad_fn.next_functions[0][0]可以上溯看function："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000017FCB406C08>\n",
      "<AddmmBackward object at 0x0000017FCB406288>\n",
      "<AccumulateGrad object at 0x0000017FCB40B988>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "为了反向传播，需要使用loss.backward()。您需要清除现有的梯度，否则梯度将积累到现有的梯度。关于为什么需要清楚梯度，这里解释的不是太清楚。因此，参考一些blog做说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "通常来说，当必须处理图像、文本、音频或视频数据时，可以使用python标准库将数据**加载到numpy数组**里。然后将这个数组**转化成torch.*Tensor**。\n",
    "\n",
    "- 对于图片，有Pillow，OpenCV等包可以使用；\n",
    "- 对于音频，有scipy和librosa等包可以使用；\n",
    "- 对于文本，不管是原生python的或者是基于Cython的文本，可以使用NLTK和SpaCy。\n",
    "\n",
    "特别对于视觉方面，我们创建了一个包，名字叫torchvision，其中包含了针对Imagenet、CIFAR10、MNIST等常用数据集的数据加载器（data loaders），还有对图片数据变形的操作，即torchvision.datasets和torch.utils.data.DataLoader。\n",
    "\n",
    "但是像网络上下载的纯数据，想要顺利地输入到自己定义的模型中，可能这一步就要花些时间来处理了。\n",
    "\n",
    "在训练中，想要利用GPU需要做一些处理：\n",
    "\n",
    "与将一个张量传递给GPU一样，可以这样**将神经网络转移到GPU上**。\n",
    "\n",
    "如果我们有cuda可用的话，让我们首先定义第一个设备为可见cuda设备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后这些方法将递归遍历所有模块，并将它们的参数和缓冲区转换为CUDA张量。请记住，我们不得不将**输入和目标**在**每一步都送入GPU**（inputs, labels = inputs.to(device), labels.to(device)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跟着例子学习PyTorch\n",
    "\n",
    "通过例子了解pytorch的基本概念。\n",
    "\n",
    "PyTorch的核心是提供两个主要功能：\n",
    "\n",
    "- n维张量，类似于numpy，但可以在GPU上运行；\n",
    "- 自动区分以构建和训练神经网络。\n",
    "\n",
    "### 张量\n",
    "\n",
    "先使用 numpy 实现网络（更具体地可以参考:[numpy部分的手写神经网络](https://github.com/OuyangWenyu/hydrus/blob/master/3-numpy-examples/neural_network.py)），这里也做些记录，详见nn_basic.ipynb\n",
    "\n",
    "Numpy 提供了一个n维的数组对象, 并提供了许多操纵这个数组对象的函数。Numpy 是科学计算的通用框架; Numpy 数组没有计算图, 也没有深度学习, 也没有梯度下降等方法实现的接口。但是可以很容易地使用 numpy 生成随机数据，并将产生的数据传入双层的神经网络,并实现这个网络的正向传播和反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N是批尺寸参数；D_in是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算并显示loss（损失）\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy 是一个伟大的框架, 但它不能利用 GPU 加速它数值计算，而对于现代的深度神经网络, GPU 往往是提供 50倍或更大的加速,所以 numpy 不足以满足现在深度学习的需求。\n",
    "\n",
    "PyTorch提供了Tensor，其在概念上与 numpy 数组相同，也是一个n维数组, 不过PyTorch 提供了很多能在这些 Tensor 上操作的函数。\n",
    "\n",
    "像 numpy 数组一样, PyTorch Tensor 也和numpy的数组对象一样不了解深度学习,计算图和梯度下降，它们只是科学计算的通用工具；\n",
    "\n",
    "然而不像 numpy, PyTorch Tensor 可以利用 GPU 加速他们的数字计算。\n",
    "\n",
    "要在 GPU 上运行 PyTorch 张量, 只需将其转换为新的数据类型.\n",
    "\n",
    "将 PyTorch Tensor 生成的随机数据传入双层的神经网络. 就像上面的 numpy 例子一样,\n",
    "我们需要手动实现网络的正向传播和反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N是批尺寸大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求导\n",
    "\n",
    "对于小型的两层网络而言，手动实现反向传递并不重要，但对于大型的复杂网络而言，这变得非常麻烦。\n",
    "\n",
    "幸运的是，我们可以使用**自动微分** 来自动计算神经网络中的反向传播。PyTorch中的 autograd软件包提供了这个功能。使用autograd时，您的网络正向传递将定义一个 计算图；图中的节点为张量，图中的边为从输入张量产生输出张量的函数。通过该图进行反向传播，可以轻松计算梯度。\n",
    "\n",
    "这听起来很复杂，在实践中非常简单。**每个张量代表计算图中的一个节点**。如果 x是一个张量，并且有 x.requires_grad=True，那么x.grad就是另一个张量，代表着x相对于某个标量值的梯度。\n",
    "\n",
    "通过使用PyTorch张量和autograd来实现网络就不再需要手动实现网络的反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N是批尺寸大小；D_in是输入维度；\n",
    "# H是隐藏层维度；D_out是输出维度 \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据，将requires_grad置为False，意味着我们不需要在反向传播时候计算这些值的梯度\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 产生随机权重tensor，将requires_grad设置为True，意味着我们希望在反向传播时候计算这些值的梯度\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：使用tensor的操作计算预测值y。\n",
    "    # 由于w1和w2有requires_grad=True,涉及这些张量的操作将让PyTorch构建计算图，从而允许自动计算梯度。\n",
    "    # 由于我们不再手工实现反向传播，所以不需要保留中间值的引用。\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    # loss是一个形状为(1,)的张量\n",
    "    # loss.item()是这个张量对应的python数值\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 使用autograd计算反向传播,这个调用将计算loss对所有requires_grad=True的tensor的梯度。\n",
    "    # 这次调用后，w1.grad和w2.grad将分别是loss对w1和w2的梯度张量。\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段构建计算图，\n",
    "    # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 反向传播之后手动将梯度置零，原因在后面第三节nn到底是什么一节中简单补充说明\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在底层，每一个原始的**自动求导运算**实际上是**两个在Tensor上运行的函数**。其中，**forward**函数计算从输入Tensors获得的输出Tensors。而**backward**函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。\n",
    "\n",
    "在PyTorch中，可以很容易地通过定义**torch.autograd.Function**的子类并实现**forward和backward函数**，来**定义自己的自动求导运算**。然后，我们可以通过**构造实例**并**像调用函数一样调用它**，并传递包含输入数据的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，并完成张量的正向和反向传播。\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        在前向传播中，我们收到包含输入和返回的张量包含输出的张量。 \n",
    "        ctx是可以使用的上下文对象存储信息以进行向后计算。 \n",
    "        您可以使用ctx.save_for_backward方法缓存任意对象，以便反向传播使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收到上下文对象和一个张量，其包含了相对于正向传播过程中产生的输出的损失的梯度。\n",
    "        我们可以从上下文对象中检索缓存的数据，并且必须计算并返回与正向传播的输入相关的损失的梯度。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N是批尺寸大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 产生随机权重的张量\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 为了使用我们的方法，我们调用Function.apply方法。 我们将其命名为“ relu”。\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 正向传播：使用张量上的操作来计算输出值y;\n",
    "    # 我们使用自定义的自动求导操作来计算 RELU.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 使用autograd计算反向传播过程。\n",
    "    loss.backward()\n",
    "\n",
    "    # 用梯度下降更新权重\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 在反向传播之后手动清零梯度\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn模块\n",
    "\n",
    "计算图和autograd是定义复杂运算符并自动采用导数的非常强大的范例。但是对于大型神经网络，原始的autograd可能会有点太低了。\n",
    "\n",
    "在TensorFlow中，诸如Keras， TensorFlow-Slim和TFLearn之类的软件包在原始计算图上提供了更高级别的抽象接口，这些封装对构建神经网络很有用。\n",
    "\n",
    "在PyTorch中，该nn程序包达到了相同的目的。该nn 包定义了**一组Modules**，它们**大致等效于神经网络层**。模块接收输入张量并计算输出张量，但也可以保持内部状态，例如包含可学习参数的张量。该nn软件包还定义了一组有用的**损失函数**，这些函数通常在训练神经网络时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包将我们的模型定义为一系列的层\n",
    "# nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出\n",
    "# 每个线性模块使用线性函数从输入计算输出，并保存其内部的权重和偏差张量\n",
    "# 在构造模型之后，我们使用.to()方法将其移动到所需的设备\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# nn包还包含常用的损失函数的定义\n",
    "# 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 前向传播：通过向模型传入x计算预测的y\n",
    "    # 模块对象重载了__call__运算符，所以可以像函数那样调用它们\n",
    "    # 这么做相当于向模块传入了一个张量，然后它返回了一个输出张量\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印损失。我们传递包含y的预测值和真实值的张量，损失函数返回包含损失的张量\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 反向传播之前清零梯度\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 反向传播：计算模型的损失对所有可学习参数的梯度\n",
    "    # 在内部，每个模块的参数存储在requires_grad=True的张量中\n",
    "    # 因此这个调用将计算模型中所有可学习参数的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    # 每个参数都是张量，所以我们可以像我们以前那样可以得到它的数值和梯度\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实践中，我们经常使用更复杂的优化器（例如AdaGrad，RMSProp，Adam等）来训练神经网络。\n",
    "\n",
    "PyTorch中的软件包optim抽象了优化算法的思想，并提供了常用优化算法的实现。\n",
    "\n",
    "在此示例中，我们将像之前一样使用nn包来定义模型，但是用optim包提供的Adam算法来优化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包定义模型和损失函数\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重\n",
    "# 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法\n",
    "# Adam构造函数的第一个参数告诉优化器应该更新哪些张量\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 前向传播：通过像模型输入x计算预测的y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重)。\n",
    "    # 这是因为默认情况下，每当调用.backward（）时，渐变都会累积在缓冲区中（即不会被覆盖）\n",
    "    # 有关更多详细信息，请查看torch.autograd.backward的文档。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 反向传播：根据模型的参数计算loss的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 调用Optimizer的step函数使它所有参数更新\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时，您将需要**指定比一系列现有模块更复杂的模型**。在这些情况下，您可以通过**继承nn.Module和定义一个forward来定义自己的模型**，这个forward模块可以使用其他模块或在Tensors上的其他自动求导运算来接收输入Tensors并生成输出Tensors。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。\n",
    "        我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 通过实例化上面定义的类来构建我们的模型\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 构造损失函数和优化器\n",
    "# SGD构造函数中对model.parameters()的调用\n",
    "# 将包含模型的一部分，即两个nn.Linear模块的可学习参数\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 前向传播：通过向模型传递x计算预测值y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并输出loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 清零梯度，反向传播，更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn到底是什么\n",
    "\n",
    "因为nn包是使用pytorch时最重要的工具，因此这里结合官网《torch.nn到底是什么》一文，认真理解一遍。这部分是本文最重要的环节。示例用的是MNIST数据。\n",
    "\n",
    "2.用PyTorch中的**nn.Linear可以代替手动定义和初始化self.weights和self.bias以及计算xb @ self.weights + self.bias**, 因为nn.Linear可以完成这些操作。 PyTorch中预设了很多类型的神经网络层，使用它们可以极大的简化我们的代码，通常还会带来速度上的提升。\n",
    "\n",
    "3.PyTorch还有一个包含很多优化算法的包————torch.optim。我们可以使用优化器中的step方法执行前向传播过程中的步骤来替换手动更新每个参数。\n",
    "这个方法将允许我们替换之前手动编写的优化步骤。\n",
    "\n",
    "### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数据集采用numpy数组格式，并已使用pickle存储，pickle是一个用来把数据序列化为python特定格式的库。\n",
    "\n",
    "每一幅图像都是28 x 28的，并被拉平成长度为784(=28x28)的一行。 我们以其中一个为例展示一下，首先需要将这个一行的数据重新变形为一个2d的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "# pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch使用torch.tensor，而不是numpy数组，所以我们需要将数据转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "# n是样本个数50000,c是每个样本长度\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不使用nn构建神经网络\n",
    "\n",
    "使用PyTorch提供的**创建随机数填充或全零填充张量**的方法，**初始化**一个简单线性模型的**权重和偏置**。 这两个都是普通的张量，但它们有一个特殊的附加条件：**设置需要计算梯度的参数为True**。这样PyTorch就会记录所有与这个张量相关的运算，使其能在反向传播阶段自动计算梯度。\n",
    "\n",
    "对于weights而言，由于我们希望初始化张量过程中存在梯度，所以我们在初始化之后**设置requires_grad**。（注意：**尾缀为_的方法**在PyTorch中表示这个操作会被立即被执行。）\n",
    "\n",
    "注意：以[Xavier初始化方法](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)（每个元素都除以1/sqrt(n)）为例来对权重进行初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "# 尾缀为_的方法在PyTorch中表示这个操作会被立即被执行\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多亏了PyTorch具有**自动梯度计算**功能，我们可以使用Python中**任何标准函数**（或者可调用对象）来创建模型！ 因此，这里编写一个普通的矩阵乘法和广播加法建立一个简单的线性模型，以及一个激活函数（使用一个log_softmax函数）。 请记住：尽管Pytorch提供了许多预先编写好的损失函数、激活函数等等，仍然可以使用纯python轻松**实现你自己的函数**。 Pytorch甚至可以自动地为你的函数创建快速的GPU代码或向量化的CPU代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0541, -2.3306, -1.9161, -2.0688, -2.8124, -2.4089, -2.2310, -2.6536,\n",
      "        -2.7899, -2.1908], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)   # @表示点积运算符\n",
    "\n",
    "bs = 64  # 一批数据个数 batchsize\n",
    "\n",
    "xb = x_train[0:bs]  # 从x获取一小批数据\n",
    "preds = model(xb)  # 预测值\n",
    "preds[0], preds.shape\n",
    "# 一次前向计算的结果\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意**张量preds不仅包括了张量值，还包括了梯度函数**（也就是因为这样，才能直接调用backward函数反向传播）。这个梯度函数我们可以在后面的反向传播阶段用到。\n",
    "\n",
    "接下来实现一个负的对数似然函数（Negative log-likehood）作为损失函数（同样也使用纯python实现）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3581, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll\n",
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来实现一个用来计算模型准确率的函数。对于每次预测，我们规定如果预测结果中概率最大的数字和图片实际对应的数字是相同的，那么这次预测就是正确的。\n",
    "\n",
    "先来看一下被随机初始化的模型的准确率，这样我们就可以看到损失值降低的时候准确率是否提高了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0469)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行一个完整的训练步骤了，每次迭代会进行以下几个操作：\n",
    "\n",
    "- 从全部数据中选择一小批数据（大小为bs，即batchsize）\n",
    "- 使用模型进行预测\n",
    "- 计算当前预测的损失值\n",
    "- 使用loss.backward()更新模型中的梯度，在这个例子中，更新的是weights和bias\n",
    "\n",
    "**利用计算出的梯度更新权值和偏置项**，因为我们**不希望这一步的操作被用于下一次迭代的梯度计算**，所以我们在**torch.no_grad()这个上下文管理器中完成**。\n",
    "\n",
    "将**梯度设置为0**，来为下一次循环做准备。因为梯度将会记录所有已经执行过的运算，即loss.backward()会将梯度变化值直接与变量已有值进行累加，而不是替换变量原有的值，这样设计的原因可以参考[PyTorch中在反向传播前为什么要手动将梯度清零？](https://www.zhihu.com/question/303070254)。总的来说，一般每次batch计算前都会把梯度清零，只计算本次的梯度，然后更新一次网络，但是也可以执行梯度累加，即之前batch计算的梯度累加到后来的梯度上，然后根据累加的梯度更新参数，过一些batch循环之后再将梯度清零，这样相当于变相地扩大了batchsize，使训练速度快一些，对显存不够的机器来说是一个trick。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0599, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # 学习率\n",
    "epochs = 2  # 训练的轮数\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # n是样本数量，bs是batchsize，每个epoch内训练的次数是batch的个数 \n",
    "    for i in range((n - 1) // bs + 1):\n",
    "#         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        # 反向传播之后还需要更新权重：       \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            # 设置梯度为0          \n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下之前所有步骤：获取数据——定义网络——初始化参数——定义激活函数、损失函数、准确率等——epoch和batch两层循环-对每个batch样本前向计算，损失计算，反向传播、权重更新、梯度归0——所有循环结束输出结果\n",
    "\n",
    "### 使用torch.nn.functional\n",
    "\n",
    "接下来使用nn的一系列模块重构代码，使代码更简洁。\n",
    "\n",
    "第一步也是最简单的一步，是使用torch.nn.functional（通过会在引用时用F表示）中的函数**替换我们自己的激活函数和损失函数**使代码变得更短。 这个模块包含了torch.nn库中的所有函数（这个库的其它部分是各种类），所以在这个模块中还会找到其它便于建立神经网络的函数，比如池化函数。（模块中还包含卷积函数，线性函数等等，不过在后面的内容中我们会看到，这些操作使用库中的其它部分会更好。）\n",
    "\n",
    "对负对数似然损失和对数柔性最大值(softmax)激活函数，PyTorch有一个结合了这两个函数的简单函数F.cross_entropy可供使用，这样我们就可以删掉模型中的激活函数。\n",
    "\n",
    "注意在model函数中我们**不再调用log_softmax**（F中的loss_func应该是有固定的调用模式的，所以要在外面调用loss_func计算，参数是model输出和实际值）。可确认一下损失值和准确率与之前相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0599, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias\n",
    "\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用torch.nn.Module\n",
    "\n",
    "接下来用nn.Moduel和nn.Parameter来完成一个更加清晰简洁的训练循环。继承nn.Module(它是一个**能够跟踪状态的类**)。\n",
    "\n",
    "新建一个类，实现**存储权重，偏置和前向传播**步骤中所有用到方法。\n",
    "\n",
    "nn.Module包含了许多属性和方法（比如.parameters()和.zero_grad()），它是一个PyTorch中特有的概念，是一个会经常用到的类。**不要和Python中module（m小写）混淆**，module是指一个可以被引入的Python代码文件。\n",
    "\n",
    "nn.Module对象的使用方式很像函数（例如它们是**可调用的**），PyTorch将会**自动调用定义的forward函数**（意味着有__call__函数）。\n",
    "\n",
    "首先实例化一个对象，然后像函数一样使用它即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3953, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "    \n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前在每个训练循环中，我们通过变量名对每个变量的值进行更新，并手动的将每个变量的梯度置为0，现在我们可以利用**model.parameters()和model.zero_grad()** （这两个都是PyTorch定义在nn.Module中的）使这些步骤变得更加简洁并且更不容易忘记更新部分参数，尤其是模型很复杂的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0822, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                # 最后两步简化                \n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                # 一步到位设置权重和偏置梯度为0                \n",
    "                model.zero_grad()\n",
    "\n",
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用torch.nn.Linear\n",
    "\n",
    "用PyTorch中的**nn.Linear代替手动定义和初始化self.weights和self.bias以及计算xb @ self.weights + self.bias**, 因为nn.Linear可以完成这些操作。 PyTorch中预设了很多类型的神经网络层，使用它们可以极大的简化我们的代码，通常还会带来速度上的提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0818, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Linear简化初始化参数和一步前向计算过程\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "    \n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用torch.optim\n",
    "\n",
    "PyTorch还有一个包含很多**优化算法**的包——torch.optim。我们可以使用优化器中的**step方法执行前向传播过程中的步骤**来替换手动更新每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0810, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"这里将建立模型和优化器的步骤放在一起方便复用\"\"\"\n",
    "    model = Mnist_Logistic()\n",
    "    # 使用SGD随机梯度下降算法，其参数是model的paramters以及学习率\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        # 一步到位更新参数，设置0梯度也是利用opt\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此，先总结一小波，对应训练神经网络的所有步骤里都用到了什么：\n",
    "\n",
    "定义神经网络（继承Module来定义网络，通过**optim定义优化算法**）——初始化参数及定义前向计算函数（使用Linear完成初始化参数以及一步前向计算定义）——定义激活函数和损失函数（使用nn.functional的函数可以一步到位直接计算出损失值）——两层循环中调用model前向计算（调用Module的子类对象就像调用函数一样，会自动调用forward函数）——计算损失值（调用loss计算函数，参数是model输出和实际值）——反向传播（直接调用loss张量的backward函数即可）——更新参数（使用optim的step函数）——梯度设为0（使用optim的zero_grad方法）\n",
    "\n",
    "接下来，在前面的数据处理环节也可以重构。\n",
    "\n",
    "### 使用Dataset\n",
    "\n",
    "Pytorch包含一个**Dataset抽象类**。Dataset可以是任何东西，但它始终包含一个 **__len__函数** （通过Python中的标准函数len调用）和一个**用来索引到内容中的__getitem__函数**。接下来以创建Dataset的**自定义子类**FacialLandmarkDataset为例进行介绍。\n",
    "\n",
    "PyTorch中的**TensorDataset是一个封装了张量的Dataset**。通过定义长度和索引的方式，是我们可以对张量的第一维进行迭代，索引和切片。这将使我们在训练中，获取同一行中的自变量和因变量更加容易。可以把x_train和y_train中的数据合并成一个简单的TensorDataset，这样就可以方便的进行迭代和切片操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0816, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# 定义TensorDataset对象\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        # 直接索引、切片 \n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用DataLoader\n",
    "\n",
    "PyTorch的DataLoader负责**批量数据管理**，你可以使用任意的Dataset创建一个DataLoader。DataLoader使得**对批量数据的迭代更容易**。DataLoader**自动的为我们提供每一小批量的数据**来代替切片的方式train_ds[i*bs:i*bs+bs]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0806, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)\n",
    "    \n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结一下，后面增加的两个类是可以使数据的处理，包括索引、切片和分batch更容易的类。\n",
    "\n",
    "接下来是一些提供模型效率所需的基本特征。\n",
    "\n",
    "### 增加验证集\n",
    "\n",
    "在实际训练中始终应该有一个验证集来确认模型是否过拟合。\n",
    "\n",
    "**打乱训练数据的顺序通常是避免不同批数据中存在相关性和过拟合的重要步骤**。但是另一方面，**无论是否打乱顺序计算出的验证集损失值都是一样的**。鉴于打乱顺序还会消耗额外的时间，所以打乱**验证集数据**是没有任何意义的。\n",
    "\n",
    "我们在验证集上用到的每批数据的数量是训练集的两倍，这是因为**在验证集上不需要进行反向传播，这样就会占用较小的内存**（因为它并不需要储存梯度）。我们利用了这一点，**使用了更大的batchsize，更快的计算出了损失值**（如果显存不够就要用小一点的batchsize了）。\n",
    "\n",
    "我们将会在每轮(epoch)结束后计算并输出验证集上的损失值。\n",
    "\n",
    "（注意：在**训练前我们总是会调用model.train()函数**，在**推断之前调用model.eval()函数**，因为这些会被nn.BatchNorm2d，nn.Dropout等层使用，确保在不同阶段的准确性。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.4151)\n",
      "1 tensor(0.3024)\n"
     ]
    }
   ],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 在每个epoch内，训练前都要调用 model.train()函数\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    # 在每个epoch内，训练后，计算loss前，都要调用 model.eval()函数\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 在no_grad环境内计算loss\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写fit()和get_data()函数\n",
    "\n",
    "我们在计算训练集和验证集上的损失值时执行了差不多的过程两次，因此我们将这部分代码提炼成一个函数loss_batch，用来计算每个批的损失值。\n",
    "\n",
    "我们为训练集传递一个优化器参数来执行反向传播。对于验证集我们不传优化器参数，这样就不会执行反向传播。\n",
    "\n",
    "fit执行了训练模型的必要操作，并在每轮(epoch)结束后计算模型在训练集和测试集上的损失。\n",
    "\n",
    "get_data返回训练集和验证集需要使用到的dataloaders。\n",
    "\n",
    "现在，我们只需要三行代码就可以获取数据、拟合模型了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.35463817160129546\n",
      "1 0.2867066876769066\n"
     ]
    }
   ],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "        \n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用GPU\n",
    "\n",
    "首先检查一下的GPU是否可以被PyTorch调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，新建一个设备对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后更新一下preprocess函数将批运算移到GPU上计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WrappedDataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-47d104ef9d6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_dl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWrappedDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mvalid_dl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWrappedDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WrappedDataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以把模型移动到GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
